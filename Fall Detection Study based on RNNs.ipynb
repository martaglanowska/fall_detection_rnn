{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fall Detection Study based on RNNs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of the research have been presented and described in the paper titled **\"Fall detection from accelerometer data using recurrent neural networks\"**\n",
    "\n",
    "**Authors:** Natalia Bartczak, Marta Glanowska, Karolina Kowalewicz, Maciej Kunin, Robert Susik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.test.is_built_with_cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fall_data = scipy.io.loadmat('./UniMiB-SHAR/data/fall_data.mat')['fall_data']\n",
    "adl_data  = scipy.io.loadmat('./UniMiB-SHAR/data/adl_data.mat')['adl_data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fall_data = pd.DataFrame(fall_data)\n",
    "adl_data = pd.DataFrame(adl_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4192, 453), (7579, 453))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fall_data.shape, adl_data.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "fall_labels = scipy.io.loadmat('./UniMiB-SHAR/data/fall_labels.mat')['fall_labels']\n",
    "fall_labels = pd.DataFrame(fall_labels)     #activity id, person id, trial number\n",
    "\n",
    "adl_labels = scipy.io.loadmat('./UniMiB-SHAR/data/adl_labels.mat')['adl_labels']\n",
    "adl_labels = pd.DataFrame(adl_labels)       #activity id, person id, trial number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4192, 3), (7579, 3))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fall_labels.shape, adl_labels.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "fall_lab = fall_labels[1]\n",
    "d = {'subject': fall_labels[1], 'label': list(np.ones(len(fall_labels[1])))}\n",
    "df_fall = pd.DataFrame(data=d)\n",
    "\n",
    "adl_lab = adl_labels[1]\n",
    "d = {'subject': adl_labels[1], 'label': list(np.zeros(len(adl_labels[1])))}\n",
    "df_adl = pd.DataFrame(data=d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4192, 2), (7579, 2))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fall.shape, df_adl.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Split to folds - stratified fold with respect to person number, proportional split (k = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_1_lab = pd.DataFrame()\n",
    "fold_2_lab = pd.DataFrame()\n",
    "fold_3_lab = pd.DataFrame()\n",
    "fold_4_lab = pd.DataFrame()\n",
    "fold_5_lab = pd.DataFrame()\n",
    "\n",
    "fold_1 = pd.DataFrame()\n",
    "fold_2 = pd.DataFrame()\n",
    "fold_3 = pd.DataFrame()\n",
    "fold_4 = pd.DataFrame()\n",
    "fold_5 = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#falls\n",
    "for num_person in range(1, 31):\n",
    "    num_per_person = df_fall[df_fall['subject'] == num_person].count()[0]\n",
    "    idxs = df_fall[df_fall['subject'] == num_person].index.to_list()\n",
    "    random.shuffle(idxs)                                                #added - random split\n",
    "    k = df_fall[df_fall['subject'] == num_person].count()[0]//5\n",
    "    fold_1_lab = pd.concat([fold_1_lab, df_fall.loc[idxs[:k]]['label']])\n",
    "    fold_1 = pd.concat([fold_1, fall_data.loc[idxs[:k]]])\n",
    "    fold_2_lab = pd.concat([fold_2_lab, df_fall.loc[idxs[k:k*2]]['label']])\n",
    "    fold_2 = pd.concat([fold_2, fall_data.loc[idxs[k:k*2]]])\n",
    "    fold_3_lab = pd.concat([fold_3_lab, df_fall.loc[idxs[k*2:k*3]]['label']])\n",
    "    fold_3 = pd.concat([fold_3, fall_data.loc[idxs[k*2:k*3]]])\n",
    "    fold_4_lab = pd.concat([fold_4_lab, df_fall.loc[idxs[k*3:k*4]]['label']])\n",
    "    fold_4 = pd.concat([fold_4, fall_data.loc[idxs[k*3:k*4]]])\n",
    "    fold_5_lab = pd.concat([fold_5_lab, df_fall.loc[idxs[k*4:k*5]]['label']])\n",
    "    fold_5 = pd.concat([fold_5, fall_data.loc[idxs[k*4:k*5]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adl\n",
    "for num_person in range(1, 31):\n",
    "    num_per_person = df_adl[df_adl['subject'] == num_person].count()[0]\n",
    "    idxs = df_adl[df_adl['subject'] == num_person].index.to_list()\n",
    "    random.shuffle(idxs)                                                #added - random split\n",
    "    k = df_adl[df_adl['subject'] == num_person].count()[0]//5\n",
    "    fold_1_lab = pd.concat([fold_1_lab, df_adl.loc[idxs[:k]]['label']])\n",
    "    fold_1 = pd.concat([fold_1, adl_data.loc[idxs[:k]]])\n",
    "    fold_2_lab = pd.concat([fold_2_lab, df_adl.loc[idxs[k:k*2]]['label']])\n",
    "    fold_2 = pd.concat([fold_2, adl_data.loc[idxs[k:k*2]]])\n",
    "    fold_3_lab = pd.concat([fold_3_lab, df_adl.loc[idxs[k*2:k*3]]['label']])\n",
    "    fold_3 = pd.concat([fold_3, adl_data.loc[idxs[k*2:k*3]]])\n",
    "    fold_4_lab = pd.concat([fold_4_lab, df_adl.loc[idxs[k*3:k*4]]['label']])\n",
    "    fold_4 = pd.concat([fold_4, adl_data.loc[idxs[k*3:k*4]]])\n",
    "    fold_5_lab = pd.concat([fold_5_lab, df_adl.loc[idxs[k*4:k*5]]['label']])\n",
    "    fold_5 = pd.concat([fold_5, adl_data.loc[idxs[k*4:k*5]]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7043</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5753</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2055</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4073</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7048</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2326 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0\n",
       "49    1.0\n",
       "91    1.0\n",
       "19    1.0\n",
       "92    1.0\n",
       "119   1.0\n",
       "...   ...\n",
       "7043  0.0\n",
       "5753  0.0\n",
       "2055  0.0\n",
       "4073  0.0\n",
       "7048  0.0\n",
       "\n",
       "[2326 rows x 1 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fold_1_lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check - original dataset samples number\n",
    "df_fall[df_fall['subject'] == 1].count()[0] + df_adl[df_adl['subject'] == 1].count()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    1503\n",
       "1.0     823\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#all subjects, fold 1\n",
    "fold_1_lab.value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN-infused model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    hidden_dim = 512\n",
    "    seq_len = fold_1.shape[1]\n",
    "    n_features = 1\n",
    "    learning_rate = 0.0003\n",
    "\n",
    "    regularizers = {\n",
    "        'kernel_regularizer':tf.keras.regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
    "        'bias_regularizer':tf.keras.regularizers.l2(1e-4),\n",
    "        'activity_regularizer':tf.keras.regularizers.l2(1e-5)\n",
    "    }\n",
    "\n",
    "    in1 = tf.keras.layers.Input(shape=(seq_len, n_features))\n",
    "    gru1 = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(hidden_dim, return_sequences=True, **regularizers))(in1)\n",
    "    gru2 = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(hidden_dim, name='latent_layer', **regularizers))(gru1)\n",
    "    rsh1 = tf.keras.layers.Reshape( (hidden_dim*2, 1) )(gru2)\n",
    "    cov1 = tf.keras.layers.Dropout(0.1)(tf.keras.layers.Conv1D(seq_len, 3, activation='relu', **regularizers)(rsh1))\n",
    "    max1 = tf.keras.layers.MaxPool1D(pool_size=3, strides=1)(cov1)\n",
    "    cov2 = tf.keras.layers.Dropout(0.1)(tf.keras.layers.Conv1D(seq_len, 5, activation='relu', **regularizers)(max1))\n",
    "    max2 = tf.keras.layers.MaxPool1D(pool_size=3, strides=1)(cov2)\n",
    "    cov3 = tf.keras.layers.Dropout(0.1)(tf.keras.layers.Conv1D(seq_len, 3, activation='relu', **regularizers)(max2))\n",
    "    max3 = tf.keras.layers.MaxPool1D(pool_size=3, strides=1)(cov3)\n",
    "    rsh2 = tf.keras.layers.Reshape( (seq_len, 1010) )(max3)\n",
    "    tdd1 = tf.keras.layers.Dropout(0.1)(tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(1,**regularizers))(rsh2))\n",
    "    f1   = tf.keras.layers.Flatten()(tdd1)\n",
    "    d1   = tf.keras.layers.Dense(1, activation='sigmoid', **regularizers)(f1)\n",
    "\n",
    "    classifier = tf.keras.Model(\n",
    "        inputs=[in1], \n",
    "        outputs=[d1]\n",
    "    )\n",
    "\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    classifier.compile(loss='binary_crossentropy', optimizer=opt, \n",
    "        metrics=['accuracy'])\n",
    "    \n",
    "    return classifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building own cross_validation model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/christianversloot/machine-learning-articles/blob/main/how-to-use-k-fold-cross-validation-with-keras.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "581/581 [==============================] - 162s 263ms/step - loss: 0.8191 - accuracy: 0.8523 - val_loss: 0.7482 - val_accuracy: 0.8160 - lr: 3.0000e-04\n",
      "Epoch 2/20\n",
      "581/581 [==============================] - 154s 263ms/step - loss: 0.4269 - accuracy: 0.9072 - val_loss: 0.4770 - val_accuracy: 0.8779 - lr: 3.0000e-04\n",
      "Epoch 3/20\n",
      "581/581 [==============================] - 152s 262ms/step - loss: 0.2540 - accuracy: 0.9480 - val_loss: 0.2779 - val_accuracy: 0.9295 - lr: 3.0000e-04\n",
      "Epoch 4/20\n",
      "581/581 [==============================] - 153s 263ms/step - loss: 0.1826 - accuracy: 0.9633 - val_loss: 0.3109 - val_accuracy: 0.9162 - lr: 3.0000e-04\n",
      "Epoch 5/20\n",
      "581/581 [==============================] - 153s 263ms/step - loss: 0.1333 - accuracy: 0.9759 - val_loss: 0.2376 - val_accuracy: 0.9291 - lr: 3.0000e-04\n",
      "Epoch 6/20\n",
      "581/581 [==============================] - 153s 263ms/step - loss: 0.1160 - accuracy: 0.9806 - val_loss: 0.2985 - val_accuracy: 0.9157 - lr: 3.0000e-04\n",
      "Epoch 7/20\n",
      "581/581 [==============================] - 152s 262ms/step - loss: 0.0978 - accuracy: 0.9862 - val_loss: 0.1808 - val_accuracy: 0.9437 - lr: 3.0000e-04\n",
      "Epoch 8/20\n",
      "581/581 [==============================] - 152s 262ms/step - loss: 0.0868 - accuracy: 0.9874 - val_loss: 0.2108 - val_accuracy: 0.9218 - lr: 3.0000e-04\n",
      "Epoch 9/20\n",
      "581/581 [==============================] - 152s 262ms/step - loss: 0.0709 - accuracy: 0.9894 - val_loss: 0.1891 - val_accuracy: 0.9381 - lr: 3.0000e-04\n",
      "Epoch 10/20\n",
      "581/581 [==============================] - 152s 262ms/step - loss: 0.0680 - accuracy: 0.9917 - val_loss: 0.0970 - val_accuracy: 0.9819 - lr: 3.0000e-04\n",
      "Epoch 11/20\n",
      "581/581 [==============================] - 152s 262ms/step - loss: 0.0593 - accuracy: 0.9928 - val_loss: 0.1648 - val_accuracy: 0.9566 - lr: 3.0000e-04\n",
      "Epoch 12/20\n",
      "581/581 [==============================] - 152s 262ms/step - loss: 0.0564 - accuracy: 0.9936 - val_loss: 0.0972 - val_accuracy: 0.9772 - lr: 3.0000e-04\n",
      "Epoch 13/20\n",
      "581/581 [==============================] - 152s 261ms/step - loss: 0.0578 - accuracy: 0.9933 - val_loss: 0.1224 - val_accuracy: 0.9600 - lr: 3.0000e-04\n",
      "Epoch 14/20\n",
      "581/581 [==============================] - 152s 262ms/step - loss: 0.0676 - accuracy: 0.9919 - val_loss: 0.1982 - val_accuracy: 0.9587 - lr: 3.0000e-04\n",
      "Epoch 15/20\n",
      "581/581 [==============================] - 152s 262ms/step - loss: 0.0745 - accuracy: 0.9966 - val_loss: 0.1326 - val_accuracy: 0.9647 - lr: 3.0000e-04\n",
      "Epoch 16/20\n",
      "581/581 [==============================] - 152s 262ms/step - loss: 0.0614 - accuracy: 0.9945 - val_loss: 0.1713 - val_accuracy: 0.9506 - lr: 3.0000e-04\n",
      "Epoch 17/20\n",
      "581/581 [==============================] - 152s 262ms/step - loss: 0.0653 - accuracy: 0.9932 - val_loss: 0.2163 - val_accuracy: 0.9334 - lr: 3.0000e-04\n",
      "Epoch 18/20\n",
      "581/581 [==============================] - 152s 262ms/step - loss: 0.0500 - accuracy: 0.9966 - val_loss: 0.0835 - val_accuracy: 0.9802 - lr: 3.0000e-04\n",
      "Epoch 19/20\n",
      "581/581 [==============================] - 152s 262ms/step - loss: 0.0408 - accuracy: 0.9973 - val_loss: 0.0886 - val_accuracy: 0.9794 - lr: 3.0000e-04\n",
      "Epoch 20/20\n",
      "581/581 [==============================] - 152s 262ms/step - loss: 0.0687 - accuracy: 0.9959 - val_loss: 0.1729 - val_accuracy: 0.9712 - lr: 3.0000e-04\n",
      "Epoch 1/20\n",
      "581/581 [==============================] - 159s 265ms/step - loss: 0.8577 - accuracy: 0.8311 - val_loss: 0.5888 - val_accuracy: 0.8310 - lr: 3.0000e-04\n",
      "Epoch 2/20\n",
      "581/581 [==============================] - 153s 263ms/step - loss: 0.4150 - accuracy: 0.8990 - val_loss: 0.3032 - val_accuracy: 0.9299 - lr: 3.0000e-04\n",
      "Epoch 3/20\n",
      "581/581 [==============================] - 153s 263ms/step - loss: 0.2426 - accuracy: 0.9463 - val_loss: 0.1983 - val_accuracy: 0.9617 - lr: 3.0000e-04\n",
      "Epoch 4/20\n",
      "581/581 [==============================] - 153s 263ms/step - loss: 0.1642 - accuracy: 0.9696 - val_loss: 0.1764 - val_accuracy: 0.9635 - lr: 3.0000e-04\n",
      "Epoch 5/20\n",
      "581/581 [==============================] - 153s 263ms/step - loss: 0.1342 - accuracy: 0.9770 - val_loss: 0.0805 - val_accuracy: 0.9953 - lr: 3.0000e-04\n",
      "Epoch 6/20\n",
      "581/581 [==============================] - 153s 263ms/step - loss: 0.1112 - accuracy: 0.9833 - val_loss: 0.1015 - val_accuracy: 0.9867 - lr: 3.0000e-04\n",
      "Epoch 7/20\n",
      "581/581 [==============================] - 153s 263ms/step - loss: 0.0845 - accuracy: 0.9879 - val_loss: 0.0827 - val_accuracy: 0.9884 - lr: 3.0000e-04\n",
      "Epoch 8/20\n",
      "581/581 [==============================] - 153s 263ms/step - loss: 0.0722 - accuracy: 0.9913 - val_loss: 0.0721 - val_accuracy: 0.9914 - lr: 3.0000e-04\n",
      "Epoch 9/20\n",
      "581/581 [==============================] - 153s 263ms/step - loss: 0.0706 - accuracy: 0.9919 - val_loss: 0.0505 - val_accuracy: 0.9948 - lr: 3.0000e-04\n",
      "Epoch 10/20\n",
      "581/581 [==============================] - 152s 262ms/step - loss: 0.0672 - accuracy: 0.9926 - val_loss: 0.0686 - val_accuracy: 0.9897 - lr: 3.0000e-04\n",
      "Epoch 11/20\n",
      "581/581 [==============================] - 152s 262ms/step - loss: 0.0571 - accuracy: 0.9943 - val_loss: 0.0562 - val_accuracy: 0.9961 - lr: 3.0000e-04\n",
      "Epoch 12/20\n",
      "581/581 [==============================] - 152s 262ms/step - loss: 0.0512 - accuracy: 0.9952 - val_loss: 0.0353 - val_accuracy: 0.9979 - lr: 3.0000e-04\n",
      "Epoch 13/20\n",
      "581/581 [==============================] - 152s 262ms/step - loss: 0.0533 - accuracy: 0.9946 - val_loss: 0.1206 - val_accuracy: 0.9768 - lr: 3.0000e-04\n",
      "Epoch 14/20\n",
      "581/581 [==============================] - 152s 262ms/step - loss: 0.0468 - accuracy: 0.9960 - val_loss: 0.0438 - val_accuracy: 0.9987 - lr: 3.0000e-04\n",
      "Epoch 15/20\n",
      "581/581 [==============================] - 152s 261ms/step - loss: 0.0509 - accuracy: 0.9949 - val_loss: 0.1594 - val_accuracy: 0.9686 - lr: 3.0000e-04\n",
      "Epoch 16/20\n",
      "581/581 [==============================] - 152s 261ms/step - loss: 0.0468 - accuracy: 0.9956 - val_loss: 0.0363 - val_accuracy: 0.9987 - lr: 3.0000e-04\n",
      "Epoch 17/20\n",
      "581/581 [==============================] - 152s 262ms/step - loss: 0.0335 - accuracy: 0.9978 - val_loss: 0.0286 - val_accuracy: 0.9987 - lr: 3.0000e-04\n",
      "Epoch 18/20\n",
      "581/581 [==============================] - 152s 262ms/step - loss: 0.0347 - accuracy: 0.9973 - val_loss: 0.0292 - val_accuracy: 0.9987 - lr: 3.0000e-04\n",
      "Epoch 19/20\n",
      "581/581 [==============================] - 152s 261ms/step - loss: 0.0341 - accuracy: 0.9968 - val_loss: 0.0516 - val_accuracy: 0.9910 - lr: 3.0000e-04\n",
      "Epoch 20/20\n",
      "581/581 [==============================] - 152s 261ms/step - loss: 0.1355 - accuracy: 0.9777 - val_loss: 1.1463 - val_accuracy: 0.8336 - lr: 3.0000e-04\n",
      "Epoch 1/20\n",
      "581/581 [==============================] - 159s 265ms/step - loss: 0.8362 - accuracy: 0.8240 - val_loss: 0.4484 - val_accuracy: 0.8925 - lr: 3.0000e-04\n",
      "Epoch 2/20\n",
      "581/581 [==============================] - 153s 264ms/step - loss: 0.4071 - accuracy: 0.8881 - val_loss: 0.3119 - val_accuracy: 0.9166 - lr: 3.0000e-04\n",
      "Epoch 3/20\n",
      "581/581 [==============================] - 153s 263ms/step - loss: 0.2706 - accuracy: 0.9313 - val_loss: 0.1789 - val_accuracy: 0.9673 - lr: 3.0000e-04\n",
      "Epoch 4/20\n",
      "581/581 [==============================] - 153s 263ms/step - loss: 0.1717 - accuracy: 0.9637 - val_loss: 0.1302 - val_accuracy: 0.9828 - lr: 3.0000e-04\n",
      "Epoch 5/20\n",
      "581/581 [==============================] - 153s 263ms/step - loss: 0.4038 - accuracy: 0.9288 - val_loss: 0.3068 - val_accuracy: 0.9755 - lr: 3.0000e-04\n",
      "Epoch 6/20\n",
      "581/581 [==============================] - 153s 263ms/step - loss: 0.2556 - accuracy: 0.9764 - val_loss: 0.1959 - val_accuracy: 0.9931 - lr: 3.0000e-04\n",
      "Epoch 7/20\n",
      "581/581 [==============================] - 153s 263ms/step - loss: 0.1887 - accuracy: 0.9850 - val_loss: 0.1486 - val_accuracy: 0.9931 - lr: 3.0000e-04\n",
      "Epoch 8/20\n",
      "581/581 [==============================] - 153s 263ms/step - loss: 0.1816 - accuracy: 0.9828 - val_loss: 0.1564 - val_accuracy: 0.9875 - lr: 3.0000e-04\n",
      "Epoch 9/20\n",
      "581/581 [==============================] - 153s 263ms/step - loss: 0.1297 - accuracy: 0.9920 - val_loss: 0.0988 - val_accuracy: 0.9970 - lr: 3.0000e-04\n",
      "Epoch 10/20\n",
      "581/581 [==============================] - 153s 263ms/step - loss: 0.1036 - accuracy: 0.9940 - val_loss: 0.1086 - val_accuracy: 0.9918 - lr: 3.0000e-04\n",
      "Epoch 11/20\n",
      "581/581 [==============================] - 153s 263ms/step - loss: 0.0896 - accuracy: 0.9934 - val_loss: 0.0736 - val_accuracy: 0.9948 - lr: 3.0000e-04\n",
      "Epoch 12/20\n",
      "581/581 [==============================] - 153s 263ms/step - loss: 0.0652 - accuracy: 0.9957 - val_loss: 0.0764 - val_accuracy: 0.9910 - lr: 3.0000e-04\n",
      "Epoch 13/20\n",
      "581/581 [==============================] - 153s 263ms/step - loss: 0.0681 - accuracy: 0.9944 - val_loss: 0.0683 - val_accuracy: 0.9957 - lr: 3.0000e-04\n",
      "Epoch 14/20\n",
      "581/581 [==============================] - 153s 263ms/step - loss: 0.0564 - accuracy: 0.9958 - val_loss: 0.0511 - val_accuracy: 0.9987 - lr: 3.0000e-04\n",
      "Epoch 15/20\n",
      "581/581 [==============================] - 152s 262ms/step - loss: 0.0555 - accuracy: 0.9958 - val_loss: 0.0448 - val_accuracy: 0.9979 - lr: 3.0000e-04\n",
      "Epoch 16/20\n",
      "581/581 [==============================] - 152s 262ms/step - loss: 0.0407 - accuracy: 0.9977 - val_loss: 0.1061 - val_accuracy: 0.9725 - lr: 3.0000e-04\n",
      "Epoch 17/20\n",
      "581/581 [==============================] - 152s 262ms/step - loss: 0.0472 - accuracy: 0.9962 - val_loss: 0.0316 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 18/20\n",
      "581/581 [==============================] - 152s 262ms/step - loss: 0.0416 - accuracy: 0.9968 - val_loss: 0.0617 - val_accuracy: 0.9905 - lr: 3.0000e-04\n",
      "Epoch 19/20\n",
      "581/581 [==============================] - 152s 262ms/step - loss: 0.0401 - accuracy: 0.9982 - val_loss: 0.0387 - val_accuracy: 0.9970 - lr: 3.0000e-04\n",
      "Epoch 20/20\n",
      "581/581 [==============================] - 153s 263ms/step - loss: 0.0514 - accuracy: 0.9963 - val_loss: 0.0334 - val_accuracy: 0.9979 - lr: 3.0000e-04\n",
      "Epoch 1/20\n",
      "581/581 [==============================] - 159s 265ms/step - loss: 0.8285 - accuracy: 0.8293 - val_loss: 0.4829 - val_accuracy: 0.9024 - lr: 3.0000e-04\n",
      "Epoch 2/20\n",
      "581/581 [==============================] - 153s 264ms/step - loss: 0.4082 - accuracy: 0.8874 - val_loss: 0.4034 - val_accuracy: 0.8568 - lr: 3.0000e-04\n",
      "Epoch 3/20\n",
      "581/581 [==============================] - 153s 264ms/step - loss: 0.2631 - accuracy: 0.9385 - val_loss: 0.3555 - val_accuracy: 0.8917 - lr: 3.0000e-04\n",
      "Epoch 4/20\n",
      "581/581 [==============================] - 153s 263ms/step - loss: 0.1596 - accuracy: 0.9696 - val_loss: 0.2663 - val_accuracy: 0.9248 - lr: 3.0000e-04\n",
      "Epoch 5/20\n",
      "581/581 [==============================] - 153s 263ms/step - loss: 0.1134 - accuracy: 0.9830 - val_loss: 0.1849 - val_accuracy: 0.9549 - lr: 3.0000e-04\n",
      "Epoch 6/20\n",
      "581/581 [==============================] - 153s 263ms/step - loss: 0.0793 - accuracy: 0.9899 - val_loss: 0.2428 - val_accuracy: 0.9265 - lr: 3.0000e-04\n",
      "Epoch 7/20\n",
      "581/581 [==============================] - 153s 263ms/step - loss: 0.0651 - accuracy: 0.9922 - val_loss: 0.2559 - val_accuracy: 0.9364 - lr: 3.0000e-04\n",
      "Epoch 8/20\n",
      "581/581 [==============================] - 153s 263ms/step - loss: 0.0707 - accuracy: 0.9900 - val_loss: 0.1960 - val_accuracy: 0.9454 - lr: 3.0000e-04\n",
      "Epoch 9/20\n",
      "581/581 [==============================] - 153s 263ms/step - loss: 0.0553 - accuracy: 0.9947 - val_loss: 0.3634 - val_accuracy: 0.8783 - lr: 3.0000e-04\n",
      "Epoch 10/20\n",
      "581/581 [==============================] - 152s 261ms/step - loss: 0.0471 - accuracy: 0.9949 - val_loss: 0.1415 - val_accuracy: 0.9579 - lr: 3.0000e-04\n",
      "Epoch 11/20\n",
      "581/581 [==============================] - 152s 262ms/step - loss: 0.0601 - accuracy: 0.9929 - val_loss: 0.2768 - val_accuracy: 0.9093 - lr: 3.0000e-04\n",
      "Epoch 12/20\n",
      "581/581 [==============================] - 152s 262ms/step - loss: 0.0512 - accuracy: 0.9945 - val_loss: 0.1228 - val_accuracy: 0.9600 - lr: 3.0000e-04\n",
      "Epoch 13/20\n",
      "581/581 [==============================] - 152s 262ms/step - loss: 0.0390 - accuracy: 0.9964 - val_loss: 0.2008 - val_accuracy: 0.9398 - lr: 3.0000e-04\n",
      "Epoch 14/20\n",
      "581/581 [==============================] - 152s 261ms/step - loss: 0.0401 - accuracy: 0.9970 - val_loss: 0.0921 - val_accuracy: 0.9725 - lr: 3.0000e-04\n",
      "Epoch 15/20\n",
      "581/581 [==============================] - 153s 264ms/step - loss: 0.0392 - accuracy: 0.9959 - val_loss: 0.0999 - val_accuracy: 0.9725 - lr: 3.0000e-04\n",
      "Epoch 16/20\n",
      "581/581 [==============================] - 153s 263ms/step - loss: 0.0401 - accuracy: 0.9963 - val_loss: 0.2746 - val_accuracy: 0.9213 - lr: 3.0000e-04\n",
      "Epoch 17/20\n",
      "581/581 [==============================] - 152s 261ms/step - loss: 0.0363 - accuracy: 0.9974 - val_loss: 0.1251 - val_accuracy: 0.9656 - lr: 3.0000e-04\n",
      "Epoch 18/20\n",
      "581/581 [==============================] - 154s 265ms/step - loss: 0.0193 - accuracy: 0.9998 - val_loss: 0.0864 - val_accuracy: 0.9811 - lr: 3.0000e-04\n",
      "Epoch 19/20\n",
      "581/581 [==============================] - 154s 265ms/step - loss: 0.1260 - accuracy: 0.9887 - val_loss: 0.1731 - val_accuracy: 0.9712 - lr: 3.0000e-04\n",
      "Epoch 20/20\n",
      "581/581 [==============================] - 154s 264ms/step - loss: 0.1982 - accuracy: 0.9852 - val_loss: 0.3348 - val_accuracy: 0.9338 - lr: 3.0000e-04\n",
      "Epoch 1/20\n",
      "581/581 [==============================] - 159s 267ms/step - loss: 0.7940 - accuracy: 0.8618 - val_loss: 0.5783 - val_accuracy: 0.8379 - lr: 3.0000e-04\n",
      "Epoch 2/20\n",
      "581/581 [==============================] - 154s 266ms/step - loss: 0.3837 - accuracy: 0.9087 - val_loss: 0.4851 - val_accuracy: 0.8306 - lr: 3.0000e-04\n",
      "Epoch 3/20\n",
      "581/581 [==============================] - 154s 265ms/step - loss: 0.2681 - accuracy: 0.9359 - val_loss: 0.3508 - val_accuracy: 0.8955 - lr: 3.0000e-04\n",
      "Epoch 4/20\n",
      "581/581 [==============================] - 154s 265ms/step - loss: 0.2141 - accuracy: 0.9495 - val_loss: 0.2454 - val_accuracy: 0.9351 - lr: 3.0000e-04\n",
      "Epoch 5/20\n",
      "581/581 [==============================] - 154s 265ms/step - loss: 0.1265 - accuracy: 0.9762 - val_loss: 0.2216 - val_accuracy: 0.9454 - lr: 3.0000e-04\n",
      "Epoch 6/20\n",
      "581/581 [==============================] - 154s 265ms/step - loss: 0.0923 - accuracy: 0.9849 - val_loss: 0.1729 - val_accuracy: 0.9527 - lr: 3.0000e-04\n",
      "Epoch 7/20\n",
      "581/581 [==============================] - 153s 264ms/step - loss: 0.0833 - accuracy: 0.9861 - val_loss: 0.5488 - val_accuracy: 0.8104 - lr: 3.0000e-04\n",
      "Epoch 8/20\n",
      "581/581 [==============================] - 153s 264ms/step - loss: 0.2583 - accuracy: 0.9690 - val_loss: 0.7221 - val_accuracy: 0.8220 - lr: 3.0000e-04\n",
      "Epoch 9/20\n",
      "581/581 [==============================] - 153s 264ms/step - loss: 0.2944 - accuracy: 0.9817 - val_loss: 0.4682 - val_accuracy: 0.9183 - lr: 3.0000e-04\n",
      "Epoch 10/20\n",
      "581/581 [==============================] - 154s 265ms/step - loss: 0.2103 - accuracy: 0.9915 - val_loss: 0.2992 - val_accuracy: 0.9450 - lr: 3.0000e-04\n",
      "Epoch 11/20\n",
      "581/581 [==============================] - 154s 264ms/step - loss: 0.1531 - accuracy: 0.9954 - val_loss: 0.2053 - val_accuracy: 0.9742 - lr: 3.0000e-04\n",
      "Epoch 12/20\n",
      "581/581 [==============================] - 153s 264ms/step - loss: 0.1272 - accuracy: 0.9961 - val_loss: 0.3733 - val_accuracy: 0.9342 - lr: 3.0000e-04\n",
      "Epoch 13/20\n",
      "581/581 [==============================] - 153s 264ms/step - loss: 0.1105 - accuracy: 0.9957 - val_loss: 0.2033 - val_accuracy: 0.9553 - lr: 3.0000e-04\n",
      "Epoch 14/20\n",
      "581/581 [==============================] - 153s 263ms/step - loss: 0.3496 - accuracy: 0.9860 - val_loss: 0.3510 - val_accuracy: 0.9742 - lr: 3.0000e-04\n",
      "Epoch 15/20\n",
      "581/581 [==============================] - 154s 264ms/step - loss: 0.1733 - accuracy: 0.9973 - val_loss: 0.2098 - val_accuracy: 0.9690 - lr: 3.0000e-04\n",
      "Epoch 16/20\n",
      "581/581 [==============================] - 154s 264ms/step - loss: 0.0804 - accuracy: 0.9994 - val_loss: 0.2111 - val_accuracy: 0.9635 - lr: 3.0000e-04\n",
      "Epoch 17/20\n",
      "581/581 [==============================] - 153s 264ms/step - loss: 0.0624 - accuracy: 0.9999 - val_loss: 0.1689 - val_accuracy: 0.9703 - lr: 6.0000e-05\n",
      "Epoch 18/20\n",
      "581/581 [==============================] - 153s 264ms/step - loss: 0.0601 - accuracy: 0.9997 - val_loss: 0.1558 - val_accuracy: 0.9708 - lr: 6.0000e-05\n",
      "Epoch 19/20\n",
      "581/581 [==============================] - 153s 264ms/step - loss: 0.0546 - accuracy: 0.9998 - val_loss: 0.1489 - val_accuracy: 0.9699 - lr: 6.0000e-05\n",
      "Epoch 20/20\n",
      "581/581 [==============================] - 153s 264ms/step - loss: 0.0487 - accuracy: 0.9998 - val_loss: 0.1678 - val_accuracy: 0.9622 - lr: 6.0000e-05\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "batch_size = 16\n",
    "\n",
    "val_acc_per_fold = []\n",
    "acc_per_fold = []\n",
    "val_loss_per_fold = []\n",
    "loss_per_fold = []\n",
    "sets = [(fold_1, fold_1_lab), (fold_2, fold_2_lab), (fold_3, fold_3_lab), (fold_4, fold_4_lab), (fold_5, fold_5_lab)]\n",
    "\n",
    "for n in range(len(sets)):\n",
    "    test_x = sets[n][0]\n",
    "    test_y = sets[n][1]\n",
    "    train_x = pd.DataFrame()\n",
    "    train_y = pd.DataFrame()\n",
    "    for s in sets[:n] + sets[n+1:]:\n",
    "        train_x = pd.concat([train_x, s[0]])\n",
    "        train_y = pd.concat([train_y, s[1]])\n",
    "\n",
    "    #print(test_x.shape[0])\n",
    "    #print(fold_1.shape[0]*4)\n",
    "    classifier = build_model()\n",
    "\n",
    "    \n",
    "    history = classifier.fit(\n",
    "        np.array(train_x), np.array(train_y),\n",
    "        epochs=epochs,\n",
    "        batch_size = batch_size,\n",
    "        steps_per_epoch=(train_x.shape[0] // batch_size),\n",
    "        validation_data=(np.array(test_x), np.array(test_y)),\n",
    "        callbacks=[tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=10, min_lr=0.00003)]\n",
    "    )\n",
    "\n",
    "    val_acc_per_fold.append(history.history['val_accuracy'])\n",
    "    acc_per_fold.append(history.history['accuracy'])\n",
    "    val_loss_per_fold.append(history.history['val_loss'])\n",
    "    loss_per_fold.append(history.history['loss'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[0.8159931302070618,\n",
       "   0.8779019713401794,\n",
       "   0.929492712020874,\n",
       "   0.9161651134490967,\n",
       "   0.9290627837181091,\n",
       "   0.9157351851463318,\n",
       "   0.9436801671981812,\n",
       "   0.9217540621757507,\n",
       "   0.9380911588668823,\n",
       "   0.9819432497024536,\n",
       "   0.9565778374671936,\n",
       "   0.9772140979766846,\n",
       "   0.960017204284668,\n",
       "   0.9587274193763733,\n",
       "   0.964746356010437,\n",
       "   0.9505589008331299,\n",
       "   0.9333620071411133,\n",
       "   0.980223536491394,\n",
       "   0.979363739490509,\n",
       "   0.9711951613426208],\n",
       "  [0.8310404419898987,\n",
       "   0.9299226403236389,\n",
       "   0.9617368578910828,\n",
       "   0.9634565711021423,\n",
       "   0.995270848274231,\n",
       "   0.9866724014282227,\n",
       "   0.9883921146392822,\n",
       "   0.9914015531539917,\n",
       "   0.9948409199714661,\n",
       "   0.9896818399429321,\n",
       "   0.9961307048797607,\n",
       "   0.9978503584861755,\n",
       "   0.9767841696739197,\n",
       "   0.9987102150917053,\n",
       "   0.9686156511306763,\n",
       "   0.9987102150917053,\n",
       "   0.9987102150917053,\n",
       "   0.9987102150917053,\n",
       "   0.9909716248512268,\n",
       "   0.8336199522018433],\n",
       "  [0.8925193548202515,\n",
       "   0.9165950417518616,\n",
       "   0.9673258662223816,\n",
       "   0.9828031063079834,\n",
       "   0.975494384765625,\n",
       "   0.9931212663650513,\n",
       "   0.9931212663650513,\n",
       "   0.9875322580337524,\n",
       "   0.9969905614852905,\n",
       "   0.9918314814567566,\n",
       "   0.9948409199714661,\n",
       "   0.9909716248512268,\n",
       "   0.9957007765769958,\n",
       "   0.9987102150917053,\n",
       "   0.9978503584861755,\n",
       "   0.9724849462509155,\n",
       "   1.0,\n",
       "   0.9905416965484619,\n",
       "   0.9969905614852905,\n",
       "   0.9978503584861755],\n",
       "  [0.9024075865745544,\n",
       "   0.8568357825279236,\n",
       "   0.8916594982147217,\n",
       "   0.924763560295105,\n",
       "   0.954858124256134,\n",
       "   0.9264832139015198,\n",
       "   0.9363714456558228,\n",
       "   0.945399820804596,\n",
       "   0.8783318996429443,\n",
       "   0.9578675627708435,\n",
       "   0.9092863202095032,\n",
       "   0.960017204284668,\n",
       "   0.9398108124732971,\n",
       "   0.9724849462509155,\n",
       "   0.9724849462509155,\n",
       "   0.9213241338729858,\n",
       "   0.9656062126159668,\n",
       "   0.9810833930969238,\n",
       "   0.9711951613426208,\n",
       "   0.9337919354438782],\n",
       "  [0.8379191756248474,\n",
       "   0.8306105136871338,\n",
       "   0.8955287933349609,\n",
       "   0.9350816607475281,\n",
       "   0.945399820804596,\n",
       "   0.9527084827423096,\n",
       "   0.8104041218757629,\n",
       "   0.8220120668411255,\n",
       "   0.9183146953582764,\n",
       "   0.944969892501831,\n",
       "   0.9742046594619751,\n",
       "   0.9342218637466431,\n",
       "   0.9552880525588989,\n",
       "   0.9742046594619751,\n",
       "   0.9690455794334412,\n",
       "   0.9634565711021423,\n",
       "   0.9703353643417358,\n",
       "   0.970765233039856,\n",
       "   0.969905436038971,\n",
       "   0.9621667861938477]],\n",
       " [[0.852302074432373,\n",
       "   0.907192051410675,\n",
       "   0.9479973912239075,\n",
       "   0.9632859826087952,\n",
       "   0.975882887840271,\n",
       "   0.9806201457977295,\n",
       "   0.9862187504768372,\n",
       "   0.9874030947685242,\n",
       "   0.9894487261772156,\n",
       "   0.9917097091674805,\n",
       "   0.9927864074707031,\n",
       "   0.9936476945877075,\n",
       "   0.9933246970176697,\n",
       "   0.991925060749054,\n",
       "   0.9965546727180481,\n",
       "   0.9945090413093567,\n",
       "   0.9932170510292053,\n",
       "   0.9965546727180481,\n",
       "   0.9973083734512329,\n",
       "   0.9959086775779724],\n",
       "  [0.8311101794242859,\n",
       "   0.8990094661712646,\n",
       "   0.9462747573852539,\n",
       "   0.9696382284164429,\n",
       "   0.9769595265388489,\n",
       "   0.9833117723464966,\n",
       "   0.9879414439201355,\n",
       "   0.9912790656089783,\n",
       "   0.991925060749054,\n",
       "   0.9925710558891296,\n",
       "   0.9942936897277832,\n",
       "   0.9951550364494324,\n",
       "   0.994616687297821,\n",
       "   0.9960163831710815,\n",
       "   0.9949396848678589,\n",
       "   0.9955856800079346,\n",
       "   0.9978466629981995,\n",
       "   0.9973083734512329,\n",
       "   0.9967700242996216,\n",
       "   0.9777131676673889],\n",
       "  [0.8240103125572205,\n",
       "   0.8881352543830872,\n",
       "   0.9313092231750488,\n",
       "   0.9637166261672974,\n",
       "   0.9288328886032104,\n",
       "   0.9764211773872375,\n",
       "   0.9850344657897949,\n",
       "   0.98277348279953,\n",
       "   0.9920327067375183,\n",
       "   0.9939706921577454,\n",
       "   0.9934324026107788,\n",
       "   0.9956933856010437,\n",
       "   0.9944013953208923,\n",
       "   0.9958010315895081,\n",
       "   0.9958010315895081,\n",
       "   0.9977390170097351,\n",
       "   0.9962316751480103,\n",
       "   0.9967700242996216,\n",
       "   0.9981696605682373,\n",
       "   0.9963393807411194],\n",
       "  [0.8292813897132874,\n",
       "   0.8873815536499023,\n",
       "   0.9385228157043457,\n",
       "   0.9696382284164429,\n",
       "   0.9829887747764587,\n",
       "   0.9898794293403625,\n",
       "   0.9922480583190918,\n",
       "   0.9899870753288269,\n",
       "   0.9947243928909302,\n",
       "   0.9949396848678589,\n",
       "   0.9928940534591675,\n",
       "   0.9945090413093567,\n",
       "   0.9964470267295837,\n",
       "   0.9969853758811951,\n",
       "   0.9959086775779724,\n",
       "   0.9963393807411194,\n",
       "   0.9974160194396973,\n",
       "   0.9997846484184265,\n",
       "   0.9886950850486755,\n",
       "   0.9852497577667236],\n",
       "  [0.8617684841156006,\n",
       "   0.9086993932723999,\n",
       "   0.935938835144043,\n",
       "   0.9495047330856323,\n",
       "   0.9762058854103088,\n",
       "   0.9849267601966858,\n",
       "   0.9861111044883728,\n",
       "   0.9689922332763672,\n",
       "   0.9816967844963074,\n",
       "   0.9914944171905518,\n",
       "   0.9953703880310059,\n",
       "   0.9961240291595459,\n",
       "   0.9956933856010437,\n",
       "   0.9860034584999084,\n",
       "   0.9973083734512329,\n",
       "   0.9993540048599243,\n",
       "   0.9998923540115356,\n",
       "   0.9996770024299622,\n",
       "   0.9997846484184265,\n",
       "   0.9997846484184265]],\n",
       " [[0.7482413053512573,\n",
       "   0.4770042300224304,\n",
       "   0.2778811454772949,\n",
       "   0.3109320402145386,\n",
       "   0.2375577986240387,\n",
       "   0.29854053258895874,\n",
       "   0.18083065748214722,\n",
       "   0.21075251698493958,\n",
       "   0.1890893280506134,\n",
       "   0.09699669480323792,\n",
       "   0.16481423377990723,\n",
       "   0.09724126011133194,\n",
       "   0.12241443991661072,\n",
       "   0.1981644630432129,\n",
       "   0.13260042667388916,\n",
       "   0.17129114270210266,\n",
       "   0.2163286656141281,\n",
       "   0.08349944651126862,\n",
       "   0.08855684101581573,\n",
       "   0.17288410663604736],\n",
       "  [0.5887907147407532,\n",
       "   0.30321425199508667,\n",
       "   0.19828088581562042,\n",
       "   0.17635056376457214,\n",
       "   0.08049933612346649,\n",
       "   0.10153188556432724,\n",
       "   0.08266962319612503,\n",
       "   0.07208389788866043,\n",
       "   0.050518348813056946,\n",
       "   0.06861530989408493,\n",
       "   0.056161198765039444,\n",
       "   0.03525108844041824,\n",
       "   0.12063110619783401,\n",
       "   0.04380643367767334,\n",
       "   0.1594395488500595,\n",
       "   0.03626697510480881,\n",
       "   0.028648993000388145,\n",
       "   0.029240939766168594,\n",
       "   0.051599327474832535,\n",
       "   1.146278977394104],\n",
       "  [0.4483628273010254,\n",
       "   0.3118959665298462,\n",
       "   0.17885850369930267,\n",
       "   0.13021114468574524,\n",
       "   0.306847482919693,\n",
       "   0.1959361732006073,\n",
       "   0.1485849767923355,\n",
       "   0.15644471347332,\n",
       "   0.0988454818725586,\n",
       "   0.10863734781742096,\n",
       "   0.07362082600593567,\n",
       "   0.076409712433815,\n",
       "   0.06830214709043503,\n",
       "   0.05105581507086754,\n",
       "   0.04481073468923569,\n",
       "   0.10614990442991257,\n",
       "   0.031551294028759,\n",
       "   0.06169668212532997,\n",
       "   0.03869578614830971,\n",
       "   0.033379606902599335],\n",
       "  [0.48294275999069214,\n",
       "   0.40340346097946167,\n",
       "   0.35548123717308044,\n",
       "   0.2663190960884094,\n",
       "   0.18487301468849182,\n",
       "   0.24279063940048218,\n",
       "   0.25588127970695496,\n",
       "   0.19603213667869568,\n",
       "   0.3633790612220764,\n",
       "   0.14147479832172394,\n",
       "   0.2768261134624481,\n",
       "   0.12275281548500061,\n",
       "   0.20082220435142517,\n",
       "   0.09210941940546036,\n",
       "   0.09985672682523727,\n",
       "   0.2746190130710602,\n",
       "   0.12508827447891235,\n",
       "   0.0863739401102066,\n",
       "   0.17307458817958832,\n",
       "   0.33482426404953003],\n",
       "  [0.5782667398452759,\n",
       "   0.48507168889045715,\n",
       "   0.35079580545425415,\n",
       "   0.24536781013011932,\n",
       "   0.22163867950439453,\n",
       "   0.17292481660842896,\n",
       "   0.5488435626029968,\n",
       "   0.7221089601516724,\n",
       "   0.4681550860404968,\n",
       "   0.29915180802345276,\n",
       "   0.2052919715642929,\n",
       "   0.3732520341873169,\n",
       "   0.2032937854528427,\n",
       "   0.351010262966156,\n",
       "   0.209771990776062,\n",
       "   0.21105195581912994,\n",
       "   0.1688884049654007,\n",
       "   0.15579158067703247,\n",
       "   0.1489114612340927,\n",
       "   0.16780507564544678]],\n",
       " [[0.8190634846687317,\n",
       "   0.42691734433174133,\n",
       "   0.25403162837028503,\n",
       "   0.1825668066740036,\n",
       "   0.13331805169582367,\n",
       "   0.11597879976034164,\n",
       "   0.09782949090003967,\n",
       "   0.08680516481399536,\n",
       "   0.07090231776237488,\n",
       "   0.06796244531869888,\n",
       "   0.05931440368294716,\n",
       "   0.056424591690301895,\n",
       "   0.05779945105314255,\n",
       "   0.0675516352057457,\n",
       "   0.07446633279323578,\n",
       "   0.06139906868338585,\n",
       "   0.06531805545091629,\n",
       "   0.04996764287352562,\n",
       "   0.04078279063105583,\n",
       "   0.06868509203195572],\n",
       "  [0.8577018976211548,\n",
       "   0.4150402545928955,\n",
       "   0.24257610738277435,\n",
       "   0.16420021653175354,\n",
       "   0.13415031135082245,\n",
       "   0.11123088002204895,\n",
       "   0.08445842564105988,\n",
       "   0.07219776511192322,\n",
       "   0.07058944553136826,\n",
       "   0.06719112396240234,\n",
       "   0.05712313577532768,\n",
       "   0.05118907615542412,\n",
       "   0.0533224381506443,\n",
       "   0.04682415351271629,\n",
       "   0.05090079829096794,\n",
       "   0.046821776777505875,\n",
       "   0.03349004685878754,\n",
       "   0.03471912071108818,\n",
       "   0.0341314934194088,\n",
       "   0.1354820877313614],\n",
       "  [0.836226761341095,\n",
       "   0.40708011388778687,\n",
       "   0.27056145668029785,\n",
       "   0.17170774936676025,\n",
       "   0.40375837683677673,\n",
       "   0.2556173503398895,\n",
       "   0.18866285681724548,\n",
       "   0.18155434727668762,\n",
       "   0.1297195851802826,\n",
       "   0.10360637307167053,\n",
       "   0.08962109684944153,\n",
       "   0.06519892811775208,\n",
       "   0.06806235760450363,\n",
       "   0.056356970220804214,\n",
       "   0.055511992424726486,\n",
       "   0.04070761427283287,\n",
       "   0.04722268134355545,\n",
       "   0.04155503213405609,\n",
       "   0.04008175805211067,\n",
       "   0.051421742886304855],\n",
       "  [0.8285316228866577,\n",
       "   0.40818142890930176,\n",
       "   0.26305025815963745,\n",
       "   0.15956667065620422,\n",
       "   0.11336860060691833,\n",
       "   0.07929468154907227,\n",
       "   0.06513911485671997,\n",
       "   0.07074858993291855,\n",
       "   0.055293403565883636,\n",
       "   0.04706067591905594,\n",
       "   0.060125693678855896,\n",
       "   0.05116409808397293,\n",
       "   0.03896490857005119,\n",
       "   0.04014488682150841,\n",
       "   0.03917394578456879,\n",
       "   0.04006857052445412,\n",
       "   0.036293283104896545,\n",
       "   0.01929033175110817,\n",
       "   0.12598055601119995,\n",
       "   0.19820871949195862],\n",
       "  [0.7940203547477722,\n",
       "   0.3836504817008972,\n",
       "   0.268146812915802,\n",
       "   0.2141486555337906,\n",
       "   0.12650154531002045,\n",
       "   0.09232256561517715,\n",
       "   0.0832914412021637,\n",
       "   0.25828927755355835,\n",
       "   0.2943619191646576,\n",
       "   0.2102823108434677,\n",
       "   0.1530812829732895,\n",
       "   0.12724852561950684,\n",
       "   0.11046592146158218,\n",
       "   0.3496488034725189,\n",
       "   0.17329731583595276,\n",
       "   0.08040645718574524,\n",
       "   0.06244737654924393,\n",
       "   0.06008773297071457,\n",
       "   0.05455155298113823,\n",
       "   0.048667777329683304]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_acc_per_fold, acc_per_fold, val_loss_per_fold, loss_per_fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'val_acc_per_fold' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpickle\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m results \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mval_acc_per_fold\u001b[39m\u001b[39m\"\u001b[39m: val_acc_per_fold, \u001b[39m\"\u001b[39m\u001b[39macc_per_fold\u001b[39m\u001b[39m\"\u001b[39m: acc_per_fold, \u001b[39m\"\u001b[39m\u001b[39mval_loss_per_fold\u001b[39m\u001b[39m\"\u001b[39m: val_loss_per_fold, \u001b[39m\"\u001b[39m\u001b[39mloss_per_fold\u001b[39m\u001b[39m\"\u001b[39m: loss_per_fold}\n\u001b[0;32m      5\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mresults_model2.pickle\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m file:\n\u001b[0;32m      6\u001b[0m  pickle\u001b[39m.\u001b[39mdump(results, file)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'val_acc_per_fold' is not defined"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "results = {\"val_acc_per_fold\": val_acc_per_fold, \"acc_per_fold\": acc_per_fold, \"val_loss_per_fold\": val_loss_per_fold, \"loss_per_fold\": loss_per_fold}\n",
    "\n",
    "with open('results_model2.pickle', 'wb') as file:\n",
    " pickle.dump(results, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "EOFError",
     "evalue": "Ran out of input",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEOFError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpickle\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mresults_model2.pickle\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m file:\n\u001b[1;32m----> 3\u001b[0m     data \u001b[39m=\u001b[39m pickle\u001b[39m.\u001b[39;49mload(file)\n",
      "\u001b[1;31mEOFError\u001b[0m: Ran out of input"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open('results_model2.pickle', 'rb') as file:\n",
    "    data = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9397248387336731"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(0.9711951613426208 + 0.8336199522018433 + 0.9978503584861755 + 0.9337919354438782 + 0.9621667861938477)/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling `save('my_model')` creates a SavedModel folder `my_model`.\n",
    "classifier.save(\"classifier\")\n",
    "\n",
    "# It can be used to reconstruct the model identically.\n",
    "#reconstructed_model = keras.models.load_model(\"classifier\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bidirectional RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_bidirectional():\n",
    "    hidden_dim = 512\n",
    "    seq_len = fold_1.shape[1]\n",
    "    n_features = 1\n",
    "    learning_rate = 0.0003\n",
    "\n",
    "    regularizers = {\n",
    "        'kernel_regularizer':tf.keras.regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
    "        'bias_regularizer':tf.keras.regularizers.l2(1e-4),\n",
    "        'activity_regularizer':tf.keras.regularizers.l2(1e-5)\n",
    "    }\n",
    "\n",
    "    regularizers_GRU = {\n",
    "        'kernel_regularizer':tf.keras.regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
    "        'bias_regularizer':tf.keras.regularizers.l2(1e-4),\n",
    "        'activity_regularizer':tf.keras.regularizers.l2(1e-5),\n",
    "        'recurrent_regularizer':tf.keras.regularizers.l2(1e-5)\n",
    "    }\n",
    "\n",
    "    in1 = tf.keras.layers.Input(shape=(seq_len, n_features))\n",
    "    gru1 = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(hidden_dim, name='latent_layer', **regularizers_GRU))(in1)\n",
    "    bn = tf.keras.layers.BatchNormalization()(gru1)\n",
    "    dense = tf.keras.layers.Dense(128, activation=\"relu\", **regularizers)(bn)\n",
    "    drop = tf.keras.layers.Dropout(0.3)(dense)\n",
    "    d1   = tf.keras.layers.Dense(1, activation='sigmoid', **regularizers)(drop)\n",
    "\n",
    "    classifier_test_opt_bi = tf.keras.Model(\n",
    "        inputs=[in1], \n",
    "        outputs=[d1]\n",
    "    )\n",
    "\n",
    "    print(classifier_test_opt_bi.summary())\n",
    "\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    classifier_test_opt_bi.compile(loss='binary_crossentropy', optimizer=opt, \n",
    "        metrics=['accuracy'])\n",
    "    \n",
    "    return classifier_test_opt_bi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 453, 1)]          0         \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 1024)             1582080   \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 1024)             4096      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               131200    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,717,505\n",
      "Trainable params: 1,715,457\n",
      "Non-trainable params: 2,048\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "581/581 [==============================] - 59s 97ms/step - loss: 0.5105 - accuracy: 0.8258 - val_loss: 0.4771 - val_accuracy: 0.8508 - lr: 3.0000e-04\n",
      "Epoch 2/20\n",
      "581/581 [==============================] - 56s 97ms/step - loss: 0.3637 - accuracy: 0.8884 - val_loss: 0.4535 - val_accuracy: 0.8766 - lr: 3.0000e-04\n",
      "Epoch 3/20\n",
      "581/581 [==============================] - 56s 97ms/step - loss: 0.2757 - accuracy: 0.9267 - val_loss: 0.2778 - val_accuracy: 0.9325 - lr: 3.0000e-04\n",
      "Epoch 4/20\n",
      "581/581 [==============================] - 58s 99ms/step - loss: 0.2205 - accuracy: 0.9452 - val_loss: 0.2356 - val_accuracy: 0.9359 - lr: 3.0000e-04\n",
      "Epoch 5/20\n",
      "581/581 [==============================] - 57s 98ms/step - loss: 0.1814 - accuracy: 0.9630 - val_loss: 0.3701 - val_accuracy: 0.9041 - lr: 3.0000e-04\n",
      "Epoch 6/20\n",
      "581/581 [==============================] - 56s 96ms/step - loss: 0.1615 - accuracy: 0.9679 - val_loss: 0.2437 - val_accuracy: 0.9278 - lr: 3.0000e-04\n",
      "Epoch 7/20\n",
      "581/581 [==============================] - 57s 98ms/step - loss: 0.1138 - accuracy: 0.9880 - val_loss: 0.1486 - val_accuracy: 0.9716 - lr: 6.0000e-05\n",
      "Epoch 8/20\n",
      "581/581 [==============================] - 58s 100ms/step - loss: 0.0985 - accuracy: 0.9925 - val_loss: 0.1377 - val_accuracy: 0.9721 - lr: 6.0000e-05\n",
      "Epoch 9/20\n",
      "581/581 [==============================] - 64s 110ms/step - loss: 0.0963 - accuracy: 0.9928 - val_loss: 0.1257 - val_accuracy: 0.9824 - lr: 6.0000e-05\n",
      "Epoch 10/20\n",
      "581/581 [==============================] - 64s 110ms/step - loss: 0.0878 - accuracy: 0.9953 - val_loss: 0.1478 - val_accuracy: 0.9712 - lr: 6.0000e-05\n",
      "Epoch 11/20\n",
      "581/581 [==============================] - 59s 102ms/step - loss: 0.0890 - accuracy: 0.9935 - val_loss: 0.1248 - val_accuracy: 0.9781 - lr: 6.0000e-05\n",
      "Epoch 12/20\n",
      "581/581 [==============================] - 64s 110ms/step - loss: 0.0856 - accuracy: 0.9944 - val_loss: 0.1442 - val_accuracy: 0.9725 - lr: 6.0000e-05\n",
      "Epoch 13/20\n",
      "581/581 [==============================] - 64s 111ms/step - loss: 0.0763 - accuracy: 0.9975 - val_loss: 0.1557 - val_accuracy: 0.9708 - lr: 6.0000e-05\n",
      "Epoch 14/20\n",
      "581/581 [==============================] - 63s 109ms/step - loss: 0.0744 - accuracy: 0.9976 - val_loss: 0.1286 - val_accuracy: 0.9798 - lr: 1.2000e-05\n",
      "Epoch 15/20\n",
      "581/581 [==============================] - 61s 106ms/step - loss: 0.0701 - accuracy: 0.9987 - val_loss: 0.1329 - val_accuracy: 0.9781 - lr: 1.2000e-05\n",
      "Epoch 16/20\n",
      "581/581 [==============================] - 63s 108ms/step - loss: 0.0705 - accuracy: 0.9990 - val_loss: 0.1340 - val_accuracy: 0.9764 - lr: 2.4000e-06\n",
      "Epoch 17/20\n",
      "581/581 [==============================] - 65s 111ms/step - loss: 0.0709 - accuracy: 0.9980 - val_loss: 0.1338 - val_accuracy: 0.9764 - lr: 2.4000e-06\n",
      "Epoch 18/20\n",
      "581/581 [==============================] - 61s 105ms/step - loss: 0.0700 - accuracy: 0.9988 - val_loss: 0.1359 - val_accuracy: 0.9751 - lr: 4.8000e-07\n",
      "Epoch 19/20\n",
      "581/581 [==============================] - 62s 106ms/step - loss: 0.0691 - accuracy: 0.9989 - val_loss: 0.1338 - val_accuracy: 0.9768 - lr: 4.8000e-07\n",
      "Epoch 20/20\n",
      "581/581 [==============================] - 59s 101ms/step - loss: 0.0699 - accuracy: 0.9985 - val_loss: 0.1341 - val_accuracy: 0.9764 - lr: 9.6000e-08\n",
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 453, 1)]          0         \n",
      "                                                                 \n",
      " bidirectional_2 (Bidirectio  (None, 1024)             1582080   \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 1024)             4096      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 128)               131200    \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,717,505\n",
      "Trainable params: 1,715,457\n",
      "Non-trainable params: 2,048\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "581/581 [==============================] - 66s 108ms/step - loss: 0.5462 - accuracy: 0.8151 - val_loss: 0.3821 - val_accuracy: 0.8994 - lr: 3.0000e-04\n",
      "Epoch 2/20\n",
      "581/581 [==============================] - 63s 108ms/step - loss: 0.4131 - accuracy: 0.8691 - val_loss: 0.2682 - val_accuracy: 0.9329 - lr: 3.0000e-04\n",
      "Epoch 3/20\n",
      "581/581 [==============================] - 63s 109ms/step - loss: 0.2757 - accuracy: 0.9275 - val_loss: 0.1827 - val_accuracy: 0.9617 - lr: 3.0000e-04\n",
      "Epoch 4/20\n",
      "581/581 [==============================] - 62s 106ms/step - loss: 0.3039 - accuracy: 0.9153 - val_loss: 0.2692 - val_accuracy: 0.9295 - lr: 3.0000e-04\n",
      "Epoch 5/20\n",
      "581/581 [==============================] - 57s 99ms/step - loss: 0.1892 - accuracy: 0.9585 - val_loss: 0.1148 - val_accuracy: 0.9884 - lr: 3.0000e-04\n",
      "Epoch 6/20\n",
      "581/581 [==============================] - 56s 97ms/step - loss: 0.1622 - accuracy: 0.9731 - val_loss: 0.1326 - val_accuracy: 0.9815 - lr: 3.0000e-04\n",
      "Epoch 7/20\n",
      "581/581 [==============================] - 58s 100ms/step - loss: 0.1274 - accuracy: 0.9823 - val_loss: 0.1166 - val_accuracy: 0.9850 - lr: 3.0000e-04\n",
      "Epoch 8/20\n",
      "581/581 [==============================] - 61s 105ms/step - loss: 0.1008 - accuracy: 0.9927 - val_loss: 0.0817 - val_accuracy: 0.9979 - lr: 6.0000e-05\n",
      "Epoch 9/20\n",
      "581/581 [==============================] - 61s 105ms/step - loss: 0.0926 - accuracy: 0.9944 - val_loss: 0.0770 - val_accuracy: 0.9996 - lr: 6.0000e-05\n",
      "Epoch 10/20\n",
      "581/581 [==============================] - 63s 108ms/step - loss: 0.0917 - accuracy: 0.9942 - val_loss: 0.0836 - val_accuracy: 0.9961 - lr: 6.0000e-05\n",
      "Epoch 11/20\n",
      "581/581 [==============================] - 65s 111ms/step - loss: 0.0883 - accuracy: 0.9952 - val_loss: 0.0753 - val_accuracy: 0.9983 - lr: 6.0000e-05\n",
      "Epoch 12/20\n",
      "581/581 [==============================] - 65s 112ms/step - loss: 0.0853 - accuracy: 0.9954 - val_loss: 0.0756 - val_accuracy: 0.9979 - lr: 6.0000e-05\n",
      "Epoch 13/20\n",
      "581/581 [==============================] - 61s 106ms/step - loss: 0.0818 - accuracy: 0.9968 - val_loss: 0.0708 - val_accuracy: 0.9991 - lr: 6.0000e-05\n",
      "Epoch 14/20\n",
      "581/581 [==============================] - 62s 106ms/step - loss: 0.0839 - accuracy: 0.9960 - val_loss: 0.0704 - val_accuracy: 0.9991 - lr: 6.0000e-05\n",
      "Epoch 15/20\n",
      "581/581 [==============================] - 58s 100ms/step - loss: 0.0784 - accuracy: 0.9961 - val_loss: 0.0705 - val_accuracy: 0.9983 - lr: 6.0000e-05\n",
      "Epoch 16/20\n",
      "581/581 [==============================] - 57s 98ms/step - loss: 0.0747 - accuracy: 0.9970 - val_loss: 0.0755 - val_accuracy: 0.9974 - lr: 6.0000e-05\n",
      "Epoch 17/20\n",
      "581/581 [==============================] - 58s 100ms/step - loss: 0.0713 - accuracy: 0.9980 - val_loss: 0.0688 - val_accuracy: 0.9987 - lr: 1.2000e-05\n",
      "Epoch 18/20\n",
      "581/581 [==============================] - 57s 99ms/step - loss: 0.0697 - accuracy: 0.9985 - val_loss: 0.0673 - val_accuracy: 0.9987 - lr: 1.2000e-05\n",
      "Epoch 19/20\n",
      "581/581 [==============================] - 56s 97ms/step - loss: 0.0704 - accuracy: 0.9977 - val_loss: 0.0687 - val_accuracy: 0.9983 - lr: 1.2000e-05\n",
      "Epoch 20/20\n",
      "581/581 [==============================] - 58s 99ms/step - loss: 0.0664 - accuracy: 0.9995 - val_loss: 0.0675 - val_accuracy: 0.9983 - lr: 1.2000e-05\n",
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 453, 1)]          0         \n",
      "                                                                 \n",
      " bidirectional_3 (Bidirectio  (None, 1024)             1582080   \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 1024)             4096      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 128)               131200    \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,717,505\n",
      "Trainable params: 1,715,457\n",
      "Non-trainable params: 2,048\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "581/581 [==============================] - 60s 98ms/step - loss: 0.5341 - accuracy: 0.8195 - val_loss: 0.4635 - val_accuracy: 0.8156 - lr: 3.0000e-04\n",
      "Epoch 2/20\n",
      "581/581 [==============================] - 58s 99ms/step - loss: 0.4142 - accuracy: 0.8643 - val_loss: 0.3485 - val_accuracy: 0.9101 - lr: 3.0000e-04\n",
      "Epoch 3/20\n",
      "581/581 [==============================] - 57s 98ms/step - loss: 0.2628 - accuracy: 0.9300 - val_loss: 0.1404 - val_accuracy: 0.9837 - lr: 3.0000e-04\n",
      "Epoch 4/20\n",
      "581/581 [==============================] - 56s 97ms/step - loss: 0.1540 - accuracy: 0.9759 - val_loss: 0.1008 - val_accuracy: 0.9923 - lr: 3.0000e-04\n",
      "Epoch 5/20\n",
      "581/581 [==============================] - 55s 95ms/step - loss: 0.1263 - accuracy: 0.9808 - val_loss: 0.0881 - val_accuracy: 0.9961 - lr: 3.0000e-04\n",
      "Epoch 6/20\n",
      "581/581 [==============================] - 57s 99ms/step - loss: 0.1172 - accuracy: 0.9860 - val_loss: 0.0894 - val_accuracy: 0.9961 - lr: 3.0000e-04\n",
      "Epoch 7/20\n",
      "581/581 [==============================] - 56s 96ms/step - loss: 0.1028 - accuracy: 0.9899 - val_loss: 0.1123 - val_accuracy: 0.9893 - lr: 3.0000e-04\n",
      "Epoch 8/20\n",
      "581/581 [==============================] - 55s 95ms/step - loss: 0.0861 - accuracy: 0.9952 - val_loss: 0.0799 - val_accuracy: 0.9974 - lr: 6.0000e-05\n",
      "Epoch 9/20\n",
      "581/581 [==============================] - 57s 99ms/step - loss: 0.0804 - accuracy: 0.9964 - val_loss: 0.0917 - val_accuracy: 0.9957 - lr: 6.0000e-05\n",
      "Epoch 10/20\n",
      "581/581 [==============================] - 57s 99ms/step - loss: 0.0774 - accuracy: 0.9973 - val_loss: 0.0724 - val_accuracy: 0.9983 - lr: 6.0000e-05\n",
      "Epoch 11/20\n",
      "581/581 [==============================] - 57s 98ms/step - loss: 0.0714 - accuracy: 0.9987 - val_loss: 0.0792 - val_accuracy: 0.9961 - lr: 6.0000e-05\n",
      "Epoch 12/20\n",
      "581/581 [==============================] - 58s 100ms/step - loss: 0.0714 - accuracy: 0.9974 - val_loss: 0.0762 - val_accuracy: 0.9970 - lr: 6.0000e-05\n",
      "Epoch 13/20\n",
      "581/581 [==============================] - 58s 100ms/step - loss: 0.0691 - accuracy: 0.9982 - val_loss: 0.0761 - val_accuracy: 0.9970 - lr: 1.2000e-05\n",
      "Epoch 14/20\n",
      "581/581 [==============================] - 57s 98ms/step - loss: 0.0666 - accuracy: 0.9992 - val_loss: 0.0767 - val_accuracy: 0.9961 - lr: 1.2000e-05\n",
      "Epoch 15/20\n",
      "581/581 [==============================] - 57s 99ms/step - loss: 0.0667 - accuracy: 0.9987 - val_loss: 0.0764 - val_accuracy: 0.9966 - lr: 2.4000e-06\n",
      "Epoch 16/20\n",
      "581/581 [==============================] - 57s 98ms/step - loss: 0.0672 - accuracy: 0.9987 - val_loss: 0.0754 - val_accuracy: 0.9966 - lr: 2.4000e-06\n",
      "Epoch 17/20\n",
      "581/581 [==============================] - 56s 97ms/step - loss: 0.0662 - accuracy: 0.9986 - val_loss: 0.0752 - val_accuracy: 0.9974 - lr: 4.8000e-07\n",
      "Epoch 18/20\n",
      "581/581 [==============================] - 56s 96ms/step - loss: 0.0654 - accuracy: 0.9994 - val_loss: 0.0756 - val_accuracy: 0.9966 - lr: 4.8000e-07\n",
      "Epoch 19/20\n",
      "581/581 [==============================] - 57s 98ms/step - loss: 0.0660 - accuracy: 0.9990 - val_loss: 0.0753 - val_accuracy: 0.9966 - lr: 9.6000e-08\n",
      "Epoch 20/20\n",
      "581/581 [==============================] - 56s 96ms/step - loss: 0.0657 - accuracy: 0.9994 - val_loss: 0.0757 - val_accuracy: 0.9966 - lr: 9.6000e-08\n",
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_5 (InputLayer)        [(None, 453, 1)]          0         \n",
      "                                                                 \n",
      " bidirectional_4 (Bidirectio  (None, 1024)             1582080   \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 1024)             4096      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 128)               131200    \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,717,505\n",
      "Trainable params: 1,715,457\n",
      "Non-trainable params: 2,048\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "581/581 [==============================] - 62s 100ms/step - loss: 0.5655 - accuracy: 0.8043 - val_loss: 0.2989 - val_accuracy: 0.9218 - lr: 3.0000e-04\n",
      "Epoch 2/20\n",
      "581/581 [==============================] - 59s 101ms/step - loss: 0.4105 - accuracy: 0.8679 - val_loss: 0.3744 - val_accuracy: 0.8770 - lr: 3.0000e-04\n",
      "Epoch 3/20\n",
      "581/581 [==============================] - 57s 99ms/step - loss: 0.2700 - accuracy: 0.9299 - val_loss: 0.8880 - val_accuracy: 0.7412 - lr: 3.0000e-04\n",
      "Epoch 4/20\n",
      "581/581 [==============================] - 55s 95ms/step - loss: 0.1597 - accuracy: 0.9723 - val_loss: 0.2232 - val_accuracy: 0.9433 - lr: 6.0000e-05\n",
      "Epoch 5/20\n",
      "581/581 [==============================] - 58s 99ms/step - loss: 0.1417 - accuracy: 0.9795 - val_loss: 0.2438 - val_accuracy: 0.9394 - lr: 6.0000e-05\n",
      "Epoch 6/20\n",
      "581/581 [==============================] - 58s 101ms/step - loss: 0.1330 - accuracy: 0.9839 - val_loss: 0.2173 - val_accuracy: 0.9445 - lr: 6.0000e-05\n",
      "Epoch 7/20\n",
      "581/581 [==============================] - 59s 102ms/step - loss: 0.1205 - accuracy: 0.9864 - val_loss: 0.2329 - val_accuracy: 0.9398 - lr: 6.0000e-05\n",
      "Epoch 8/20\n",
      "581/581 [==============================] - 55s 95ms/step - loss: 0.1101 - accuracy: 0.9898 - val_loss: 0.1603 - val_accuracy: 0.9733 - lr: 6.0000e-05\n",
      "Epoch 9/20\n",
      "581/581 [==============================] - 55s 94ms/step - loss: 0.1085 - accuracy: 0.9904 - val_loss: 0.1921 - val_accuracy: 0.9566 - lr: 6.0000e-05\n",
      "Epoch 10/20\n",
      "581/581 [==============================] - 55s 94ms/step - loss: 0.1015 - accuracy: 0.9934 - val_loss: 0.3002 - val_accuracy: 0.9308 - lr: 6.0000e-05\n",
      "Epoch 11/20\n",
      "581/581 [==============================] - 55s 94ms/step - loss: 0.0916 - accuracy: 0.9946 - val_loss: 0.1466 - val_accuracy: 0.9768 - lr: 1.2000e-05\n",
      "Epoch 12/20\n",
      "581/581 [==============================] - 55s 95ms/step - loss: 0.0913 - accuracy: 0.9945 - val_loss: 0.1518 - val_accuracy: 0.9725 - lr: 1.2000e-05\n",
      "Epoch 13/20\n",
      "581/581 [==============================] - 56s 97ms/step - loss: 0.0892 - accuracy: 0.9953 - val_loss: 0.1695 - val_accuracy: 0.9699 - lr: 1.2000e-05\n",
      "Epoch 14/20\n",
      "581/581 [==============================] - 55s 95ms/step - loss: 0.0893 - accuracy: 0.9952 - val_loss: 0.1560 - val_accuracy: 0.9725 - lr: 2.4000e-06\n",
      "Epoch 15/20\n",
      "581/581 [==============================] - 54s 93ms/step - loss: 0.0869 - accuracy: 0.9964 - val_loss: 0.1528 - val_accuracy: 0.9729 - lr: 2.4000e-06\n",
      "Epoch 16/20\n",
      "581/581 [==============================] - 57s 98ms/step - loss: 0.0880 - accuracy: 0.9957 - val_loss: 0.1521 - val_accuracy: 0.9725 - lr: 4.8000e-07\n",
      "Epoch 17/20\n",
      "581/581 [==============================] - 54s 93ms/step - loss: 0.0878 - accuracy: 0.9961 - val_loss: 0.1527 - val_accuracy: 0.9729 - lr: 4.8000e-07\n",
      "Epoch 18/20\n",
      "581/581 [==============================] - 53s 92ms/step - loss: 0.0868 - accuracy: 0.9969 - val_loss: 0.1534 - val_accuracy: 0.9725 - lr: 9.6000e-08\n",
      "Epoch 19/20\n",
      "581/581 [==============================] - 54s 92ms/step - loss: 0.0879 - accuracy: 0.9954 - val_loss: 0.1535 - val_accuracy: 0.9716 - lr: 9.6000e-08\n",
      "Epoch 20/20\n",
      "581/581 [==============================] - 54s 93ms/step - loss: 0.0846 - accuracy: 0.9972 - val_loss: 0.1536 - val_accuracy: 0.9742 - lr: 1.9200e-08\n",
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_6 (InputLayer)        [(None, 453, 1)]          0         \n",
      "                                                                 \n",
      " bidirectional_5 (Bidirectio  (None, 1024)             1582080   \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 1024)             4096      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 128)               131200    \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,717,505\n",
      "Trainable params: 1,715,457\n",
      "Non-trainable params: 2,048\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "581/581 [==============================] - 57s 94ms/step - loss: 0.4904 - accuracy: 0.8408 - val_loss: 0.5088 - val_accuracy: 0.8203 - lr: 3.0000e-04\n",
      "Epoch 2/20\n",
      "581/581 [==============================] - 54s 92ms/step - loss: 0.3588 - accuracy: 0.8965 - val_loss: 0.7096 - val_accuracy: 0.7820 - lr: 3.0000e-04\n",
      "Epoch 3/20\n",
      "581/581 [==============================] - 54s 92ms/step - loss: 0.3593 - accuracy: 0.8898 - val_loss: 0.5051 - val_accuracy: 0.8461 - lr: 3.0000e-04\n",
      "Epoch 4/20\n",
      "581/581 [==============================] - 54s 93ms/step - loss: 0.2627 - accuracy: 0.9287 - val_loss: 0.2648 - val_accuracy: 0.9342 - lr: 3.0000e-04\n",
      "Epoch 5/20\n",
      "581/581 [==============================] - 54s 93ms/step - loss: 0.2109 - accuracy: 0.9502 - val_loss: 0.2814 - val_accuracy: 0.9243 - lr: 3.0000e-04\n",
      "Epoch 6/20\n",
      "581/581 [==============================] - 54s 92ms/step - loss: 0.1295 - accuracy: 0.9816 - val_loss: 0.3942 - val_accuracy: 0.9269 - lr: 3.0000e-04\n",
      "Epoch 7/20\n",
      "581/581 [==============================] - 54s 93ms/step - loss: 0.0981 - accuracy: 0.9931 - val_loss: 0.2846 - val_accuracy: 0.9557 - lr: 6.0000e-05\n",
      "Epoch 8/20\n",
      "581/581 [==============================] - 54s 93ms/step - loss: 0.0977 - accuracy: 0.9932 - val_loss: 0.2887 - val_accuracy: 0.9518 - lr: 6.0000e-05\n",
      "Epoch 9/20\n",
      "581/581 [==============================] - 54s 92ms/step - loss: 0.0912 - accuracy: 0.9945 - val_loss: 0.2956 - val_accuracy: 0.9493 - lr: 1.2000e-05\n",
      "Epoch 10/20\n",
      "581/581 [==============================] - 54s 93ms/step - loss: 0.0884 - accuracy: 0.9964 - val_loss: 0.2890 - val_accuracy: 0.9540 - lr: 1.2000e-05\n",
      "Epoch 11/20\n",
      "581/581 [==============================] - 55s 95ms/step - loss: 0.0886 - accuracy: 0.9957 - val_loss: 0.3032 - val_accuracy: 0.9497 - lr: 2.4000e-06\n",
      "Epoch 12/20\n",
      "581/581 [==============================] - 55s 94ms/step - loss: 0.0869 - accuracy: 0.9964 - val_loss: 0.3023 - val_accuracy: 0.9506 - lr: 2.4000e-06\n",
      "Epoch 13/20\n",
      "581/581 [==============================] - 55s 95ms/step - loss: 0.0878 - accuracy: 0.9964 - val_loss: 0.3089 - val_accuracy: 0.9493 - lr: 4.8000e-07\n",
      "Epoch 14/20\n",
      "581/581 [==============================] - 55s 94ms/step - loss: 0.0866 - accuracy: 0.9966 - val_loss: 0.3394 - val_accuracy: 0.9454 - lr: 4.8000e-07\n",
      "Epoch 15/20\n",
      "581/581 [==============================] - 55s 95ms/step - loss: 0.0864 - accuracy: 0.9962 - val_loss: 0.3149 - val_accuracy: 0.9501 - lr: 9.6000e-08\n",
      "Epoch 16/20\n",
      "581/581 [==============================] - 55s 94ms/step - loss: 0.0853 - accuracy: 0.9969 - val_loss: 0.3287 - val_accuracy: 0.9493 - lr: 9.6000e-08\n",
      "Epoch 17/20\n",
      "581/581 [==============================] - 54s 93ms/step - loss: 0.0866 - accuracy: 0.9969 - val_loss: 0.3089 - val_accuracy: 0.9514 - lr: 1.9200e-08\n",
      "Epoch 18/20\n",
      "581/581 [==============================] - 55s 94ms/step - loss: 0.0864 - accuracy: 0.9969 - val_loss: 0.3152 - val_accuracy: 0.9501 - lr: 1.9200e-08\n",
      "Epoch 19/20\n",
      "581/581 [==============================] - 56s 96ms/step - loss: 0.0877 - accuracy: 0.9971 - val_loss: 0.3196 - val_accuracy: 0.9493 - lr: 1.0000e-08\n",
      "Epoch 20/20\n",
      "581/581 [==============================] - 55s 95ms/step - loss: 0.0874 - accuracy: 0.9961 - val_loss: 0.3237 - val_accuracy: 0.9493 - lr: 1.0000e-08\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "batch_size = 16\n",
    "\n",
    "val_acc_per_fold = []\n",
    "acc_per_fold = []\n",
    "val_loss_per_fold = []\n",
    "loss_per_fold = []\n",
    "sets = [(fold_1, fold_1_lab), (fold_2, fold_2_lab), (fold_3, fold_3_lab), (fold_4, fold_4_lab), (fold_5, fold_5_lab)]\n",
    "\n",
    "for n in range(len(sets)):\n",
    "    test_x = sets[n][0]\n",
    "    test_y = sets[n][1]\n",
    "    train_x = pd.DataFrame()\n",
    "    train_y = pd.DataFrame()\n",
    "    for s in sets[:n] + sets[n+1:]:\n",
    "        train_x = pd.concat([train_x, s[0]])\n",
    "        train_y = pd.concat([train_y, s[1]])\n",
    "\n",
    "    classifier_test_opt_bi = build_model_bidirectional()\n",
    "\n",
    "    history_test_opt_bi = classifier_test_opt_bi.fit(\n",
    "        np.array(train_x), np.array(train_y),\n",
    "        epochs=epochs,\n",
    "        batch_size = batch_size,\n",
    "        steps_per_epoch=train_x.shape[0] // batch_size,\n",
    "        validation_data=(np.array(test_x), np.array(test_y)),\n",
    "        callbacks=[tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=1e-8)]\n",
    "    )\n",
    "\n",
    "    val_acc_per_fold.append(history_test_opt_bi.history['val_accuracy'])\n",
    "    acc_per_fold.append(history_test_opt_bi.history['accuracy'])\n",
    "    val_loss_per_fold.append(history_test_opt_bi.history['val_loss'])\n",
    "    loss_per_fold.append(history_test_opt_bi.history['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[0.8508168458938599,\n",
       "   0.8766121864318848,\n",
       "   0.9325021505355835,\n",
       "   0.9359415173530579,\n",
       "   0.9041272401809692,\n",
       "   0.9277729988098145,\n",
       "   0.9716250896453857,\n",
       "   0.9720550179481506,\n",
       "   0.9823731780052185,\n",
       "   0.9711951613426208,\n",
       "   0.9780739545822144,\n",
       "   0.9724849462509155,\n",
       "   0.970765233039856,\n",
       "   0.9797936081886292,\n",
       "   0.9780739545822144,\n",
       "   0.9763542413711548,\n",
       "   0.9763542413711548,\n",
       "   0.9750645160675049,\n",
       "   0.9767841696739197,\n",
       "   0.9763542413711548],\n",
       "  [0.8993980884552002,\n",
       "   0.9329320788383484,\n",
       "   0.9617368578910828,\n",
       "   0.929492712020874,\n",
       "   0.9883921146392822,\n",
       "   0.9815133213996887,\n",
       "   0.9849526882171631,\n",
       "   0.9978503584861755,\n",
       "   0.9995700716972351,\n",
       "   0.9961307048797607,\n",
       "   0.9982802867889404,\n",
       "   0.9978503584861755,\n",
       "   0.9991401433944702,\n",
       "   0.9991401433944702,\n",
       "   0.9982802867889404,\n",
       "   0.9974204897880554,\n",
       "   0.9987102150917053,\n",
       "   0.9987102150917053,\n",
       "   0.9982802867889404,\n",
       "   0.9982802867889404],\n",
       "  [0.8155632019042969,\n",
       "   0.910146176815033,\n",
       "   0.9836629629135132,\n",
       "   0.9922614097595215,\n",
       "   0.9961307048797607,\n",
       "   0.9961307048797607,\n",
       "   0.9892519116401672,\n",
       "   0.9974204897880554,\n",
       "   0.9957007765769958,\n",
       "   0.9982802867889404,\n",
       "   0.9961307048797607,\n",
       "   0.9969905614852905,\n",
       "   0.9969905614852905,\n",
       "   0.9961307048797607,\n",
       "   0.9965606331825256,\n",
       "   0.9965606331825256,\n",
       "   0.9974204897880554,\n",
       "   0.9965606331825256,\n",
       "   0.9965606331825256,\n",
       "   0.9965606331825256],\n",
       "  [0.9217540621757507,\n",
       "   0.8770421147346497,\n",
       "   0.7411865592002869,\n",
       "   0.9432502388954163,\n",
       "   0.9393808841705322,\n",
       "   0.9445399641990662,\n",
       "   0.9398108124732971,\n",
       "   0.9733448028564453,\n",
       "   0.9565778374671936,\n",
       "   0.9307824373245239,\n",
       "   0.9767841696739197,\n",
       "   0.9724849462509155,\n",
       "   0.969905436038971,\n",
       "   0.9724849462509155,\n",
       "   0.9729148745536804,\n",
       "   0.9724849462509155,\n",
       "   0.9729148745536804,\n",
       "   0.9724849462509155,\n",
       "   0.9716250896453857,\n",
       "   0.9742046594619751],\n",
       "  [0.8202923536300659,\n",
       "   0.7820292115211487,\n",
       "   0.8460876941680908,\n",
       "   0.9342218637466431,\n",
       "   0.9243336319923401,\n",
       "   0.9269131422042847,\n",
       "   0.9557179808616638,\n",
       "   0.9518486857414246,\n",
       "   0.9492691159248352,\n",
       "   0.9539982676506042,\n",
       "   0.9496990442276001,\n",
       "   0.9505589008331299,\n",
       "   0.9492691159248352,\n",
       "   0.945399820804596,\n",
       "   0.950128972530365,\n",
       "   0.9492691159248352,\n",
       "   0.9514187574386597,\n",
       "   0.950128972530365,\n",
       "   0.9492691159248352,\n",
       "   0.9492691159248352]],\n",
       " [[0.8258390426635742,\n",
       "   0.8883505463600159,\n",
       "   0.9266796112060547,\n",
       "   0.945198118686676,\n",
       "   0.9629629850387573,\n",
       "   0.9679155945777893,\n",
       "   0.9880490899085999,\n",
       "   0.9924634099006653,\n",
       "   0.9927864074707031,\n",
       "   0.9952626824378967,\n",
       "   0.9935400485992432,\n",
       "   0.9944013953208923,\n",
       "   0.9975236654281616,\n",
       "   0.9976313710212708,\n",
       "   0.9987080097198486,\n",
       "   0.9990310072898865,\n",
       "   0.9979543685913086,\n",
       "   0.998815655708313,\n",
       "   0.9989233613014221,\n",
       "   0.9984926581382751],\n",
       "  [0.815081775188446,\n",
       "   0.8690783977508545,\n",
       "   0.9275408983230591,\n",
       "   0.9152669906616211,\n",
       "   0.9585486650466919,\n",
       "   0.9730835556983948,\n",
       "   0.9823427796363831,\n",
       "   0.992678701877594,\n",
       "   0.9944013953208923,\n",
       "   0.9941860437393188,\n",
       "   0.9951550364494324,\n",
       "   0.9953703880310059,\n",
       "   0.9967700242996216,\n",
       "   0.9960163831710815,\n",
       "   0.9961240291595459,\n",
       "   0.9969853758811951,\n",
       "   0.9979543685913086,\n",
       "   0.9984926581382751,\n",
       "   0.9977390170097351,\n",
       "   0.9994616508483887],\n",
       "  [0.8194922804832458,\n",
       "   0.8643410801887512,\n",
       "   0.9300172328948975,\n",
       "   0.975882887840271,\n",
       "   0.980835497379303,\n",
       "   0.9860034584999084,\n",
       "   0.9898794293403625,\n",
       "   0.9951550364494324,\n",
       "   0.9964470267295837,\n",
       "   0.9973083734512329,\n",
       "   0.9987080097198486,\n",
       "   0.9974160194396973,\n",
       "   0.9981696605682373,\n",
       "   0.99924635887146,\n",
       "   0.9987080097198486,\n",
       "   0.9987080097198486,\n",
       "   0.9986003637313843,\n",
       "   0.9993540048599243,\n",
       "   0.9990310072898865,\n",
       "   0.9993540048599243],\n",
       "  [0.8043244481086731,\n",
       "   0.8678940534591675,\n",
       "   0.9299095869064331,\n",
       "   0.9723299145698547,\n",
       "   0.9795435070991516,\n",
       "   0.9838501214981079,\n",
       "   0.9864341020584106,\n",
       "   0.9897717237472534,\n",
       "   0.9904177188873291,\n",
       "   0.9934324026107788,\n",
       "   0.994616687297821,\n",
       "   0.9945090413093567,\n",
       "   0.9952626824378967,\n",
       "   0.9951550364494324,\n",
       "   0.9964470267295837,\n",
       "   0.9956933856010437,\n",
       "   0.9961240291595459,\n",
       "   0.9968776702880859,\n",
       "   0.9953703880310059,\n",
       "   0.9972006678581238],\n",
       "  [0.8407917618751526,\n",
       "   0.8965331315994263,\n",
       "   0.8897502422332764,\n",
       "   0.9287252426147461,\n",
       "   0.950150728225708,\n",
       "   0.981589138507843,\n",
       "   0.993109405040741,\n",
       "   0.9932170510292053,\n",
       "   0.9945090413093567,\n",
       "   0.9964470267295837,\n",
       "   0.9956933856010437,\n",
       "   0.9964470267295837,\n",
       "   0.9964470267295837,\n",
       "   0.9965546727180481,\n",
       "   0.9962316751480103,\n",
       "   0.9968776702880859,\n",
       "   0.9968776702880859,\n",
       "   0.9968776702880859,\n",
       "   0.9970930218696594,\n",
       "   0.9961240291595459]],\n",
       " [[0.4770844876766205,\n",
       "   0.45345285534858704,\n",
       "   0.2777680456638336,\n",
       "   0.23562318086624146,\n",
       "   0.3701457679271698,\n",
       "   0.24368944764137268,\n",
       "   0.14864864945411682,\n",
       "   0.13768184185028076,\n",
       "   0.12570208311080933,\n",
       "   0.14775970578193665,\n",
       "   0.12480734288692474,\n",
       "   0.14424200356006622,\n",
       "   0.15569782257080078,\n",
       "   0.12857967615127563,\n",
       "   0.1329493671655655,\n",
       "   0.13402758538722992,\n",
       "   0.13383406400680542,\n",
       "   0.1358625441789627,\n",
       "   0.13380110263824463,\n",
       "   0.13409706950187683],\n",
       "  [0.3821285665035248,\n",
       "   0.2682100236415863,\n",
       "   0.18266169726848602,\n",
       "   0.26916447281837463,\n",
       "   0.11475704610347748,\n",
       "   0.13256774842739105,\n",
       "   0.11659206449985504,\n",
       "   0.08168201893568039,\n",
       "   0.07700396329164505,\n",
       "   0.0836237445473671,\n",
       "   0.07534610480070114,\n",
       "   0.07563988864421844,\n",
       "   0.0708097368478775,\n",
       "   0.07044463604688644,\n",
       "   0.0705074667930603,\n",
       "   0.07547620683908463,\n",
       "   0.06880959123373032,\n",
       "   0.0672718957066536,\n",
       "   0.06869147717952728,\n",
       "   0.06746675819158554],\n",
       "  [0.463533878326416,\n",
       "   0.3485298752784729,\n",
       "   0.14037193357944489,\n",
       "   0.10078790038824081,\n",
       "   0.0880996361374855,\n",
       "   0.08942750841379166,\n",
       "   0.11225619167089462,\n",
       "   0.07991763204336166,\n",
       "   0.09173553436994553,\n",
       "   0.0723932608962059,\n",
       "   0.07924214750528336,\n",
       "   0.07620332390069962,\n",
       "   0.07608737051486969,\n",
       "   0.07670244574546814,\n",
       "   0.07636924088001251,\n",
       "   0.0753854438662529,\n",
       "   0.07517579197883606,\n",
       "   0.07563076168298721,\n",
       "   0.0753093957901001,\n",
       "   0.07565705478191376],\n",
       "  [0.2989048659801483,\n",
       "   0.3743866980075836,\n",
       "   0.8879613876342773,\n",
       "   0.22321511805057526,\n",
       "   0.2437661737203598,\n",
       "   0.2172858864068985,\n",
       "   0.23293818533420563,\n",
       "   0.16028790175914764,\n",
       "   0.19208629429340363,\n",
       "   0.3002333343029022,\n",
       "   0.14659233391284943,\n",
       "   0.15176579356193542,\n",
       "   0.16950881481170654,\n",
       "   0.15600501000881195,\n",
       "   0.1528482884168625,\n",
       "   0.15209922194480896,\n",
       "   0.15271960198879242,\n",
       "   0.15344886481761932,\n",
       "   0.1535286158323288,\n",
       "   0.1535990834236145],\n",
       "  [0.5087644457817078,\n",
       "   0.7096115946769714,\n",
       "   0.5051237940788269,\n",
       "   0.264849454164505,\n",
       "   0.2813641428947449,\n",
       "   0.3941897451877594,\n",
       "   0.28459444642066956,\n",
       "   0.28865373134613037,\n",
       "   0.2956260144710541,\n",
       "   0.28903689980506897,\n",
       "   0.30322161316871643,\n",
       "   0.30225977301597595,\n",
       "   0.30892735719680786,\n",
       "   0.33936607837677,\n",
       "   0.3148731589317322,\n",
       "   0.3286876082420349,\n",
       "   0.308925062417984,\n",
       "   0.315206915140152,\n",
       "   0.3195500373840332,\n",
       "   0.3236933946609497]],\n",
       " [[0.51054847240448,\n",
       "   0.3636512756347656,\n",
       "   0.27565494179725647,\n",
       "   0.22053205966949463,\n",
       "   0.1814448982477188,\n",
       "   0.1615382581949234,\n",
       "   0.11378169804811478,\n",
       "   0.09850586950778961,\n",
       "   0.09628571569919586,\n",
       "   0.08779799938201904,\n",
       "   0.08903974294662476,\n",
       "   0.08557873964309692,\n",
       "   0.0763435885310173,\n",
       "   0.07439744472503662,\n",
       "   0.07010860741138458,\n",
       "   0.07052193582057953,\n",
       "   0.0709494948387146,\n",
       "   0.070013627409935,\n",
       "   0.06905088573694229,\n",
       "   0.0698595717549324],\n",
       "  [0.5462020039558411,\n",
       "   0.41312915086746216,\n",
       "   0.2757348120212555,\n",
       "   0.30390089750289917,\n",
       "   0.189249187707901,\n",
       "   0.1622132509946823,\n",
       "   0.12737560272216797,\n",
       "   0.10078665614128113,\n",
       "   0.09256093949079514,\n",
       "   0.0916954055428505,\n",
       "   0.08826672285795212,\n",
       "   0.08530548214912415,\n",
       "   0.08176825940608978,\n",
       "   0.08388764411211014,\n",
       "   0.07836654782295227,\n",
       "   0.07470399886369705,\n",
       "   0.07131170481443405,\n",
       "   0.06969554722309113,\n",
       "   0.0704001635313034,\n",
       "   0.06644290685653687],\n",
       "  [0.5341109037399292,\n",
       "   0.41417187452316284,\n",
       "   0.2628370225429535,\n",
       "   0.15400399267673492,\n",
       "   0.1262815296649933,\n",
       "   0.11718738079071045,\n",
       "   0.10284694284200668,\n",
       "   0.08606944233179092,\n",
       "   0.08036018162965775,\n",
       "   0.07738330960273743,\n",
       "   0.07138988375663757,\n",
       "   0.07137328386306763,\n",
       "   0.06910885125398636,\n",
       "   0.06656736135482788,\n",
       "   0.06668958067893982,\n",
       "   0.06717464327812195,\n",
       "   0.06624661386013031,\n",
       "   0.0654052272439003,\n",
       "   0.06597886234521866,\n",
       "   0.06567642837762833],\n",
       "  [0.5654615759849548,\n",
       "   0.4104622006416321,\n",
       "   0.27004551887512207,\n",
       "   0.15967322885990143,\n",
       "   0.1416824758052826,\n",
       "   0.13296008110046387,\n",
       "   0.12047047913074493,\n",
       "   0.11014024913311005,\n",
       "   0.10845619440078735,\n",
       "   0.10150302201509476,\n",
       "   0.09155057370662689,\n",
       "   0.09134913235902786,\n",
       "   0.08924352377653122,\n",
       "   0.08929076790809631,\n",
       "   0.0869387686252594,\n",
       "   0.08804983645677567,\n",
       "   0.08782723546028137,\n",
       "   0.08679604530334473,\n",
       "   0.08792146295309067,\n",
       "   0.08464504778385162],\n",
       "  [0.4904381334781647,\n",
       "   0.3587794899940491,\n",
       "   0.35934147238731384,\n",
       "   0.26267147064208984,\n",
       "   0.21089696884155273,\n",
       "   0.12950707972049713,\n",
       "   0.09807104617357254,\n",
       "   0.09774014353752136,\n",
       "   0.0911572203040123,\n",
       "   0.0884140208363533,\n",
       "   0.08855059742927551,\n",
       "   0.08685477077960968,\n",
       "   0.08775796741247177,\n",
       "   0.0865657776594162,\n",
       "   0.08635316789150238,\n",
       "   0.08533168584108353,\n",
       "   0.08659451454877853,\n",
       "   0.08640643954277039,\n",
       "   0.08766786009073257,\n",
       "   0.08738881349563599]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_acc_per_fold, acc_per_fold, val_loss_per_fold, loss_per_fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "results = {\"val_acc_per_fold\": val_acc_per_fold, \"acc_per_fold\": acc_per_fold, \"val_loss_per_fold\": val_loss_per_fold, \"loss_per_fold\": loss_per_fold}\n",
    "\n",
    "with open('results_model_bi.pickle', 'wb') as file:\n",
    " pickle.dump(results, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9763542413711548\n",
      "0.9982802867889404\n",
      "0.9965606331825256\n",
      "0.9742046594619751\n",
      "0.9492691159248352\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9789337873458862"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 0\n",
    "for fold in val_acc_per_fold:\n",
    "    print(fold[-1])\n",
    "    s += fold[-1]\n",
    "s/5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bidirectional RNN model - zmniejszenie learning rate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_bidirectional():\n",
    "    hidden_dim = 512\n",
    "    seq_len = fold_1.shape[1]\n",
    "    n_features = 1\n",
    "    learning_rate = 0.00003\n",
    "\n",
    "    regularizers = {\n",
    "        'kernel_regularizer':tf.keras.regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
    "        'bias_regularizer':tf.keras.regularizers.l2(1e-4),\n",
    "        'activity_regularizer':tf.keras.regularizers.l2(1e-5)\n",
    "    }\n",
    "\n",
    "    regularizers_GRU = {\n",
    "        'kernel_regularizer':tf.keras.regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
    "        'bias_regularizer':tf.keras.regularizers.l2(1e-4),\n",
    "        'activity_regularizer':tf.keras.regularizers.l2(1e-5),\n",
    "        'recurrent_regularizer':tf.keras.regularizers.l2(1e-5)\n",
    "    }\n",
    "\n",
    "    in1 = tf.keras.layers.Input(shape=(seq_len, n_features))\n",
    "    gru1 = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(hidden_dim, name='latent_layer', **regularizers_GRU))(in1)\n",
    "    bn = tf.keras.layers.BatchNormalization()(gru1)\n",
    "    dense = tf.keras.layers.Dense(128, activation=\"relu\", **regularizers)(bn)\n",
    "    drop = tf.keras.layers.Dropout(0.3)(dense)\n",
    "    d1   = tf.keras.layers.Dense(1, activation='sigmoid', **regularizers)(drop)\n",
    "\n",
    "    classifier_test_opt_bi = tf.keras.Model(\n",
    "        inputs=[in1], \n",
    "        outputs=[d1]\n",
    "    )\n",
    "\n",
    "    #print(classifier_test_opt_bi.summary())\n",
    "\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    classifier_test_opt_bi.compile(loss='binary_crossentropy', optimizer=opt, \n",
    "        metrics=['accuracy'])\n",
    "    \n",
    "    return classifier_test_opt_bi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "581/581 [==============================] - 50s 83ms/step - loss: 0.5407 - accuracy: 0.7984 - val_loss: 0.4716 - val_accuracy: 0.8362 - lr: 3.0000e-05\n",
      "Epoch 2/40\n",
      "581/581 [==============================] - 48s 83ms/step - loss: 0.4667 - accuracy: 0.8411 - val_loss: 0.4446 - val_accuracy: 0.8543 - lr: 3.0000e-05\n",
      "Epoch 3/40\n",
      "581/581 [==============================] - 48s 83ms/step - loss: 0.4474 - accuracy: 0.8480 - val_loss: 0.4641 - val_accuracy: 0.8336 - lr: 3.0000e-05\n",
      "Epoch 4/40\n",
      "581/581 [==============================] - 48s 82ms/step - loss: 0.4408 - accuracy: 0.8526 - val_loss: 0.4237 - val_accuracy: 0.8633 - lr: 3.0000e-05\n",
      "Epoch 5/40\n",
      "581/581 [==============================] - 48s 82ms/step - loss: 0.4246 - accuracy: 0.8622 - val_loss: 0.4073 - val_accuracy: 0.8646 - lr: 3.0000e-05\n",
      "Epoch 6/40\n",
      "581/581 [==============================] - 48s 83ms/step - loss: 0.4111 - accuracy: 0.8670 - val_loss: 0.3900 - val_accuracy: 0.8719 - lr: 3.0000e-05\n",
      "Epoch 7/40\n",
      "581/581 [==============================] - 51s 87ms/step - loss: 0.3963 - accuracy: 0.8739 - val_loss: 0.3853 - val_accuracy: 0.8852 - lr: 3.0000e-05\n",
      "Epoch 8/40\n",
      "581/581 [==============================] - 50s 86ms/step - loss: 0.3783 - accuracy: 0.8794 - val_loss: 0.3804 - val_accuracy: 0.8891 - lr: 3.0000e-05\n",
      "Epoch 9/40\n",
      "581/581 [==============================] - 49s 85ms/step - loss: 0.3647 - accuracy: 0.8886 - val_loss: 0.3542 - val_accuracy: 0.8990 - lr: 3.0000e-05\n",
      "Epoch 10/40\n",
      "581/581 [==============================] - 49s 84ms/step - loss: 0.3611 - accuracy: 0.8900 - val_loss: 0.3451 - val_accuracy: 0.8994 - lr: 3.0000e-05\n",
      "Epoch 11/40\n",
      "581/581 [==============================] - 49s 84ms/step - loss: 0.3370 - accuracy: 0.8967 - val_loss: 0.3368 - val_accuracy: 0.9020 - lr: 3.0000e-05\n",
      "Epoch 12/40\n",
      "581/581 [==============================] - 49s 85ms/step - loss: 0.3231 - accuracy: 0.9054 - val_loss: 0.3003 - val_accuracy: 0.9175 - lr: 3.0000e-05\n",
      "Epoch 13/40\n",
      "581/581 [==============================] - 49s 84ms/step - loss: 0.2915 - accuracy: 0.9189 - val_loss: 0.2722 - val_accuracy: 0.9364 - lr: 3.0000e-05\n",
      "Epoch 14/40\n",
      "581/581 [==============================] - 51s 88ms/step - loss: 0.2496 - accuracy: 0.9367 - val_loss: 0.2352 - val_accuracy: 0.9454 - lr: 3.0000e-05\n",
      "Epoch 15/40\n",
      "581/581 [==============================] - 49s 84ms/step - loss: 0.2331 - accuracy: 0.9437 - val_loss: 0.2191 - val_accuracy: 0.9544 - lr: 3.0000e-05\n",
      "Epoch 16/40\n",
      "581/581 [==============================] - 49s 84ms/step - loss: 0.2106 - accuracy: 0.9527 - val_loss: 0.1977 - val_accuracy: 0.9544 - lr: 3.0000e-05\n",
      "Epoch 17/40\n",
      "581/581 [==============================] - 49s 84ms/step - loss: 0.1974 - accuracy: 0.9577 - val_loss: 0.1779 - val_accuracy: 0.9630 - lr: 3.0000e-05\n",
      "Epoch 18/40\n",
      "581/581 [==============================] - 49s 84ms/step - loss: 0.1872 - accuracy: 0.9613 - val_loss: 0.1872 - val_accuracy: 0.9583 - lr: 3.0000e-05\n",
      "Epoch 19/40\n",
      "581/581 [==============================] - 49s 85ms/step - loss: 0.1799 - accuracy: 0.9631 - val_loss: 0.1659 - val_accuracy: 0.9665 - lr: 3.0000e-05\n",
      "Epoch 20/40\n",
      "581/581 [==============================] - 49s 85ms/step - loss: 0.1741 - accuracy: 0.9660 - val_loss: 0.1592 - val_accuracy: 0.9708 - lr: 3.0000e-05\n",
      "Epoch 21/40\n",
      "581/581 [==============================] - 49s 84ms/step - loss: 0.1633 - accuracy: 0.9690 - val_loss: 0.1673 - val_accuracy: 0.9725 - lr: 3.0000e-05\n",
      "Epoch 22/40\n",
      "581/581 [==============================] - 49s 84ms/step - loss: 0.1646 - accuracy: 0.9673 - val_loss: 0.1447 - val_accuracy: 0.9789 - lr: 3.0000e-05\n",
      "Epoch 23/40\n",
      "581/581 [==============================] - 49s 84ms/step - loss: 0.1574 - accuracy: 0.9709 - val_loss: 0.1413 - val_accuracy: 0.9772 - lr: 3.0000e-05\n",
      "Epoch 24/40\n",
      "581/581 [==============================] - 49s 84ms/step - loss: 0.1480 - accuracy: 0.9758 - val_loss: 0.1500 - val_accuracy: 0.9751 - lr: 3.0000e-05\n",
      "Epoch 25/40\n",
      "581/581 [==============================] - 50s 86ms/step - loss: 0.1480 - accuracy: 0.9762 - val_loss: 0.1428 - val_accuracy: 0.9785 - lr: 3.0000e-05\n",
      "Epoch 26/40\n",
      "581/581 [==============================] - 50s 86ms/step - loss: 0.1327 - accuracy: 0.9815 - val_loss: 0.1221 - val_accuracy: 0.9858 - lr: 6.0000e-06\n",
      "Epoch 27/40\n",
      "581/581 [==============================] - 49s 85ms/step - loss: 0.1345 - accuracy: 0.9825 - val_loss: 0.1207 - val_accuracy: 0.9858 - lr: 6.0000e-06\n",
      "Epoch 28/40\n",
      "581/581 [==============================] - 49s 85ms/step - loss: 0.1320 - accuracy: 0.9820 - val_loss: 0.1215 - val_accuracy: 0.9832 - lr: 6.0000e-06\n",
      "Epoch 29/40\n",
      "581/581 [==============================] - 49s 85ms/step - loss: 0.1287 - accuracy: 0.9828 - val_loss: 0.1237 - val_accuracy: 0.9845 - lr: 6.0000e-06\n",
      "Epoch 30/40\n",
      "581/581 [==============================] - 49s 84ms/step - loss: 0.1284 - accuracy: 0.9834 - val_loss: 0.1168 - val_accuracy: 0.9871 - lr: 1.2000e-06\n",
      "Epoch 31/40\n",
      "581/581 [==============================] - 49s 84ms/step - loss: 0.1245 - accuracy: 0.9840 - val_loss: 0.1166 - val_accuracy: 0.9875 - lr: 1.2000e-06\n",
      "Epoch 32/40\n",
      "581/581 [==============================] - 49s 84ms/step - loss: 0.1250 - accuracy: 0.9845 - val_loss: 0.1158 - val_accuracy: 0.9875 - lr: 1.2000e-06\n",
      "Epoch 33/40\n",
      "581/581 [==============================] - 49s 85ms/step - loss: 0.1269 - accuracy: 0.9836 - val_loss: 0.1157 - val_accuracy: 0.9875 - lr: 1.2000e-06\n",
      "Epoch 34/40\n",
      "581/581 [==============================] - 49s 84ms/step - loss: 0.1205 - accuracy: 0.9850 - val_loss: 0.1153 - val_accuracy: 0.9875 - lr: 1.2000e-06\n",
      "Epoch 35/40\n",
      "581/581 [==============================] - 49s 85ms/step - loss: 0.1234 - accuracy: 0.9843 - val_loss: 0.1151 - val_accuracy: 0.9871 - lr: 1.2000e-06\n",
      "Epoch 36/40\n",
      "581/581 [==============================] - 49s 85ms/step - loss: 0.1257 - accuracy: 0.9828 - val_loss: 0.1157 - val_accuracy: 0.9880 - lr: 1.2000e-06\n",
      "Epoch 37/40\n",
      "581/581 [==============================] - 49s 85ms/step - loss: 0.1261 - accuracy: 0.9843 - val_loss: 0.1149 - val_accuracy: 0.9884 - lr: 1.2000e-06\n",
      "Epoch 38/40\n",
      "581/581 [==============================] - 50s 85ms/step - loss: 0.1232 - accuracy: 0.9854 - val_loss: 0.1147 - val_accuracy: 0.9880 - lr: 1.2000e-06\n",
      "Epoch 39/40\n",
      "581/581 [==============================] - 50s 86ms/step - loss: 0.1231 - accuracy: 0.9846 - val_loss: 0.1147 - val_accuracy: 0.9880 - lr: 1.2000e-06\n",
      "Epoch 40/40\n",
      "581/581 [==============================] - 49s 85ms/step - loss: 0.1255 - accuracy: 0.9846 - val_loss: 0.1141 - val_accuracy: 0.9875 - lr: 1.2000e-06\n",
      "Epoch 1/40\n",
      "581/581 [==============================] - 52s 86ms/step - loss: 0.5361 - accuracy: 0.8055 - val_loss: 0.4446 - val_accuracy: 0.8577 - lr: 3.0000e-05\n",
      "Epoch 2/40\n",
      "581/581 [==============================] - 49s 84ms/step - loss: 0.4689 - accuracy: 0.8388 - val_loss: 0.4188 - val_accuracy: 0.8702 - lr: 3.0000e-05\n",
      "Epoch 3/40\n",
      "581/581 [==============================] - 49s 84ms/step - loss: 0.4486 - accuracy: 0.8495 - val_loss: 0.4190 - val_accuracy: 0.8667 - lr: 3.0000e-05\n",
      "Epoch 4/40\n",
      "581/581 [==============================] - 49s 84ms/step - loss: 0.4346 - accuracy: 0.8552 - val_loss: 0.4066 - val_accuracy: 0.8667 - lr: 3.0000e-05\n",
      "Epoch 5/40\n",
      "581/581 [==============================] - 49s 84ms/step - loss: 0.4337 - accuracy: 0.8539 - val_loss: 0.4124 - val_accuracy: 0.8646 - lr: 3.0000e-05\n",
      "Epoch 6/40\n",
      "581/581 [==============================] - 50s 86ms/step - loss: 0.4223 - accuracy: 0.8609 - val_loss: 0.3924 - val_accuracy: 0.8779 - lr: 3.0000e-05\n",
      "Epoch 7/40\n",
      "581/581 [==============================] - 50s 86ms/step - loss: 0.4085 - accuracy: 0.8672 - val_loss: 0.3789 - val_accuracy: 0.8813 - lr: 3.0000e-05\n",
      "Epoch 8/40\n",
      "581/581 [==============================] - 49s 85ms/step - loss: 0.3988 - accuracy: 0.8733 - val_loss: 0.3690 - val_accuracy: 0.8839 - lr: 3.0000e-05\n",
      "Epoch 9/40\n",
      "581/581 [==============================] - 49s 85ms/step - loss: 0.3807 - accuracy: 0.8801 - val_loss: 0.3471 - val_accuracy: 0.8917 - lr: 3.0000e-05\n",
      "Epoch 10/40\n",
      "581/581 [==============================] - 49s 85ms/step - loss: 0.3605 - accuracy: 0.8893 - val_loss: 0.3387 - val_accuracy: 0.8942 - lr: 3.0000e-05\n",
      "Epoch 11/40\n",
      "581/581 [==============================] - 50s 85ms/step - loss: 0.3435 - accuracy: 0.8977 - val_loss: 0.3012 - val_accuracy: 0.9213 - lr: 3.0000e-05\n",
      "Epoch 12/40\n",
      "581/581 [==============================] - 49s 85ms/step - loss: 0.3254 - accuracy: 0.9046 - val_loss: 0.2965 - val_accuracy: 0.9179 - lr: 3.0000e-05\n",
      "Epoch 13/40\n",
      "581/581 [==============================] - 49s 85ms/step - loss: 0.2877 - accuracy: 0.9188 - val_loss: 0.2200 - val_accuracy: 0.9510 - lr: 3.0000e-05\n",
      "Epoch 14/40\n",
      "581/581 [==============================] - 49s 85ms/step - loss: 0.2515 - accuracy: 0.9358 - val_loss: 0.1908 - val_accuracy: 0.9617 - lr: 3.0000e-05\n",
      "Epoch 15/40\n",
      "581/581 [==============================] - 50s 85ms/step - loss: 0.2286 - accuracy: 0.9425 - val_loss: 0.1881 - val_accuracy: 0.9669 - lr: 3.0000e-05\n",
      "Epoch 16/40\n",
      "581/581 [==============================] - 49s 85ms/step - loss: 0.2155 - accuracy: 0.9499 - val_loss: 0.1757 - val_accuracy: 0.9652 - lr: 3.0000e-05\n",
      "Epoch 17/40\n",
      "581/581 [==============================] - 49s 84ms/step - loss: 0.2015 - accuracy: 0.9566 - val_loss: 0.1714 - val_accuracy: 0.9673 - lr: 3.0000e-05\n",
      "Epoch 18/40\n",
      "581/581 [==============================] - 49s 85ms/step - loss: 0.1857 - accuracy: 0.9633 - val_loss: 0.1691 - val_accuracy: 0.9708 - lr: 3.0000e-05\n",
      "Epoch 19/40\n",
      "581/581 [==============================] - 49s 85ms/step - loss: 0.1774 - accuracy: 0.9651 - val_loss: 0.1558 - val_accuracy: 0.9751 - lr: 3.0000e-05\n",
      "Epoch 20/40\n",
      "581/581 [==============================] - 49s 85ms/step - loss: 0.1668 - accuracy: 0.9715 - val_loss: 0.1461 - val_accuracy: 0.9755 - lr: 3.0000e-05\n",
      "Epoch 21/40\n",
      "581/581 [==============================] - 50s 86ms/step - loss: 0.1572 - accuracy: 0.9735 - val_loss: 0.1840 - val_accuracy: 0.9686 - lr: 3.0000e-05\n",
      "Epoch 22/40\n",
      "581/581 [==============================] - 49s 85ms/step - loss: 0.1620 - accuracy: 0.9718 - val_loss: 0.1484 - val_accuracy: 0.9789 - lr: 3.0000e-05\n",
      "Epoch 23/40\n",
      "581/581 [==============================] - 49s 85ms/step - loss: 0.1426 - accuracy: 0.9779 - val_loss: 0.1287 - val_accuracy: 0.9798 - lr: 6.0000e-06\n",
      "Epoch 24/40\n",
      "581/581 [==============================] - 49s 85ms/step - loss: 0.1377 - accuracy: 0.9803 - val_loss: 0.1260 - val_accuracy: 0.9837 - lr: 6.0000e-06\n",
      "Epoch 25/40\n",
      "581/581 [==============================] - 49s 84ms/step - loss: 0.1380 - accuracy: 0.9805 - val_loss: 0.1285 - val_accuracy: 0.9815 - lr: 6.0000e-06\n",
      "Epoch 26/40\n",
      "581/581 [==============================] - 52s 89ms/step - loss: 0.1383 - accuracy: 0.9793 - val_loss: 0.1232 - val_accuracy: 0.9832 - lr: 6.0000e-06\n",
      "Epoch 27/40\n",
      "581/581 [==============================] - 50s 87ms/step - loss: 0.1372 - accuracy: 0.9828 - val_loss: 0.1230 - val_accuracy: 0.9819 - lr: 6.0000e-06\n",
      "Epoch 28/40\n",
      "581/581 [==============================] - 49s 85ms/step - loss: 0.1330 - accuracy: 0.9828 - val_loss: 0.1230 - val_accuracy: 0.9850 - lr: 6.0000e-06\n",
      "Epoch 29/40\n",
      "581/581 [==============================] - 49s 85ms/step - loss: 0.1369 - accuracy: 0.9805 - val_loss: 0.1260 - val_accuracy: 0.9837 - lr: 6.0000e-06\n",
      "Epoch 30/40\n",
      "581/581 [==============================] - 49s 85ms/step - loss: 0.1304 - accuracy: 0.9812 - val_loss: 0.1203 - val_accuracy: 0.9841 - lr: 1.2000e-06\n",
      "Epoch 31/40\n",
      "581/581 [==============================] - 49s 85ms/step - loss: 0.1285 - accuracy: 0.9827 - val_loss: 0.1203 - val_accuracy: 0.9845 - lr: 1.2000e-06\n",
      "Epoch 32/40\n",
      "581/581 [==============================] - 49s 85ms/step - loss: 0.1289 - accuracy: 0.9813 - val_loss: 0.1208 - val_accuracy: 0.9854 - lr: 1.2000e-06\n",
      "Epoch 33/40\n",
      "581/581 [==============================] - 50s 87ms/step - loss: 0.1333 - accuracy: 0.9815 - val_loss: 0.1203 - val_accuracy: 0.9850 - lr: 2.4000e-07\n",
      "Epoch 34/40\n",
      "581/581 [==============================] - 49s 84ms/step - loss: 0.1308 - accuracy: 0.9822 - val_loss: 0.1201 - val_accuracy: 0.9854 - lr: 2.4000e-07\n",
      "Epoch 35/40\n",
      "581/581 [==============================] - 49s 85ms/step - loss: 0.1302 - accuracy: 0.9821 - val_loss: 0.1201 - val_accuracy: 0.9850 - lr: 2.4000e-07\n",
      "Epoch 36/40\n",
      "581/581 [==============================] - 50s 86ms/step - loss: 0.1290 - accuracy: 0.9817 - val_loss: 0.1201 - val_accuracy: 0.9850 - lr: 2.4000e-07\n",
      "Epoch 37/40\n",
      "581/581 [==============================] - 51s 87ms/step - loss: 0.1269 - accuracy: 0.9837 - val_loss: 0.1202 - val_accuracy: 0.9854 - lr: 4.8000e-08\n",
      "Epoch 38/40\n",
      "581/581 [==============================] - 49s 85ms/step - loss: 0.1269 - accuracy: 0.9830 - val_loss: 0.1200 - val_accuracy: 0.9850 - lr: 4.8000e-08\n",
      "Epoch 39/40\n",
      "581/581 [==============================] - 49s 85ms/step - loss: 0.1284 - accuracy: 0.9832 - val_loss: 0.1202 - val_accuracy: 0.9850 - lr: 4.8000e-08\n",
      "Epoch 40/40\n",
      "581/581 [==============================] - 49s 84ms/step - loss: 0.1298 - accuracy: 0.9833 - val_loss: 0.1200 - val_accuracy: 0.9854 - lr: 4.8000e-08\n",
      "Epoch 1/40\n",
      "581/581 [==============================] - 52s 85ms/step - loss: 0.5433 - accuracy: 0.7974 - val_loss: 0.4583 - val_accuracy: 0.8431 - lr: 3.0000e-05\n",
      "Epoch 2/40\n",
      "581/581 [==============================] - 49s 85ms/step - loss: 0.4658 - accuracy: 0.8425 - val_loss: 0.4310 - val_accuracy: 0.8500 - lr: 3.0000e-05\n",
      "Epoch 3/40\n",
      "581/581 [==============================] - 49s 84ms/step - loss: 0.4461 - accuracy: 0.8515 - val_loss: 0.4161 - val_accuracy: 0.8568 - lr: 3.0000e-05\n",
      "Epoch 4/40\n",
      "581/581 [==============================] - 49s 85ms/step - loss: 0.4366 - accuracy: 0.8549 - val_loss: 0.4036 - val_accuracy: 0.8581 - lr: 3.0000e-05\n",
      "Epoch 5/40\n",
      "581/581 [==============================] - 49s 85ms/step - loss: 0.4321 - accuracy: 0.8600 - val_loss: 0.3929 - val_accuracy: 0.8611 - lr: 3.0000e-05\n",
      "Epoch 6/40\n",
      "581/581 [==============================] - 49s 85ms/step - loss: 0.4191 - accuracy: 0.8584 - val_loss: 0.3814 - val_accuracy: 0.8727 - lr: 3.0000e-05\n",
      "Epoch 7/40\n",
      "581/581 [==============================] - 49s 85ms/step - loss: 0.4048 - accuracy: 0.8658 - val_loss: 0.3698 - val_accuracy: 0.8788 - lr: 3.0000e-05\n",
      "Epoch 8/40\n",
      "581/581 [==============================] - 50s 86ms/step - loss: 0.3865 - accuracy: 0.8759 - val_loss: 0.3697 - val_accuracy: 0.8822 - lr: 3.0000e-05\n",
      "Epoch 9/40\n",
      "581/581 [==============================] - 49s 85ms/step - loss: 0.3738 - accuracy: 0.8846 - val_loss: 0.3345 - val_accuracy: 0.9007 - lr: 3.0000e-05\n",
      "Epoch 10/40\n",
      "581/581 [==============================] - 49s 85ms/step - loss: 0.3560 - accuracy: 0.8921 - val_loss: 0.3422 - val_accuracy: 0.8947 - lr: 3.0000e-05\n",
      "Epoch 11/40\n",
      "581/581 [==============================] - 50s 85ms/step - loss: 0.3375 - accuracy: 0.8970 - val_loss: 0.3050 - val_accuracy: 0.9136 - lr: 3.0000e-05\n",
      "Epoch 12/40\n",
      "581/581 [==============================] - 50s 86ms/step - loss: 0.3162 - accuracy: 0.9055 - val_loss: 0.2819 - val_accuracy: 0.9213 - lr: 3.0000e-05\n",
      "Epoch 13/40\n",
      "581/581 [==============================] - 49s 85ms/step - loss: 0.2879 - accuracy: 0.9232 - val_loss: 0.2443 - val_accuracy: 0.9433 - lr: 3.0000e-05\n",
      "Epoch 14/40\n",
      "581/581 [==============================] - 50s 85ms/step - loss: 0.2509 - accuracy: 0.9338 - val_loss: 0.1984 - val_accuracy: 0.9583 - lr: 3.0000e-05\n",
      "Epoch 15/40\n",
      "581/581 [==============================] - 49s 85ms/step - loss: 0.2162 - accuracy: 0.9507 - val_loss: 0.1857 - val_accuracy: 0.9574 - lr: 3.0000e-05\n",
      "Epoch 16/40\n",
      "581/581 [==============================] - 50s 85ms/step - loss: 0.2091 - accuracy: 0.9522 - val_loss: 0.1809 - val_accuracy: 0.9622 - lr: 3.0000e-05\n",
      "Epoch 17/40\n",
      "581/581 [==============================] - 49s 85ms/step - loss: 0.1976 - accuracy: 0.9574 - val_loss: 0.1617 - val_accuracy: 0.9660 - lr: 3.0000e-05\n",
      "Epoch 18/40\n",
      "581/581 [==============================] - 49s 85ms/step - loss: 0.1845 - accuracy: 0.9645 - val_loss: 0.1539 - val_accuracy: 0.9725 - lr: 3.0000e-05\n",
      "Epoch 19/40\n",
      "581/581 [==============================] - 49s 85ms/step - loss: 0.1702 - accuracy: 0.9680 - val_loss: 0.1552 - val_accuracy: 0.9716 - lr: 3.0000e-05\n",
      "Epoch 20/40\n",
      "581/581 [==============================] - 50s 86ms/step - loss: 0.1675 - accuracy: 0.9692 - val_loss: 0.1519 - val_accuracy: 0.9733 - lr: 3.0000e-05\n",
      "Epoch 21/40\n",
      "581/581 [==============================] - 49s 85ms/step - loss: 0.1637 - accuracy: 0.9714 - val_loss: 0.1371 - val_accuracy: 0.9789 - lr: 3.0000e-05\n",
      "Epoch 22/40\n",
      "581/581 [==============================] - 49s 85ms/step - loss: 0.1521 - accuracy: 0.9734 - val_loss: 0.1641 - val_accuracy: 0.9652 - lr: 3.0000e-05\n",
      "Epoch 23/40\n",
      "581/581 [==============================] - 49s 85ms/step - loss: 0.1498 - accuracy: 0.9761 - val_loss: 0.1567 - val_accuracy: 0.9699 - lr: 3.0000e-05\n",
      "Epoch 24/40\n",
      "581/581 [==============================] - 49s 85ms/step - loss: 0.1436 - accuracy: 0.9776 - val_loss: 0.1266 - val_accuracy: 0.9815 - lr: 6.0000e-06\n",
      "Epoch 25/40\n",
      "581/581 [==============================] - 49s 85ms/step - loss: 0.1306 - accuracy: 0.9831 - val_loss: 0.1259 - val_accuracy: 0.9837 - lr: 6.0000e-06\n",
      "Epoch 26/40\n",
      "581/581 [==============================] - 50s 86ms/step - loss: 0.1317 - accuracy: 0.9815 - val_loss: 0.1272 - val_accuracy: 0.9824 - lr: 6.0000e-06\n",
      "Epoch 27/40\n",
      "581/581 [==============================] - 49s 85ms/step - loss: 0.1343 - accuracy: 0.9806 - val_loss: 0.1272 - val_accuracy: 0.9811 - lr: 6.0000e-06\n",
      "Epoch 28/40\n",
      "581/581 [==============================] - 50s 85ms/step - loss: 0.1284 - accuracy: 0.9828 - val_loss: 0.1231 - val_accuracy: 0.9824 - lr: 1.2000e-06\n",
      "Epoch 29/40\n",
      "581/581 [==============================] - 49s 85ms/step - loss: 0.1274 - accuracy: 0.9832 - val_loss: 0.1232 - val_accuracy: 0.9832 - lr: 1.2000e-06\n",
      "Epoch 30/40\n",
      "581/581 [==============================] - 50s 85ms/step - loss: 0.1272 - accuracy: 0.9834 - val_loss: 0.1236 - val_accuracy: 0.9824 - lr: 1.2000e-06\n",
      "Epoch 31/40\n",
      "581/581 [==============================] - 49s 85ms/step - loss: 0.1293 - accuracy: 0.9818 - val_loss: 0.1233 - val_accuracy: 0.9824 - lr: 2.4000e-07\n",
      "Epoch 32/40\n",
      "581/581 [==============================] - 49s 85ms/step - loss: 0.1300 - accuracy: 0.9825 - val_loss: 0.1232 - val_accuracy: 0.9819 - lr: 2.4000e-07\n",
      "Epoch 33/40\n",
      "581/581 [==============================] - 49s 85ms/step - loss: 0.1251 - accuracy: 0.9856 - val_loss: 0.1232 - val_accuracy: 0.9819 - lr: 4.8000e-08\n",
      "Epoch 34/40\n",
      "581/581 [==============================] - 49s 85ms/step - loss: 0.1239 - accuracy: 0.9856 - val_loss: 0.1232 - val_accuracy: 0.9824 - lr: 4.8000e-08\n",
      "Epoch 35/40\n",
      "581/581 [==============================] - 49s 85ms/step - loss: 0.1307 - accuracy: 0.9821 - val_loss: 0.1233 - val_accuracy: 0.9828 - lr: 1.0000e-08\n",
      "Epoch 36/40\n",
      "581/581 [==============================] - 49s 85ms/step - loss: 0.1254 - accuracy: 0.9831 - val_loss: 0.1234 - val_accuracy: 0.9819 - lr: 1.0000e-08\n",
      "Epoch 37/40\n",
      "581/581 [==============================] - 49s 85ms/step - loss: 0.1249 - accuracy: 0.9837 - val_loss: 0.1228 - val_accuracy: 0.9828 - lr: 1.0000e-08\n",
      "Epoch 38/40\n",
      "581/581 [==============================] - 49s 85ms/step - loss: 0.1299 - accuracy: 0.9817 - val_loss: 0.1233 - val_accuracy: 0.9819 - lr: 1.0000e-08\n",
      "Epoch 39/40\n",
      "581/581 [==============================] - 50s 85ms/step - loss: 0.1255 - accuracy: 0.9847 - val_loss: 0.1230 - val_accuracy: 0.9824 - lr: 1.0000e-08\n",
      "Epoch 40/40\n",
      "581/581 [==============================] - 49s 85ms/step - loss: 0.1301 - accuracy: 0.9834 - val_loss: 0.1231 - val_accuracy: 0.9819 - lr: 1.0000e-08\n",
      "Epoch 1/40\n",
      "581/581 [==============================] - 52s 85ms/step - loss: 0.5368 - accuracy: 0.8017 - val_loss: 0.4438 - val_accuracy: 0.8564 - lr: 3.0000e-05\n",
      "Epoch 2/40\n",
      "581/581 [==============================] - 48s 83ms/step - loss: 0.4685 - accuracy: 0.8412 - val_loss: 0.4087 - val_accuracy: 0.8672 - lr: 3.0000e-05\n",
      "Epoch 3/40\n",
      "581/581 [==============================] - 48s 83ms/step - loss: 0.4499 - accuracy: 0.8471 - val_loss: 0.4008 - val_accuracy: 0.8770 - lr: 3.0000e-05\n",
      "Epoch 4/40\n",
      "581/581 [==============================] - 48s 83ms/step - loss: 0.4407 - accuracy: 0.8539 - val_loss: 0.3990 - val_accuracy: 0.8715 - lr: 3.0000e-05\n",
      "Epoch 5/40\n",
      "581/581 [==============================] - 49s 84ms/step - loss: 0.4278 - accuracy: 0.8575 - val_loss: 0.4080 - val_accuracy: 0.8702 - lr: 3.0000e-05\n",
      "Epoch 6/40\n",
      "581/581 [==============================] - 50s 85ms/step - loss: 0.4218 - accuracy: 0.8599 - val_loss: 0.3848 - val_accuracy: 0.8887 - lr: 3.0000e-05\n",
      "Epoch 7/40\n",
      "581/581 [==============================] - 50s 85ms/step - loss: 0.3982 - accuracy: 0.8736 - val_loss: 0.3657 - val_accuracy: 0.8861 - lr: 3.0000e-05\n",
      "Epoch 8/40\n",
      "581/581 [==============================] - 50s 86ms/step - loss: 0.3815 - accuracy: 0.8829 - val_loss: 0.3374 - val_accuracy: 0.9020 - lr: 3.0000e-05\n",
      "Epoch 9/40\n",
      "581/581 [==============================] - 49s 85ms/step - loss: 0.3594 - accuracy: 0.8902 - val_loss: 0.3215 - val_accuracy: 0.9054 - lr: 3.0000e-05\n",
      "Epoch 10/40\n",
      "581/581 [==============================] - 49s 85ms/step - loss: 0.3485 - accuracy: 0.8934 - val_loss: 0.3015 - val_accuracy: 0.9106 - lr: 3.0000e-05\n",
      "Epoch 11/40\n",
      "581/581 [==============================] - 49s 85ms/step - loss: 0.3383 - accuracy: 0.9005 - val_loss: 0.2895 - val_accuracy: 0.9127 - lr: 3.0000e-05\n",
      "Epoch 12/40\n",
      "581/581 [==============================] - 49s 85ms/step - loss: 0.3180 - accuracy: 0.9071 - val_loss: 0.2980 - val_accuracy: 0.9114 - lr: 3.0000e-05\n",
      "Epoch 13/40\n",
      "581/581 [==============================] - 49s 85ms/step - loss: 0.2982 - accuracy: 0.9154 - val_loss: 0.2421 - val_accuracy: 0.9347 - lr: 3.0000e-05\n",
      "Epoch 14/40\n",
      "581/581 [==============================] - 50s 86ms/step - loss: 0.2606 - accuracy: 0.9364 - val_loss: 0.2175 - val_accuracy: 0.9480 - lr: 3.0000e-05\n",
      "Epoch 15/40\n",
      "581/581 [==============================] - 50s 85ms/step - loss: 0.2370 - accuracy: 0.9454 - val_loss: 0.1953 - val_accuracy: 0.9553 - lr: 3.0000e-05\n",
      "Epoch 16/40\n",
      "581/581 [==============================] - 49s 85ms/step - loss: 0.2279 - accuracy: 0.9479 - val_loss: 0.1950 - val_accuracy: 0.9566 - lr: 3.0000e-05\n",
      "Epoch 17/40\n",
      "581/581 [==============================] - 49s 85ms/step - loss: 0.2069 - accuracy: 0.9533 - val_loss: 0.1822 - val_accuracy: 0.9583 - lr: 3.0000e-05\n",
      "Epoch 18/40\n",
      "581/581 [==============================] - 49s 85ms/step - loss: 0.1949 - accuracy: 0.9596 - val_loss: 0.1705 - val_accuracy: 0.9639 - lr: 3.0000e-05\n",
      "Epoch 19/40\n",
      "581/581 [==============================] - 49s 85ms/step - loss: 0.1900 - accuracy: 0.9588 - val_loss: 0.1698 - val_accuracy: 0.9647 - lr: 3.0000e-05\n",
      "Epoch 20/40\n",
      "581/581 [==============================] - 49s 85ms/step - loss: 0.1811 - accuracy: 0.9629 - val_loss: 0.1527 - val_accuracy: 0.9733 - lr: 3.0000e-05\n",
      "Epoch 21/40\n",
      "581/581 [==============================] - 49s 85ms/step - loss: 0.1715 - accuracy: 0.9674 - val_loss: 0.2332 - val_accuracy: 0.9454 - lr: 3.0000e-05\n",
      "Epoch 22/40\n",
      "581/581 [==============================] - 49s 85ms/step - loss: 0.1661 - accuracy: 0.9685 - val_loss: 0.1429 - val_accuracy: 0.9755 - lr: 3.0000e-05\n",
      "Epoch 23/40\n",
      "581/581 [==============================] - 50s 85ms/step - loss: 0.1546 - accuracy: 0.9727 - val_loss: 0.1315 - val_accuracy: 0.9815 - lr: 3.0000e-05\n",
      "Epoch 24/40\n",
      "581/581 [==============================] - 49s 85ms/step - loss: 0.1501 - accuracy: 0.9749 - val_loss: 0.1309 - val_accuracy: 0.9828 - lr: 3.0000e-05\n",
      "Epoch 25/40\n",
      "581/581 [==============================] - 50s 85ms/step - loss: 0.1439 - accuracy: 0.9760 - val_loss: 0.1245 - val_accuracy: 0.9845 - lr: 3.0000e-05\n",
      "Epoch 26/40\n",
      "581/581 [==============================] - 49s 85ms/step - loss: 0.1421 - accuracy: 0.9775 - val_loss: 0.1333 - val_accuracy: 0.9789 - lr: 3.0000e-05\n",
      "Epoch 27/40\n",
      "581/581 [==============================] - 49s 85ms/step - loss: 0.1416 - accuracy: 0.9791 - val_loss: 0.1293 - val_accuracy: 0.9811 - lr: 3.0000e-05\n",
      "Epoch 28/40\n",
      "581/581 [==============================] - 49s 85ms/step - loss: 0.1248 - accuracy: 0.9835 - val_loss: 0.1166 - val_accuracy: 0.9897 - lr: 6.0000e-06\n",
      "Epoch 29/40\n",
      "581/581 [==============================] - 50s 85ms/step - loss: 0.1187 - accuracy: 0.9861 - val_loss: 0.1172 - val_accuracy: 0.9897 - lr: 6.0000e-06\n",
      "Epoch 30/40\n",
      "581/581 [==============================] - 49s 85ms/step - loss: 0.1227 - accuracy: 0.9848 - val_loss: 0.1193 - val_accuracy: 0.9884 - lr: 6.0000e-06\n",
      "Epoch 31/40\n",
      "581/581 [==============================] - 49s 85ms/step - loss: 0.1169 - accuracy: 0.9860 - val_loss: 0.1171 - val_accuracy: 0.9893 - lr: 1.2000e-06\n",
      "Epoch 32/40\n",
      "581/581 [==============================] - 50s 85ms/step - loss: 0.1179 - accuracy: 0.9872 - val_loss: 0.1169 - val_accuracy: 0.9884 - lr: 1.2000e-06\n",
      "Epoch 33/40\n",
      "581/581 [==============================] - 49s 85ms/step - loss: 0.1160 - accuracy: 0.9887 - val_loss: 0.1161 - val_accuracy: 0.9884 - lr: 2.4000e-07\n",
      "Epoch 34/40\n",
      "581/581 [==============================] - 49s 85ms/step - loss: 0.1177 - accuracy: 0.9860 - val_loss: 0.1162 - val_accuracy: 0.9897 - lr: 2.4000e-07\n",
      "Epoch 35/40\n",
      "581/581 [==============================] - 49s 85ms/step - loss: 0.1154 - accuracy: 0.9865 - val_loss: 0.1159 - val_accuracy: 0.9893 - lr: 2.4000e-07\n",
      "Epoch 36/40\n",
      "581/581 [==============================] - 49s 84ms/step - loss: 0.1160 - accuracy: 0.9870 - val_loss: 0.1160 - val_accuracy: 0.9888 - lr: 2.4000e-07\n",
      "Epoch 37/40\n",
      "581/581 [==============================] - 49s 85ms/step - loss: 0.1168 - accuracy: 0.9859 - val_loss: 0.1160 - val_accuracy: 0.9893 - lr: 2.4000e-07\n",
      "Epoch 38/40\n",
      "581/581 [==============================] - 49s 85ms/step - loss: 0.1157 - accuracy: 0.9865 - val_loss: 0.1156 - val_accuracy: 0.9897 - lr: 4.8000e-08\n",
      "Epoch 39/40\n",
      "581/581 [==============================] - 49s 85ms/step - loss: 0.1190 - accuracy: 0.9858 - val_loss: 0.1161 - val_accuracy: 0.9884 - lr: 4.8000e-08\n",
      "Epoch 40/40\n",
      "581/581 [==============================] - 49s 85ms/step - loss: 0.1142 - accuracy: 0.9872 - val_loss: 0.1160 - val_accuracy: 0.9884 - lr: 4.8000e-08\n",
      "Epoch 1/40\n",
      "581/581 [==============================] - 51s 85ms/step - loss: 0.5411 - accuracy: 0.8024 - val_loss: 0.4683 - val_accuracy: 0.8383 - lr: 3.0000e-05\n",
      "Epoch 2/40\n",
      "581/581 [==============================] - 49s 84ms/step - loss: 0.4648 - accuracy: 0.8437 - val_loss: 0.4308 - val_accuracy: 0.8474 - lr: 3.0000e-05\n",
      "Epoch 3/40\n",
      "581/581 [==============================] - 49s 84ms/step - loss: 0.4484 - accuracy: 0.8527 - val_loss: 0.4160 - val_accuracy: 0.8581 - lr: 3.0000e-05\n",
      "Epoch 4/40\n",
      "581/581 [==============================] - 49s 85ms/step - loss: 0.4379 - accuracy: 0.8584 - val_loss: 0.4095 - val_accuracy: 0.8633 - lr: 3.0000e-05\n",
      "Epoch 5/40\n",
      "581/581 [==============================] - 49s 85ms/step - loss: 0.4211 - accuracy: 0.8615 - val_loss: 0.3973 - val_accuracy: 0.8654 - lr: 3.0000e-05\n",
      "Epoch 6/40\n",
      "581/581 [==============================] - 49s 84ms/step - loss: 0.4099 - accuracy: 0.8641 - val_loss: 0.4036 - val_accuracy: 0.8646 - lr: 3.0000e-05\n",
      "Epoch 7/40\n",
      "581/581 [==============================] - 49s 85ms/step - loss: 0.3994 - accuracy: 0.8700 - val_loss: 0.3807 - val_accuracy: 0.8762 - lr: 3.0000e-05\n",
      "Epoch 8/40\n",
      "581/581 [==============================] - 49s 85ms/step - loss: 0.3786 - accuracy: 0.8808 - val_loss: 0.3491 - val_accuracy: 0.8891 - lr: 3.0000e-05\n",
      "Epoch 9/40\n",
      "581/581 [==============================] - 49s 85ms/step - loss: 0.3665 - accuracy: 0.8863 - val_loss: 0.3511 - val_accuracy: 0.8951 - lr: 3.0000e-05\n",
      "Epoch 10/40\n",
      "581/581 [==============================] - 49s 85ms/step - loss: 0.3533 - accuracy: 0.8915 - val_loss: 0.3137 - val_accuracy: 0.9093 - lr: 3.0000e-05\n",
      "Epoch 11/40\n",
      "581/581 [==============================] - 49s 85ms/step - loss: 0.3305 - accuracy: 0.8984 - val_loss: 0.3063 - val_accuracy: 0.9063 - lr: 3.0000e-05\n",
      "Epoch 12/40\n",
      "581/581 [==============================] - 49s 85ms/step - loss: 0.3161 - accuracy: 0.9082 - val_loss: 0.3024 - val_accuracy: 0.9093 - lr: 3.0000e-05\n",
      "Epoch 13/40\n",
      "581/581 [==============================] - 49s 85ms/step - loss: 0.2928 - accuracy: 0.9177 - val_loss: 0.2513 - val_accuracy: 0.9347 - lr: 3.0000e-05\n",
      "Epoch 14/40\n",
      "581/581 [==============================] - 49s 85ms/step - loss: 0.2562 - accuracy: 0.9334 - val_loss: 0.2201 - val_accuracy: 0.9480 - lr: 3.0000e-05\n",
      "Epoch 15/40\n",
      "581/581 [==============================] - 49s 85ms/step - loss: 0.2317 - accuracy: 0.9438 - val_loss: 0.1980 - val_accuracy: 0.9566 - lr: 3.0000e-05\n",
      "Epoch 16/40\n",
      "581/581 [==============================] - 49s 85ms/step - loss: 0.2156 - accuracy: 0.9497 - val_loss: 0.1873 - val_accuracy: 0.9604 - lr: 3.0000e-05\n",
      "Epoch 17/40\n",
      "581/581 [==============================] - 49s 85ms/step - loss: 0.2016 - accuracy: 0.9546 - val_loss: 0.1977 - val_accuracy: 0.9518 - lr: 3.0000e-05\n",
      "Epoch 18/40\n",
      "581/581 [==============================] - 49s 84ms/step - loss: 0.1841 - accuracy: 0.9641 - val_loss: 0.1876 - val_accuracy: 0.9587 - lr: 3.0000e-05\n",
      "Epoch 19/40\n",
      "581/581 [==============================] - 49s 85ms/step - loss: 0.1697 - accuracy: 0.9654 - val_loss: 0.1655 - val_accuracy: 0.9695 - lr: 6.0000e-06\n",
      "Epoch 20/40\n",
      "581/581 [==============================] - 49s 85ms/step - loss: 0.1672 - accuracy: 0.9688 - val_loss: 0.1583 - val_accuracy: 0.9686 - lr: 6.0000e-06\n",
      "Epoch 21/40\n",
      "581/581 [==============================] - 49s 85ms/step - loss: 0.1661 - accuracy: 0.9688 - val_loss: 0.1527 - val_accuracy: 0.9712 - lr: 6.0000e-06\n",
      "Epoch 22/40\n",
      "581/581 [==============================] - 50s 86ms/step - loss: 0.1592 - accuracy: 0.9704 - val_loss: 0.1516 - val_accuracy: 0.9751 - lr: 6.0000e-06\n",
      "Epoch 23/40\n",
      "581/581 [==============================] - 49s 84ms/step - loss: 0.1624 - accuracy: 0.9699 - val_loss: 0.1504 - val_accuracy: 0.9725 - lr: 6.0000e-06\n",
      "Epoch 24/40\n",
      "581/581 [==============================] - 49s 84ms/step - loss: 0.1628 - accuracy: 0.9717 - val_loss: 0.1469 - val_accuracy: 0.9742 - lr: 6.0000e-06\n",
      "Epoch 25/40\n",
      "581/581 [==============================] - 49s 84ms/step - loss: 0.1603 - accuracy: 0.9707 - val_loss: 0.1455 - val_accuracy: 0.9764 - lr: 6.0000e-06\n",
      "Epoch 26/40\n",
      "581/581 [==============================] - 49s 84ms/step - loss: 0.1544 - accuracy: 0.9720 - val_loss: 0.1451 - val_accuracy: 0.9772 - lr: 6.0000e-06\n",
      "Epoch 27/40\n",
      "581/581 [==============================] - 49s 84ms/step - loss: 0.1506 - accuracy: 0.9762 - val_loss: 0.1419 - val_accuracy: 0.9764 - lr: 6.0000e-06\n",
      "Epoch 28/40\n",
      "581/581 [==============================] - 49s 84ms/step - loss: 0.1512 - accuracy: 0.9749 - val_loss: 0.1440 - val_accuracy: 0.9755 - lr: 6.0000e-06\n",
      "Epoch 29/40\n",
      "581/581 [==============================] - 49s 84ms/step - loss: 0.1507 - accuracy: 0.9732 - val_loss: 0.1419 - val_accuracy: 0.9755 - lr: 6.0000e-06\n",
      "Epoch 30/40\n",
      "581/581 [==============================] - 49s 84ms/step - loss: 0.1434 - accuracy: 0.9775 - val_loss: 0.1412 - val_accuracy: 0.9781 - lr: 1.2000e-06\n",
      "Epoch 31/40\n",
      "581/581 [==============================] - 50s 85ms/step - loss: 0.1482 - accuracy: 0.9746 - val_loss: 0.1402 - val_accuracy: 0.9794 - lr: 1.2000e-06\n",
      "Epoch 32/40\n",
      "581/581 [==============================] - 50s 86ms/step - loss: 0.1460 - accuracy: 0.9764 - val_loss: 0.1403 - val_accuracy: 0.9794 - lr: 1.2000e-06\n",
      "Epoch 33/40\n",
      "581/581 [==============================] - 49s 84ms/step - loss: 0.1448 - accuracy: 0.9769 - val_loss: 0.1394 - val_accuracy: 0.9807 - lr: 1.2000e-06\n",
      "Epoch 34/40\n",
      "581/581 [==============================] - 48s 83ms/step - loss: 0.1470 - accuracy: 0.9765 - val_loss: 0.1385 - val_accuracy: 0.9764 - lr: 1.2000e-06\n",
      "Epoch 35/40\n",
      "581/581 [==============================] - 49s 84ms/step - loss: 0.1432 - accuracy: 0.9771 - val_loss: 0.1380 - val_accuracy: 0.9815 - lr: 1.2000e-06\n",
      "Epoch 36/40\n",
      "581/581 [==============================] - 49s 84ms/step - loss: 0.1443 - accuracy: 0.9751 - val_loss: 0.1382 - val_accuracy: 0.9794 - lr: 1.2000e-06\n",
      "Epoch 37/40\n",
      "581/581 [==============================] - 49s 84ms/step - loss: 0.1490 - accuracy: 0.9755 - val_loss: 0.1381 - val_accuracy: 0.9811 - lr: 1.2000e-06\n",
      "Epoch 38/40\n",
      "581/581 [==============================] - 49s 85ms/step - loss: 0.1404 - accuracy: 0.9788 - val_loss: 0.1374 - val_accuracy: 0.9811 - lr: 2.4000e-07\n",
      "Epoch 39/40\n",
      "581/581 [==============================] - 50s 86ms/step - loss: 0.1447 - accuracy: 0.9764 - val_loss: 0.1371 - val_accuracy: 0.9811 - lr: 2.4000e-07\n",
      "Epoch 40/40\n",
      "581/581 [==============================] - 49s 85ms/step - loss: 0.1425 - accuracy: 0.9787 - val_loss: 0.1368 - val_accuracy: 0.9807 - lr: 2.4000e-07\n"
     ]
    }
   ],
   "source": [
    "epochs = 40\n",
    "batch_size = 16\n",
    "\n",
    "val_acc_per_fold = []\n",
    "acc_per_fold = []\n",
    "val_loss_per_fold = []\n",
    "loss_per_fold = []\n",
    "sets = [(fold_1, fold_1_lab), (fold_2, fold_2_lab), (fold_3, fold_3_lab), (fold_4, fold_4_lab), (fold_5, fold_5_lab)]\n",
    "\n",
    "for n in range(len(sets)):\n",
    "    test_x = sets[n][0]\n",
    "    test_y = sets[n][1]\n",
    "    train_x = pd.DataFrame()\n",
    "    train_y = pd.DataFrame()\n",
    "    for s in sets[:n] + sets[n+1:]:\n",
    "        train_x = pd.concat([train_x, s[0]])\n",
    "        train_y = pd.concat([train_y, s[1]])\n",
    "\n",
    "    classifier_test_opt_bi = build_model_bidirectional()\n",
    "\n",
    "    history_test_opt_bi = classifier_test_opt_bi.fit(\n",
    "        np.array(train_x), np.array(train_y),\n",
    "        epochs=epochs,\n",
    "        batch_size = batch_size,\n",
    "        steps_per_epoch=train_x.shape[0] // batch_size,\n",
    "        validation_data=(np.array(test_x), np.array(test_y)),\n",
    "        callbacks=[tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=1e-8)]\n",
    "    )\n",
    "\n",
    "    val_acc_per_fold.append(history_test_opt_bi.history['val_accuracy'])\n",
    "    acc_per_fold.append(history_test_opt_bi.history['accuracy'])\n",
    "    val_loss_per_fold.append(history_test_opt_bi.history['val_loss'])\n",
    "    loss_per_fold.append(history_test_opt_bi.history['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[0.8361994624137878,\n",
       "   0.8542562127113342,\n",
       "   0.8336199522018433,\n",
       "   0.8632845878601074,\n",
       "   0.8645743727684021,\n",
       "   0.8718830347061157,\n",
       "   0.8852106332778931,\n",
       "   0.8890799880027771,\n",
       "   0.8989681601524353,\n",
       "   0.8993980884552002,\n",
       "   0.9019776582717896,\n",
       "   0.9174548387527466,\n",
       "   0.9363714456558228,\n",
       "   0.945399820804596,\n",
       "   0.9544281959533691,\n",
       "   0.9544281959533691,\n",
       "   0.9630266427993774,\n",
       "   0.9582974910736084,\n",
       "   0.9664660096168518,\n",
       "   0.970765233039856,\n",
       "   0.9724849462509155,\n",
       "   0.9789338111877441,\n",
       "   0.9772140979766846,\n",
       "   0.9750645160675049,\n",
       "   0.9785038828849792,\n",
       "   0.9858125448226929,\n",
       "   0.9858125448226929,\n",
       "   0.9832330346107483,\n",
       "   0.9845227599143982,\n",
       "   0.9871023297309875,\n",
       "   0.9875322580337524,\n",
       "   0.9875322580337524,\n",
       "   0.9875322580337524,\n",
       "   0.9875322580337524,\n",
       "   0.9871023297309875,\n",
       "   0.9879621863365173,\n",
       "   0.9883921146392822,\n",
       "   0.9879621863365173,\n",
       "   0.9879621863365173,\n",
       "   0.9875322580337524],\n",
       "  [0.8576956391334534,\n",
       "   0.8701633810997009,\n",
       "   0.8667240142822266,\n",
       "   0.8667240142822266,\n",
       "   0.8645743727684021,\n",
       "   0.8779019713401794,\n",
       "   0.8813413381576538,\n",
       "   0.8839209079742432,\n",
       "   0.8916594982147217,\n",
       "   0.8942390084266663,\n",
       "   0.9213241338729858,\n",
       "   0.9178847670555115,\n",
       "   0.9509888291358948,\n",
       "   0.9617368578910828,\n",
       "   0.9668959379196167,\n",
       "   0.9651762843132019,\n",
       "   0.9673258662223816,\n",
       "   0.970765233039856,\n",
       "   0.9750645160675049,\n",
       "   0.975494384765625,\n",
       "   0.9686156511306763,\n",
       "   0.9789338111877441,\n",
       "   0.9797936081886292,\n",
       "   0.9836629629135132,\n",
       "   0.9815133213996887,\n",
       "   0.9832330346107483,\n",
       "   0.9819432497024536,\n",
       "   0.9849526882171631,\n",
       "   0.9836629629135132,\n",
       "   0.9840928912162781,\n",
       "   0.9845227599143982,\n",
       "   0.985382616519928,\n",
       "   0.9849526882171631,\n",
       "   0.985382616519928,\n",
       "   0.9849526882171631,\n",
       "   0.9849526882171631,\n",
       "   0.985382616519928,\n",
       "   0.9849526882171631,\n",
       "   0.9849526882171631,\n",
       "   0.985382616519928],\n",
       "  [0.8430782556533813,\n",
       "   0.8499569892883301,\n",
       "   0.8568357825279236,\n",
       "   0.8581255078315735,\n",
       "   0.8611350059509277,\n",
       "   0.8727428913116455,\n",
       "   0.8787618279457092,\n",
       "   0.8822011947631836,\n",
       "   0.9006878733634949,\n",
       "   0.8946689367294312,\n",
       "   0.9135855436325073,\n",
       "   0.9213241338729858,\n",
       "   0.9432502388954163,\n",
       "   0.9582974910736084,\n",
       "   0.9574376344680786,\n",
       "   0.9621667861938477,\n",
       "   0.9660361409187317,\n",
       "   0.9724849462509155,\n",
       "   0.9716250896453857,\n",
       "   0.9733448028564453,\n",
       "   0.9789338111877441,\n",
       "   0.9651762843132019,\n",
       "   0.969905436038971,\n",
       "   0.9815133213996887,\n",
       "   0.9836629629135132,\n",
       "   0.9823731780052185,\n",
       "   0.9810833930969238,\n",
       "   0.9823731780052185,\n",
       "   0.9832330346107483,\n",
       "   0.9823731780052185,\n",
       "   0.9823731780052185,\n",
       "   0.9819432497024536,\n",
       "   0.9819432497024536,\n",
       "   0.9823731780052185,\n",
       "   0.9828031063079834,\n",
       "   0.9819432497024536,\n",
       "   0.9828031063079834,\n",
       "   0.9819432497024536,\n",
       "   0.9823731780052185,\n",
       "   0.9819432497024536],\n",
       "  [0.8564058542251587,\n",
       "   0.8671538829803467,\n",
       "   0.8770421147346497,\n",
       "   0.8714531660079956,\n",
       "   0.8701633810997009,\n",
       "   0.8886500597000122,\n",
       "   0.8860704898834229,\n",
       "   0.9019776582717896,\n",
       "   0.9054170250892639,\n",
       "   0.9105761051177979,\n",
       "   0.9127256870269775,\n",
       "   0.9114359617233276,\n",
       "   0.934651792049408,\n",
       "   0.9479793906211853,\n",
       "   0.9552880525588989,\n",
       "   0.9565778374671936,\n",
       "   0.9582974910736084,\n",
       "   0.9638864994049072,\n",
       "   0.964746356010437,\n",
       "   0.9733448028564453,\n",
       "   0.945399820804596,\n",
       "   0.975494384765625,\n",
       "   0.9815133213996887,\n",
       "   0.9828031063079834,\n",
       "   0.9845227599143982,\n",
       "   0.9789338111877441,\n",
       "   0.9810833930969238,\n",
       "   0.9896818399429321,\n",
       "   0.9896818399429321,\n",
       "   0.9883921146392822,\n",
       "   0.9892519116401672,\n",
       "   0.9883921146392822,\n",
       "   0.9883921146392822,\n",
       "   0.9896818399429321,\n",
       "   0.9892519116401672,\n",
       "   0.9888219833374023,\n",
       "   0.9892519116401672,\n",
       "   0.9896818399429321,\n",
       "   0.9883921146392822,\n",
       "   0.9883921146392822],\n",
       "  [0.8383491039276123,\n",
       "   0.8473774790763855,\n",
       "   0.8581255078315735,\n",
       "   0.8632845878601074,\n",
       "   0.8654342293739319,\n",
       "   0.8645743727684021,\n",
       "   0.8761822581291199,\n",
       "   0.8890799880027771,\n",
       "   0.895098865032196,\n",
       "   0.9092863202095032,\n",
       "   0.9062768816947937,\n",
       "   0.9092863202095032,\n",
       "   0.934651792049408,\n",
       "   0.9479793906211853,\n",
       "   0.9565778374671936,\n",
       "   0.9604471325874329,\n",
       "   0.9518486857414246,\n",
       "   0.9587274193763733,\n",
       "   0.969475507736206,\n",
       "   0.9686156511306763,\n",
       "   0.9711951613426208,\n",
       "   0.9750645160675049,\n",
       "   0.9724849462509155,\n",
       "   0.9742046594619751,\n",
       "   0.9763542413711548,\n",
       "   0.9772140979766846,\n",
       "   0.9763542413711548,\n",
       "   0.975494384765625,\n",
       "   0.975494384765625,\n",
       "   0.9780739545822144,\n",
       "   0.979363739490509,\n",
       "   0.979363739490509,\n",
       "   0.9806534647941589,\n",
       "   0.9763542413711548,\n",
       "   0.9815133213996887,\n",
       "   0.979363739490509,\n",
       "   0.9810833930969238,\n",
       "   0.9810833930969238,\n",
       "   0.9810833930969238,\n",
       "   0.9806534647941589]],\n",
       " [[0.7984079122543335,\n",
       "   0.8410852551460266,\n",
       "   0.8479759097099304,\n",
       "   0.8526055216789246,\n",
       "   0.8621877431869507,\n",
       "   0.8670327067375183,\n",
       "   0.8739233613014221,\n",
       "   0.8794143199920654,\n",
       "   0.8885658979415894,\n",
       "   0.8899655342102051,\n",
       "   0.8967484831809998,\n",
       "   0.9053617715835571,\n",
       "   0.9189276695251465,\n",
       "   0.9366925358772278,\n",
       "   0.9436907768249512,\n",
       "   0.9527347087860107,\n",
       "   0.9576873183250427,\n",
       "   0.9613479971885681,\n",
       "   0.9630706310272217,\n",
       "   0.9659776091575623,\n",
       "   0.9689922332763672,\n",
       "   0.9672695994377136,\n",
       "   0.9709302186965942,\n",
       "   0.9757751822471619,\n",
       "   0.9762058854103088,\n",
       "   0.9814814925193787,\n",
       "   0.9824504852294922,\n",
       "   0.9820197820663452,\n",
       "   0.98277348279953,\n",
       "   0.9834194779396057,\n",
       "   0.9839577674865723,\n",
       "   0.9844961166381836,\n",
       "   0.9836347699165344,\n",
       "   0.9850344657897949,\n",
       "   0.9842807650566101,\n",
       "   0.98277348279953,\n",
       "   0.9842807650566101,\n",
       "   0.9853574633598328,\n",
       "   0.984603762626648,\n",
       "   0.984603762626648],\n",
       "  [0.8055077195167542,\n",
       "   0.8388242721557617,\n",
       "   0.8494831919670105,\n",
       "   0.8551895022392273,\n",
       "   0.8538975119590759,\n",
       "   0.8608957529067993,\n",
       "   0.8672480583190918,\n",
       "   0.8732773661613464,\n",
       "   0.8800603151321411,\n",
       "   0.8893195390701294,\n",
       "   0.8977174758911133,\n",
       "   0.9046080708503723,\n",
       "   0.9188199639320374,\n",
       "   0.9358311891555786,\n",
       "   0.9425064325332642,\n",
       "   0.9499353766441345,\n",
       "   0.9566106796264648,\n",
       "   0.9632859826087952,\n",
       "   0.9651162624359131,\n",
       "   0.9714685678482056,\n",
       "   0.973514199256897,\n",
       "   0.9717915654182434,\n",
       "   0.9779285192489624,\n",
       "   0.9802971482276917,\n",
       "   0.9805124998092651,\n",
       "   0.9793281555175781,\n",
       "   0.98277348279953,\n",
       "   0.98277348279953,\n",
       "   0.9805124998092651,\n",
       "   0.9811584949493408,\n",
       "   0.9826657772064209,\n",
       "   0.9812661409378052,\n",
       "   0.9814814925193787,\n",
       "   0.9822351336479187,\n",
       "   0.9821274876594543,\n",
       "   0.9816967844963074,\n",
       "   0.9837424755096436,\n",
       "   0.9829887747764587,\n",
       "   0.9832041263580322,\n",
       "   0.9833117723464966],\n",
       "  [0.7974397540092468,\n",
       "   0.8424849510192871,\n",
       "   0.8515288829803467,\n",
       "   0.8548665046691895,\n",
       "   0.8600344657897949,\n",
       "   0.8584194779396057,\n",
       "   0.8658484220504761,\n",
       "   0.8758613467216492,\n",
       "   0.8845822811126709,\n",
       "   0.8921188712120056,\n",
       "   0.8969638347625732,\n",
       "   0.9054694175720215,\n",
       "   0.9232342839241028,\n",
       "   0.9337855577468872,\n",
       "   0.9506890773773193,\n",
       "   0.9521963596343994,\n",
       "   0.9573643207550049,\n",
       "   0.9644702672958374,\n",
       "   0.9680232405662537,\n",
       "   0.9692075848579407,\n",
       "   0.9713609218597412,\n",
       "   0.9734065532684326,\n",
       "   0.9760981798171997,\n",
       "   0.9776055216789246,\n",
       "   0.9830964803695679,\n",
       "   0.9814814925193787,\n",
       "   0.9806201457977295,\n",
       "   0.98277348279953,\n",
       "   0.9832041263580322,\n",
       "   0.9834194779396057,\n",
       "   0.9818044900894165,\n",
       "   0.9824504852294922,\n",
       "   0.9855727553367615,\n",
       "   0.9855727553367615,\n",
       "   0.9821274876594543,\n",
       "   0.9830964803695679,\n",
       "   0.9837424755096436,\n",
       "   0.9816967844963074,\n",
       "   0.9847114682197571,\n",
       "   0.9834194779396057],\n",
       "  [0.801742672920227,\n",
       "   0.8411929607391357,\n",
       "   0.8471145629882812,\n",
       "   0.8538975119590759,\n",
       "   0.8574504852294922,\n",
       "   0.8599267601966858,\n",
       "   0.8736003637313843,\n",
       "   0.8828595876693726,\n",
       "   0.8901808857917786,\n",
       "   0.893410861492157,\n",
       "   0.9005168080329895,\n",
       "   0.9070844054222107,\n",
       "   0.9153746962547302,\n",
       "   0.9363695383071899,\n",
       "   0.9454134106636047,\n",
       "   0.9478897452354431,\n",
       "   0.9532730579376221,\n",
       "   0.9596253037452698,\n",
       "   0.9587640166282654,\n",
       "   0.9628552794456482,\n",
       "   0.967377245426178,\n",
       "   0.9684539437294006,\n",
       "   0.9726529121398926,\n",
       "   0.9749138951301575,\n",
       "   0.9759905338287354,\n",
       "   0.9774978756904602,\n",
       "   0.9791128635406494,\n",
       "   0.9835271239280701,\n",
       "   0.9861111044883728,\n",
       "   0.9848191142082214,\n",
       "   0.9860034584999084,\n",
       "   0.9871877431869507,\n",
       "   0.9886950850486755,\n",
       "   0.9860034584999084,\n",
       "   0.986541748046875,\n",
       "   0.986972451210022,\n",
       "   0.9858957529067993,\n",
       "   0.986541748046875,\n",
       "   0.985788106918335,\n",
       "   0.9871877431869507],\n",
       "  [0.8023881316184998,\n",
       "   0.8436692357063293,\n",
       "   0.8527131676673889,\n",
       "   0.8584194779396057,\n",
       "   0.861541748046875,\n",
       "   0.8641257286071777,\n",
       "   0.870047390460968,\n",
       "   0.8808139562606812,\n",
       "   0.8863049149513245,\n",
       "   0.8914728760719299,\n",
       "   0.898363471031189,\n",
       "   0.9081610441207886,\n",
       "   0.9177433252334595,\n",
       "   0.9333548545837402,\n",
       "   0.9437984228134155,\n",
       "   0.9497200846672058,\n",
       "   0.9545650482177734,\n",
       "   0.9641472697257996,\n",
       "   0.9654392600059509,\n",
       "   0.9687769412994385,\n",
       "   0.9687769412994385,\n",
       "   0.9703919291496277,\n",
       "   0.9698535799980164,\n",
       "   0.971683919429779,\n",
       "   0.9707149267196655,\n",
       "   0.9720069169998169,\n",
       "   0.9762058854103088,\n",
       "   0.9749138951301575,\n",
       "   0.9731912016868591,\n",
       "   0.9774978756904602,\n",
       "   0.9745908975601196,\n",
       "   0.9764211773872375,\n",
       "   0.9768518805503845,\n",
       "   0.9765288829803467,\n",
       "   0.9770671725273132,\n",
       "   0.9751291871070862,\n",
       "   0.975452184677124,\n",
       "   0.9787898659706116,\n",
       "   0.9764211773872375,\n",
       "   0.9786821603775024]],\n",
       " [[0.4716198742389679,\n",
       "   0.4445933699607849,\n",
       "   0.4640536606311798,\n",
       "   0.4236723482608795,\n",
       "   0.4073012173175812,\n",
       "   0.38998761773109436,\n",
       "   0.38527825474739075,\n",
       "   0.38044726848602295,\n",
       "   0.3542340099811554,\n",
       "   0.34513387084007263,\n",
       "   0.33677753806114197,\n",
       "   0.30032768845558167,\n",
       "   0.2722025513648987,\n",
       "   0.23520322144031525,\n",
       "   0.21910858154296875,\n",
       "   0.1976921707391739,\n",
       "   0.1779211014509201,\n",
       "   0.1872304081916809,\n",
       "   0.16589733958244324,\n",
       "   0.15917204320430756,\n",
       "   0.1673128306865692,\n",
       "   0.1446666419506073,\n",
       "   0.14125517010688782,\n",
       "   0.14997634291648865,\n",
       "   0.14275480806827545,\n",
       "   0.12213477492332458,\n",
       "   0.12070481479167938,\n",
       "   0.12147185951471329,\n",
       "   0.12369155138731003,\n",
       "   0.11676307767629623,\n",
       "   0.11657330393791199,\n",
       "   0.11582053452730179,\n",
       "   0.11570096760988235,\n",
       "   0.1152777224779129,\n",
       "   0.11509556323289871,\n",
       "   0.11570284515619278,\n",
       "   0.11492142081260681,\n",
       "   0.11466427892446518,\n",
       "   0.11466164141893387,\n",
       "   0.11413487792015076],\n",
       "  [0.4445978105068207,\n",
       "   0.4187528192996979,\n",
       "   0.4189583361148834,\n",
       "   0.4065990149974823,\n",
       "   0.4123850166797638,\n",
       "   0.39235448837280273,\n",
       "   0.37886884808540344,\n",
       "   0.36902645230293274,\n",
       "   0.34714657068252563,\n",
       "   0.33868733048439026,\n",
       "   0.30118322372436523,\n",
       "   0.2964608073234558,\n",
       "   0.21999536454677582,\n",
       "   0.1907799392938614,\n",
       "   0.18809525668621063,\n",
       "   0.17566965520381927,\n",
       "   0.1713855266571045,\n",
       "   0.16912858188152313,\n",
       "   0.15575803816318512,\n",
       "   0.14608967304229736,\n",
       "   0.1840411275625229,\n",
       "   0.14844205975532532,\n",
       "   0.12870971858501434,\n",
       "   0.12598001956939697,\n",
       "   0.12848013639450073,\n",
       "   0.12316121906042099,\n",
       "   0.1230434775352478,\n",
       "   0.12298203259706497,\n",
       "   0.1260497272014618,\n",
       "   0.12027789652347565,\n",
       "   0.12026670575141907,\n",
       "   0.12079929560422897,\n",
       "   0.12031190097332001,\n",
       "   0.12013500183820724,\n",
       "   0.12005389481782913,\n",
       "   0.12010396271944046,\n",
       "   0.12017734348773956,\n",
       "   0.12001442909240723,\n",
       "   0.12017200142145157,\n",
       "   0.12000477313995361],\n",
       "  [0.4583454728126526,\n",
       "   0.43102261424064636,\n",
       "   0.41606491804122925,\n",
       "   0.4035623371601105,\n",
       "   0.39288175106048584,\n",
       "   0.38136380910873413,\n",
       "   0.36984628438949585,\n",
       "   0.3697332441806793,\n",
       "   0.3345189690589905,\n",
       "   0.34223535656929016,\n",
       "   0.30504634976387024,\n",
       "   0.2819059193134308,\n",
       "   0.2443009465932846,\n",
       "   0.19838763773441315,\n",
       "   0.18569183349609375,\n",
       "   0.18085676431655884,\n",
       "   0.16172482073307037,\n",
       "   0.15394189953804016,\n",
       "   0.1552388221025467,\n",
       "   0.15193256735801697,\n",
       "   0.1371401697397232,\n",
       "   0.16406528651714325,\n",
       "   0.1566912680864334,\n",
       "   0.12656325101852417,\n",
       "   0.12586334347724915,\n",
       "   0.1271955668926239,\n",
       "   0.12718690931797028,\n",
       "   0.12308450043201447,\n",
       "   0.12320678681135178,\n",
       "   0.12362220883369446,\n",
       "   0.12326405197381973,\n",
       "   0.12319725751876831,\n",
       "   0.1231837347149849,\n",
       "   0.12320359796285629,\n",
       "   0.12327844649553299,\n",
       "   0.12337929010391235,\n",
       "   0.12283948063850403,\n",
       "   0.1232856884598732,\n",
       "   0.12302794307470322,\n",
       "   0.12314416468143463],\n",
       "  [0.44383710622787476,\n",
       "   0.40871164202690125,\n",
       "   0.40075838565826416,\n",
       "   0.3989517390727997,\n",
       "   0.40796253085136414,\n",
       "   0.3848193287849426,\n",
       "   0.3657090663909912,\n",
       "   0.3374245762825012,\n",
       "   0.3214547038078308,\n",
       "   0.3014885485172272,\n",
       "   0.28948351740837097,\n",
       "   0.29801684617996216,\n",
       "   0.24206776916980743,\n",
       "   0.21747170388698578,\n",
       "   0.19533461332321167,\n",
       "   0.1949601173400879,\n",
       "   0.1822056770324707,\n",
       "   0.17050990462303162,\n",
       "   0.16975060105323792,\n",
       "   0.15267020463943481,\n",
       "   0.23316602408885956,\n",
       "   0.14288197457790375,\n",
       "   0.13147853314876556,\n",
       "   0.1308842897415161,\n",
       "   0.12454582005739212,\n",
       "   0.13331101834774017,\n",
       "   0.12933869659900665,\n",
       "   0.11664249747991562,\n",
       "   0.11717928946018219,\n",
       "   0.11933498829603195,\n",
       "   0.11713915318250656,\n",
       "   0.11687475442886353,\n",
       "   0.11612674593925476,\n",
       "   0.1162005215883255,\n",
       "   0.1158980205655098,\n",
       "   0.11603936553001404,\n",
       "   0.11596034467220306,\n",
       "   0.11561420559883118,\n",
       "   0.11611440032720566,\n",
       "   0.11604979634284973],\n",
       "  [0.46825987100601196,\n",
       "   0.43077901005744934,\n",
       "   0.4160243272781372,\n",
       "   0.40949124097824097,\n",
       "   0.3973354995250702,\n",
       "   0.40362295508384705,\n",
       "   0.38068753480911255,\n",
       "   0.34914135932922363,\n",
       "   0.3510715961456299,\n",
       "   0.31368544697761536,\n",
       "   0.30631911754608154,\n",
       "   0.3024454414844513,\n",
       "   0.2513141632080078,\n",
       "   0.22009673714637756,\n",
       "   0.1979881227016449,\n",
       "   0.18733006715774536,\n",
       "   0.1976960152387619,\n",
       "   0.18763230741024017,\n",
       "   0.16549648344516754,\n",
       "   0.15825499594211578,\n",
       "   0.1527293622493744,\n",
       "   0.15162445604801178,\n",
       "   0.1503782570362091,\n",
       "   0.1468803882598877,\n",
       "   0.14554578065872192,\n",
       "   0.14508554339408875,\n",
       "   0.14188286662101746,\n",
       "   0.1440434455871582,\n",
       "   0.14185266196727753,\n",
       "   0.14117693901062012,\n",
       "   0.14018653333187103,\n",
       "   0.14025439321994781,\n",
       "   0.1393986940383911,\n",
       "   0.1384565830230713,\n",
       "   0.13804221153259277,\n",
       "   0.13823939859867096,\n",
       "   0.13805916905403137,\n",
       "   0.13739033043384552,\n",
       "   0.13705883920192719,\n",
       "   0.13682062923908234]],\n",
       " [[0.5407373309135437,\n",
       "   0.46671995520591736,\n",
       "   0.447406530380249,\n",
       "   0.4407844841480255,\n",
       "   0.42462125420570374,\n",
       "   0.4110878109931946,\n",
       "   0.3963334858417511,\n",
       "   0.37829190492630005,\n",
       "   0.3646954298019409,\n",
       "   0.3611399829387665,\n",
       "   0.33697381615638733,\n",
       "   0.32310280203819275,\n",
       "   0.2915208339691162,\n",
       "   0.24959631264209747,\n",
       "   0.23309281468391418,\n",
       "   0.21063897013664246,\n",
       "   0.19735278189182281,\n",
       "   0.18722105026245117,\n",
       "   0.17990510165691376,\n",
       "   0.1741376668214798,\n",
       "   0.16332386434078217,\n",
       "   0.16461753845214844,\n",
       "   0.15737417340278625,\n",
       "   0.1480473130941391,\n",
       "   0.14801810681819916,\n",
       "   0.13270702958106995,\n",
       "   0.1344863325357437,\n",
       "   0.13198892772197723,\n",
       "   0.128689244389534,\n",
       "   0.1283622533082962,\n",
       "   0.12447386980056763,\n",
       "   0.12502451241016388,\n",
       "   0.12691597640514374,\n",
       "   0.12052363157272339,\n",
       "   0.1234326958656311,\n",
       "   0.1256815493106842,\n",
       "   0.12610848248004913,\n",
       "   0.12315905094146729,\n",
       "   0.1231214851140976,\n",
       "   0.1254512518644333],\n",
       "  [0.5360897183418274,\n",
       "   0.46892815828323364,\n",
       "   0.4486096501350403,\n",
       "   0.4345704913139343,\n",
       "   0.43367379903793335,\n",
       "   0.4222867786884308,\n",
       "   0.4084947109222412,\n",
       "   0.3987795412540436,\n",
       "   0.3806583881378174,\n",
       "   0.3605087399482727,\n",
       "   0.34351062774658203,\n",
       "   0.3253985643386841,\n",
       "   0.28770169615745544,\n",
       "   0.2514791786670685,\n",
       "   0.22864791750907898,\n",
       "   0.2154940962791443,\n",
       "   0.2014927864074707,\n",
       "   0.18567882478237152,\n",
       "   0.17738644778728485,\n",
       "   0.16676177084445953,\n",
       "   0.15724782645702362,\n",
       "   0.16199037432670593,\n",
       "   0.1426001936197281,\n",
       "   0.13774989545345306,\n",
       "   0.1380409300327301,\n",
       "   0.13825832307338715,\n",
       "   0.13719075918197632,\n",
       "   0.13295575976371765,\n",
       "   0.13685089349746704,\n",
       "   0.13035207986831665,\n",
       "   0.12851469218730927,\n",
       "   0.12893593311309814,\n",
       "   0.13328981399536133,\n",
       "   0.130762979388237,\n",
       "   0.13021600246429443,\n",
       "   0.12898916006088257,\n",
       "   0.12693984806537628,\n",
       "   0.12685656547546387,\n",
       "   0.12839701771736145,\n",
       "   0.12980963289737701],\n",
       "  [0.5433049201965332,\n",
       "   0.46584248542785645,\n",
       "   0.44608545303344727,\n",
       "   0.4365983009338379,\n",
       "   0.43207353353500366,\n",
       "   0.41911980509757996,\n",
       "   0.4048168957233429,\n",
       "   0.38653865456581116,\n",
       "   0.37378305196762085,\n",
       "   0.3559771478176117,\n",
       "   0.33748582005500793,\n",
       "   0.31623539328575134,\n",
       "   0.2878716289997101,\n",
       "   0.25092148780822754,\n",
       "   0.21620754897594452,\n",
       "   0.20914140343666077,\n",
       "   0.19756542146205902,\n",
       "   0.18450644612312317,\n",
       "   0.1702035516500473,\n",
       "   0.16748228669166565,\n",
       "   0.16365617513656616,\n",
       "   0.1521347463130951,\n",
       "   0.1497660130262375,\n",
       "   0.14355407655239105,\n",
       "   0.13056328892707825,\n",
       "   0.13174299895763397,\n",
       "   0.134269580245018,\n",
       "   0.12844951450824738,\n",
       "   0.12742485105991364,\n",
       "   0.12721925973892212,\n",
       "   0.1292816400527954,\n",
       "   0.13004472851753235,\n",
       "   0.12512201070785522,\n",
       "   0.12388378381729126,\n",
       "   0.1307150423526764,\n",
       "   0.1253865361213684,\n",
       "   0.12494063377380371,\n",
       "   0.1299196034669876,\n",
       "   0.12545351684093475,\n",
       "   0.13011376559734344],\n",
       "  [0.536845862865448,\n",
       "   0.46854665875434875,\n",
       "   0.44994214177131653,\n",
       "   0.4406895637512207,\n",
       "   0.4278436303138733,\n",
       "   0.42183762788772583,\n",
       "   0.39819881319999695,\n",
       "   0.3815389573574066,\n",
       "   0.35935670137405396,\n",
       "   0.34849438071250916,\n",
       "   0.33830517530441284,\n",
       "   0.3179936408996582,\n",
       "   0.2981531322002411,\n",
       "   0.2605942487716675,\n",
       "   0.23695792257785797,\n",
       "   0.22787357866764069,\n",
       "   0.20690147578716278,\n",
       "   0.1949402242898941,\n",
       "   0.19003167748451233,\n",
       "   0.18114672601222992,\n",
       "   0.17147761583328247,\n",
       "   0.16611914336681366,\n",
       "   0.15455088019371033,\n",
       "   0.15011045336723328,\n",
       "   0.1439315378665924,\n",
       "   0.1420501172542572,\n",
       "   0.14163535833358765,\n",
       "   0.12479370832443237,\n",
       "   0.11867610365152359,\n",
       "   0.12267167121171951,\n",
       "   0.11688525229692459,\n",
       "   0.11794885247945786,\n",
       "   0.11602499336004257,\n",
       "   0.11774404346942902,\n",
       "   0.11538819968700409,\n",
       "   0.1160028725862503,\n",
       "   0.1168377473950386,\n",
       "   0.115713931620121,\n",
       "   0.11903591454029083,\n",
       "   0.11417713016271591],\n",
       "  [0.5411302447319031,\n",
       "   0.46483150124549866,\n",
       "   0.4483815133571625,\n",
       "   0.43790724873542786,\n",
       "   0.4211289584636688,\n",
       "   0.40987175703048706,\n",
       "   0.3994167745113373,\n",
       "   0.3785965144634247,\n",
       "   0.3665412366390228,\n",
       "   0.353284627199173,\n",
       "   0.3304734230041504,\n",
       "   0.3160898983478546,\n",
       "   0.29283618927001953,\n",
       "   0.2561887204647064,\n",
       "   0.231729194521904,\n",
       "   0.21558092534542084,\n",
       "   0.2016230672597885,\n",
       "   0.184149831533432,\n",
       "   0.16971658170223236,\n",
       "   0.16719529032707214,\n",
       "   0.1661129891872406,\n",
       "   0.15923812985420227,\n",
       "   0.16237947344779968,\n",
       "   0.16283483803272247,\n",
       "   0.1603403091430664,\n",
       "   0.15439026057720184,\n",
       "   0.15064097940921783,\n",
       "   0.15119947493076324,\n",
       "   0.15070362389087677,\n",
       "   0.14339543879032135,\n",
       "   0.1481761485338211,\n",
       "   0.14603158831596375,\n",
       "   0.14481082558631897,\n",
       "   0.14704962074756622,\n",
       "   0.1432417929172516,\n",
       "   0.14433801174163818,\n",
       "   0.14896047115325928,\n",
       "   0.14043767750263214,\n",
       "   0.14468605816364288,\n",
       "   0.1425352394580841]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_acc_per_fold, acc_per_fold, val_loss_per_fold, loss_per_fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "results = {\"val_acc_per_fold\": val_acc_per_fold, \"acc_per_fold\": acc_per_fold, \"val_loss_per_fold\": val_loss_per_fold, \"loss_per_fold\": loss_per_fold}\n",
    "\n",
    "with open('results_model_bi_small_lr.pickle', 'wb') as file:\n",
    " pickle.dump(results, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9875322580337524\n",
      "0.985382616519928\n",
      "0.9819432497024536\n",
      "0.9883921146392822\n",
      "0.9806534647941589\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.984780740737915"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 0\n",
    "for fold in val_acc_per_fold:\n",
    "    print(fold[-1])\n",
    "    s += fold[-1]\n",
    "s/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as gru_cell_16_layer_call_fn, gru_cell_16_layer_call_and_return_conditional_losses, gru_cell_17_layer_call_fn, gru_cell_17_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: classifier_test_opt_bi_small_lr\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: classifier_test_opt_bi_small_lr\\assets\n"
     ]
    }
   ],
   "source": [
    "# Calling `save('my_model')` creates a SavedModel folder `my_model`.\n",
    "classifier_test_opt_bi.save(\"classifier_test_opt_bi_small_lr\")\n",
    "\n",
    "# It can be used to reconstruct the model identically.\n",
    "#reconstructed_model = keras.models.load_model(\"classifier_test_opt_bi_small_lr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed_model = tf.keras.models.load_model(\"classifier_test_opt_bi_small_lr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73/73 [==============================] - 3s 34ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.9996544 ],\n",
       "       [0.9971124 ],\n",
       "       [0.98283744],\n",
       "       ...,\n",
       "       [0.00115953],\n",
       "       [0.00104059],\n",
       "       [0.05627833]], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reconstructed_model.predict(np.array(fold_5))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bidirectional RNN model - randomly created folds, regular lr**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_15\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_16 (InputLayer)       [(None, 453, 1)]          0         \n",
      "                                                                 \n",
      " bidirectional_5 (Bidirectio  (None, 1024)             1582080   \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " batch_normalization_15 (Bat  (None, 1024)             4096      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_30 (Dense)            (None, 128)               131200    \n",
      "                                                                 \n",
      " dropout_15 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_31 (Dense)            (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,717,505\n",
      "Trainable params: 1,715,457\n",
      "Non-trainable params: 2,048\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "581/581 [==============================] - 51s 83ms/step - loss: 0.5316 - accuracy: 0.8184 - val_loss: 0.4895 - val_accuracy: 0.8504 - lr: 3.0000e-04\n",
      "Epoch 2/20\n",
      "581/581 [==============================] - 47s 81ms/step - loss: 0.3937 - accuracy: 0.8764 - val_loss: 0.3130 - val_accuracy: 0.9256 - lr: 3.0000e-04\n",
      "Epoch 3/20\n",
      "581/581 [==============================] - 47s 82ms/step - loss: 0.3570 - accuracy: 0.8872 - val_loss: 0.3721 - val_accuracy: 0.8891 - lr: 3.0000e-04\n",
      "Epoch 4/20\n",
      "581/581 [==============================] - 47s 81ms/step - loss: 0.3270 - accuracy: 0.8989 - val_loss: 0.2410 - val_accuracy: 0.9282 - lr: 3.0000e-04\n",
      "Epoch 5/20\n",
      "581/581 [==============================] - 47s 82ms/step - loss: 0.2322 - accuracy: 0.9415 - val_loss: 0.1532 - val_accuracy: 0.9746 - lr: 3.0000e-04\n",
      "Epoch 6/20\n",
      "581/581 [==============================] - 47s 81ms/step - loss: 0.2251 - accuracy: 0.9436 - val_loss: 0.1776 - val_accuracy: 0.9630 - lr: 3.0000e-04\n",
      "Epoch 7/20\n",
      "581/581 [==============================] - 47s 81ms/step - loss: 0.2159 - accuracy: 0.9477 - val_loss: 0.1358 - val_accuracy: 0.9807 - lr: 3.0000e-04\n",
      "Epoch 8/20\n",
      "581/581 [==============================] - 49s 83ms/step - loss: 0.1314 - accuracy: 0.9792 - val_loss: 0.1165 - val_accuracy: 0.9871 - lr: 3.0000e-04\n",
      "Epoch 9/20\n",
      "581/581 [==============================] - 48s 83ms/step - loss: 0.1146 - accuracy: 0.9869 - val_loss: 0.1127 - val_accuracy: 0.9854 - lr: 3.0000e-04\n",
      "Epoch 10/20\n",
      "581/581 [==============================] - 48s 83ms/step - loss: 0.1075 - accuracy: 0.9891 - val_loss: 0.1058 - val_accuracy: 0.9905 - lr: 3.0000e-04\n",
      "Epoch 11/20\n",
      "581/581 [==============================] - 48s 83ms/step - loss: 0.1092 - accuracy: 0.9861 - val_loss: 0.0916 - val_accuracy: 0.9936 - lr: 3.0000e-04\n",
      "Epoch 12/20\n",
      "581/581 [==============================] - 48s 83ms/step - loss: 0.0891 - accuracy: 0.9939 - val_loss: 0.1026 - val_accuracy: 0.9918 - lr: 3.0000e-04\n",
      "Epoch 13/20\n",
      "581/581 [==============================] - 48s 83ms/step - loss: 0.0866 - accuracy: 0.9924 - val_loss: 0.0813 - val_accuracy: 0.9940 - lr: 3.0000e-04\n",
      "Epoch 14/20\n",
      "581/581 [==============================] - 48s 83ms/step - loss: 0.0850 - accuracy: 0.9920 - val_loss: 0.1116 - val_accuracy: 0.9819 - lr: 3.0000e-04\n",
      "Epoch 15/20\n",
      "581/581 [==============================] - 47s 81ms/step - loss: 0.0834 - accuracy: 0.9924 - val_loss: 0.0882 - val_accuracy: 0.9931 - lr: 3.0000e-04\n",
      "Epoch 16/20\n",
      "581/581 [==============================] - 48s 82ms/step - loss: 0.0672 - accuracy: 0.9972 - val_loss: 0.0728 - val_accuracy: 0.9957 - lr: 6.0000e-05\n",
      "Epoch 17/20\n",
      "581/581 [==============================] - 50s 86ms/step - loss: 0.0624 - accuracy: 0.9980 - val_loss: 0.0680 - val_accuracy: 0.9979 - lr: 6.0000e-05\n",
      "Epoch 18/20\n",
      "581/581 [==============================] - 50s 85ms/step - loss: 0.0599 - accuracy: 0.9987 - val_loss: 0.0708 - val_accuracy: 0.9957 - lr: 6.0000e-05\n",
      "Epoch 19/20\n",
      "581/581 [==============================] - 49s 85ms/step - loss: 0.0566 - accuracy: 0.9990 - val_loss: 0.0714 - val_accuracy: 0.9961 - lr: 6.0000e-05\n",
      "Epoch 20/20\n",
      "581/581 [==============================] - 50s 85ms/step - loss: 0.0563 - accuracy: 0.9990 - val_loss: 0.0657 - val_accuracy: 0.9979 - lr: 1.2000e-05\n",
      "Model: \"model_16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_17 (InputLayer)       [(None, 453, 1)]          0         \n",
      "                                                                 \n",
      " bidirectional_6 (Bidirectio  (None, 1024)             1582080   \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " batch_normalization_16 (Bat  (None, 1024)             4096      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_32 (Dense)            (None, 128)               131200    \n",
      "                                                                 \n",
      " dropout_16 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_33 (Dense)            (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,717,505\n",
      "Trainable params: 1,715,457\n",
      "Non-trainable params: 2,048\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "581/581 [==============================] - 53s 86ms/step - loss: 0.5372 - accuracy: 0.8183 - val_loss: 0.3762 - val_accuracy: 0.8818 - lr: 3.0000e-04\n",
      "Epoch 2/20\n",
      "581/581 [==============================] - 51s 88ms/step - loss: 0.3964 - accuracy: 0.8802 - val_loss: 0.2528 - val_accuracy: 0.9394 - lr: 3.0000e-04\n",
      "Epoch 3/20\n",
      "581/581 [==============================] - 49s 84ms/step - loss: 0.3788 - accuracy: 0.8760 - val_loss: 0.3089 - val_accuracy: 0.9080 - lr: 3.0000e-04\n",
      "Epoch 4/20\n",
      "581/581 [==============================] - 49s 84ms/step - loss: 0.3168 - accuracy: 0.9067 - val_loss: 0.3506 - val_accuracy: 0.9136 - lr: 3.0000e-04\n",
      "Epoch 5/20\n",
      "581/581 [==============================] - 49s 84ms/step - loss: 0.1850 - accuracy: 0.9609 - val_loss: 0.1526 - val_accuracy: 0.9742 - lr: 6.0000e-05\n",
      "Epoch 6/20\n",
      "581/581 [==============================] - 49s 84ms/step - loss: 0.1595 - accuracy: 0.9708 - val_loss: 0.1339 - val_accuracy: 0.9824 - lr: 6.0000e-05\n",
      "Epoch 7/20\n",
      "581/581 [==============================] - 49s 85ms/step - loss: 0.1367 - accuracy: 0.9813 - val_loss: 0.1452 - val_accuracy: 0.9785 - lr: 6.0000e-05\n",
      "Epoch 8/20\n",
      "581/581 [==============================] - 49s 85ms/step - loss: 0.1260 - accuracy: 0.9841 - val_loss: 0.1193 - val_accuracy: 0.9862 - lr: 6.0000e-05\n",
      "Epoch 9/20\n",
      "581/581 [==============================] - 49s 83ms/step - loss: 0.1204 - accuracy: 0.9852 - val_loss: 0.1010 - val_accuracy: 0.9918 - lr: 6.0000e-05\n",
      "Epoch 10/20\n",
      "581/581 [==============================] - 49s 84ms/step - loss: 0.1162 - accuracy: 0.9870 - val_loss: 0.1120 - val_accuracy: 0.9888 - lr: 6.0000e-05\n",
      "Epoch 11/20\n",
      "581/581 [==============================] - 48s 83ms/step - loss: 0.1098 - accuracy: 0.9874 - val_loss: 0.2196 - val_accuracy: 0.9493 - lr: 6.0000e-05\n",
      "Epoch 12/20\n",
      "581/581 [==============================] - 49s 84ms/step - loss: 0.1003 - accuracy: 0.9924 - val_loss: 0.0918 - val_accuracy: 0.9961 - lr: 1.2000e-05\n",
      "Epoch 13/20\n",
      "581/581 [==============================] - 49s 84ms/step - loss: 0.0982 - accuracy: 0.9927 - val_loss: 0.1001 - val_accuracy: 0.9914 - lr: 1.2000e-05\n",
      "Epoch 14/20\n",
      "581/581 [==============================] - 49s 84ms/step - loss: 0.0974 - accuracy: 0.9929 - val_loss: 0.0889 - val_accuracy: 0.9961 - lr: 1.2000e-05\n",
      "Epoch 15/20\n",
      "581/581 [==============================] - 49s 84ms/step - loss: 0.0969 - accuracy: 0.9940 - val_loss: 0.0882 - val_accuracy: 0.9966 - lr: 1.2000e-05\n",
      "Epoch 16/20\n",
      "581/581 [==============================] - 49s 84ms/step - loss: 0.0923 - accuracy: 0.9945 - val_loss: 0.0949 - val_accuracy: 0.9948 - lr: 1.2000e-05\n",
      "Epoch 17/20\n",
      "581/581 [==============================] - 49s 84ms/step - loss: 0.0943 - accuracy: 0.9932 - val_loss: 0.0882 - val_accuracy: 0.9961 - lr: 1.2000e-05\n",
      "Epoch 18/20\n",
      "581/581 [==============================] - 49s 84ms/step - loss: 0.0903 - accuracy: 0.9956 - val_loss: 0.0871 - val_accuracy: 0.9957 - lr: 2.4000e-06\n",
      "Epoch 19/20\n",
      "581/581 [==============================] - 49s 84ms/step - loss: 0.0900 - accuracy: 0.9950 - val_loss: 0.0862 - val_accuracy: 0.9961 - lr: 2.4000e-06\n",
      "Epoch 20/20\n",
      "581/581 [==============================] - 50s 86ms/step - loss: 0.0886 - accuracy: 0.9959 - val_loss: 0.0859 - val_accuracy: 0.9970 - lr: 2.4000e-06\n",
      "Model: \"model_17\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_18 (InputLayer)       [(None, 453, 1)]          0         \n",
      "                                                                 \n",
      " bidirectional_7 (Bidirectio  (None, 1024)             1582080   \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " batch_normalization_17 (Bat  (None, 1024)             4096      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_34 (Dense)            (None, 128)               131200    \n",
      "                                                                 \n",
      " dropout_17 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_35 (Dense)            (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,717,505\n",
      "Trainable params: 1,715,457\n",
      "Non-trainable params: 2,048\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "581/581 [==============================] - 54s 87ms/step - loss: 0.5286 - accuracy: 0.8184 - val_loss: 0.4016 - val_accuracy: 0.8697 - lr: 3.0000e-04\n",
      "Epoch 2/20\n",
      "581/581 [==============================] - 49s 85ms/step - loss: 0.3952 - accuracy: 0.8776 - val_loss: 0.3145 - val_accuracy: 0.9273 - lr: 3.0000e-04\n",
      "Epoch 3/20\n",
      "581/581 [==============================] - 50s 85ms/step - loss: 0.3290 - accuracy: 0.9030 - val_loss: 0.2273 - val_accuracy: 0.9411 - lr: 3.0000e-04\n",
      "Epoch 4/20\n",
      "581/581 [==============================] - 50s 86ms/step - loss: 0.2383 - accuracy: 0.9427 - val_loss: 0.1748 - val_accuracy: 0.9665 - lr: 3.0000e-04\n",
      "Epoch 5/20\n",
      "581/581 [==============================] - 50s 85ms/step - loss: 0.1834 - accuracy: 0.9617 - val_loss: 0.1517 - val_accuracy: 0.9716 - lr: 3.0000e-04\n",
      "Epoch 6/20\n",
      "581/581 [==============================] - 50s 85ms/step - loss: 0.1569 - accuracy: 0.9729 - val_loss: 0.1063 - val_accuracy: 0.9927 - lr: 3.0000e-04\n",
      "Epoch 7/20\n",
      "581/581 [==============================] - 50s 86ms/step - loss: 0.1448 - accuracy: 0.9775 - val_loss: 0.1011 - val_accuracy: 0.9914 - lr: 3.0000e-04\n",
      "Epoch 8/20\n",
      "581/581 [==============================] - 50s 85ms/step - loss: 0.1224 - accuracy: 0.9845 - val_loss: 0.0929 - val_accuracy: 0.9923 - lr: 3.0000e-04\n",
      "Epoch 9/20\n",
      "581/581 [==============================] - 49s 85ms/step - loss: 0.1082 - accuracy: 0.9874 - val_loss: 0.0797 - val_accuracy: 0.9957 - lr: 3.0000e-04\n",
      "Epoch 10/20\n",
      "581/581 [==============================] - 49s 85ms/step - loss: 0.0947 - accuracy: 0.9919 - val_loss: 0.0776 - val_accuracy: 0.9979 - lr: 3.0000e-04\n",
      "Epoch 11/20\n",
      "581/581 [==============================] - 49s 85ms/step - loss: 0.0909 - accuracy: 0.9919 - val_loss: 0.0808 - val_accuracy: 0.9940 - lr: 3.0000e-04\n",
      "Epoch 12/20\n",
      "581/581 [==============================] - 49s 85ms/step - loss: 0.0837 - accuracy: 0.9932 - val_loss: 0.0918 - val_accuracy: 0.9888 - lr: 3.0000e-04\n",
      "Epoch 13/20\n",
      "581/581 [==============================] - 49s 85ms/step - loss: 0.0683 - accuracy: 0.9977 - val_loss: 0.0646 - val_accuracy: 0.9987 - lr: 6.0000e-05\n",
      "Epoch 14/20\n",
      "581/581 [==============================] - 49s 84ms/step - loss: 0.0629 - accuracy: 0.9984 - val_loss: 0.0631 - val_accuracy: 0.9983 - lr: 6.0000e-05\n",
      "Epoch 15/20\n",
      "581/581 [==============================] - 49s 84ms/step - loss: 0.0628 - accuracy: 0.9985 - val_loss: 0.0604 - val_accuracy: 0.9987 - lr: 6.0000e-05\n",
      "Epoch 16/20\n",
      "581/581 [==============================] - 49s 84ms/step - loss: 0.0595 - accuracy: 0.9986 - val_loss: 0.0597 - val_accuracy: 0.9987 - lr: 6.0000e-05\n",
      "Epoch 17/20\n",
      "581/581 [==============================] - 49s 84ms/step - loss: 0.0566 - accuracy: 0.9997 - val_loss: 0.0593 - val_accuracy: 0.9979 - lr: 6.0000e-05\n",
      "Epoch 18/20\n",
      "581/581 [==============================] - 49s 84ms/step - loss: 0.0577 - accuracy: 0.9983 - val_loss: 0.0529 - val_accuracy: 0.9991 - lr: 6.0000e-05\n",
      "Epoch 19/20\n",
      "581/581 [==============================] - 49s 84ms/step - loss: 0.0546 - accuracy: 0.9987 - val_loss: 0.0520 - val_accuracy: 0.9996 - lr: 6.0000e-05\n",
      "Epoch 20/20\n",
      "581/581 [==============================] - 49s 85ms/step - loss: 0.0525 - accuracy: 0.9988 - val_loss: 0.0496 - val_accuracy: 0.9991 - lr: 6.0000e-05\n",
      "Model: \"model_18\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_19 (InputLayer)       [(None, 453, 1)]          0         \n",
      "                                                                 \n",
      " bidirectional_8 (Bidirectio  (None, 1024)             1582080   \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " batch_normalization_18 (Bat  (None, 1024)             4096      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_36 (Dense)            (None, 128)               131200    \n",
      "                                                                 \n",
      " dropout_18 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_37 (Dense)            (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,717,505\n",
      "Trainable params: 1,715,457\n",
      "Non-trainable params: 2,048\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "581/581 [==============================] - 52s 85ms/step - loss: 0.5387 - accuracy: 0.8174 - val_loss: 0.3937 - val_accuracy: 0.8822 - lr: 3.0000e-04\n",
      "Epoch 2/20\n",
      "581/581 [==============================] - 49s 84ms/step - loss: 0.4147 - accuracy: 0.8700 - val_loss: 0.3557 - val_accuracy: 0.8964 - lr: 3.0000e-04\n",
      "Epoch 3/20\n",
      "581/581 [==============================] - 49s 84ms/step - loss: 0.2886 - accuracy: 0.9194 - val_loss: 0.1939 - val_accuracy: 0.9570 - lr: 3.0000e-04\n",
      "Epoch 4/20\n",
      "581/581 [==============================] - 49s 84ms/step - loss: 0.2126 - accuracy: 0.9514 - val_loss: 0.1692 - val_accuracy: 0.9626 - lr: 3.0000e-04\n",
      "Epoch 5/20\n",
      "581/581 [==============================] - 49s 85ms/step - loss: 0.1845 - accuracy: 0.9631 - val_loss: 0.1766 - val_accuracy: 0.9626 - lr: 3.0000e-04\n",
      "Epoch 6/20\n",
      "581/581 [==============================] - 49s 85ms/step - loss: 0.1568 - accuracy: 0.9703 - val_loss: 0.2611 - val_accuracy: 0.9415 - lr: 3.0000e-04\n",
      "Epoch 7/20\n",
      "581/581 [==============================] - 49s 85ms/step - loss: 0.1156 - accuracy: 0.9872 - val_loss: 0.0995 - val_accuracy: 0.9905 - lr: 6.0000e-05\n",
      "Epoch 8/20\n",
      "581/581 [==============================] - 49s 85ms/step - loss: 0.1007 - accuracy: 0.9914 - val_loss: 0.1003 - val_accuracy: 0.9923 - lr: 6.0000e-05\n",
      "Epoch 9/20\n",
      "581/581 [==============================] - 49s 84ms/step - loss: 0.0946 - accuracy: 0.9936 - val_loss: 0.0938 - val_accuracy: 0.9931 - lr: 6.0000e-05\n",
      "Epoch 10/20\n",
      "581/581 [==============================] - 49s 85ms/step - loss: 0.0926 - accuracy: 0.9930 - val_loss: 0.0967 - val_accuracy: 0.9914 - lr: 6.0000e-05\n",
      "Epoch 11/20\n",
      "581/581 [==============================] - 49s 85ms/step - loss: 0.0884 - accuracy: 0.9947 - val_loss: 0.1074 - val_accuracy: 0.9875 - lr: 6.0000e-05\n",
      "Epoch 12/20\n",
      "581/581 [==============================] - 49s 85ms/step - loss: 0.0811 - accuracy: 0.9968 - val_loss: 0.0941 - val_accuracy: 0.9918 - lr: 1.2000e-05\n",
      "Epoch 13/20\n",
      "581/581 [==============================] - 50s 85ms/step - loss: 0.0795 - accuracy: 0.9972 - val_loss: 0.0940 - val_accuracy: 0.9936 - lr: 1.2000e-05\n",
      "Epoch 14/20\n",
      "581/581 [==============================] - 49s 85ms/step - loss: 0.0770 - accuracy: 0.9981 - val_loss: 0.0934 - val_accuracy: 0.9931 - lr: 2.4000e-06\n",
      "Epoch 15/20\n",
      "581/581 [==============================] - 50s 86ms/step - loss: 0.0758 - accuracy: 0.9983 - val_loss: 0.0936 - val_accuracy: 0.9931 - lr: 2.4000e-06\n",
      "Epoch 16/20\n",
      "581/581 [==============================] - 50s 85ms/step - loss: 0.0758 - accuracy: 0.9985 - val_loss: 0.0939 - val_accuracy: 0.9931 - lr: 2.4000e-06\n",
      "Epoch 17/20\n",
      "581/581 [==============================] - 49s 84ms/step - loss: 0.0756 - accuracy: 0.9976 - val_loss: 0.0935 - val_accuracy: 0.9931 - lr: 4.8000e-07\n",
      "Epoch 18/20\n",
      "581/581 [==============================] - 49s 84ms/step - loss: 0.0737 - accuracy: 0.9987 - val_loss: 0.0934 - val_accuracy: 0.9931 - lr: 4.8000e-07\n",
      "Epoch 19/20\n",
      "581/581 [==============================] - 53s 91ms/step - loss: 0.0756 - accuracy: 0.9985 - val_loss: 0.0935 - val_accuracy: 0.9931 - lr: 9.6000e-08\n",
      "Epoch 20/20\n",
      "581/581 [==============================] - 51s 87ms/step - loss: 0.0749 - accuracy: 0.9985 - val_loss: 0.0932 - val_accuracy: 0.9931 - lr: 9.6000e-08\n",
      "Model: \"model_19\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_20 (InputLayer)       [(None, 453, 1)]          0         \n",
      "                                                                 \n",
      " bidirectional_9 (Bidirectio  (None, 1024)             1582080   \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " batch_normalization_19 (Bat  (None, 1024)             4096      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_38 (Dense)            (None, 128)               131200    \n",
      "                                                                 \n",
      " dropout_19 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_39 (Dense)            (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,717,505\n",
      "Trainable params: 1,715,457\n",
      "Non-trainable params: 2,048\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "581/581 [==============================] - 54s 87ms/step - loss: 0.5515 - accuracy: 0.8112 - val_loss: 0.4639 - val_accuracy: 0.8457 - lr: 3.0000e-04\n",
      "Epoch 2/20\n",
      "581/581 [==============================] - 50s 86ms/step - loss: 0.4075 - accuracy: 0.8674 - val_loss: 0.4042 - val_accuracy: 0.8637 - lr: 3.0000e-04\n",
      "Epoch 3/20\n",
      "581/581 [==============================] - 50s 86ms/step - loss: 0.3787 - accuracy: 0.8824 - val_loss: 0.2349 - val_accuracy: 0.9428 - lr: 3.0000e-04\n",
      "Epoch 4/20\n",
      "581/581 [==============================] - 50s 86ms/step - loss: 0.1797 - accuracy: 0.9648 - val_loss: 0.1137 - val_accuracy: 0.9888 - lr: 3.0000e-04\n",
      "Epoch 5/20\n",
      "581/581 [==============================] - 51s 87ms/step - loss: 0.1387 - accuracy: 0.9813 - val_loss: 0.1080 - val_accuracy: 0.9914 - lr: 3.0000e-04\n",
      "Epoch 6/20\n",
      "581/581 [==============================] - 50s 87ms/step - loss: 0.1310 - accuracy: 0.9813 - val_loss: 0.1268 - val_accuracy: 0.9841 - lr: 3.0000e-04\n",
      "Epoch 7/20\n",
      "581/581 [==============================] - 50s 86ms/step - loss: 0.1146 - accuracy: 0.9861 - val_loss: 0.0874 - val_accuracy: 0.9953 - lr: 3.0000e-04\n",
      "Epoch 8/20\n",
      "581/581 [==============================] - 50s 86ms/step - loss: 0.1059 - accuracy: 0.9890 - val_loss: 0.0844 - val_accuracy: 0.9957 - lr: 3.0000e-04\n",
      "Epoch 9/20\n",
      "581/581 [==============================] - 51s 87ms/step - loss: 0.0942 - accuracy: 0.9920 - val_loss: 0.2887 - val_accuracy: 0.9282 - lr: 3.0000e-04\n",
      "Epoch 10/20\n",
      "581/581 [==============================] - 50s 87ms/step - loss: 0.0942 - accuracy: 0.9913 - val_loss: 0.0796 - val_accuracy: 0.9923 - lr: 3.0000e-04\n",
      "Epoch 11/20\n",
      "581/581 [==============================] - 49s 85ms/step - loss: 0.0874 - accuracy: 0.9935 - val_loss: 0.1571 - val_accuracy: 0.9699 - lr: 3.0000e-04\n",
      "Epoch 12/20\n",
      "581/581 [==============================] - 50s 85ms/step - loss: 0.0804 - accuracy: 0.9944 - val_loss: 0.0678 - val_accuracy: 0.9966 - lr: 3.0000e-04\n",
      "Epoch 13/20\n",
      "581/581 [==============================] - 50s 85ms/step - loss: 0.0689 - accuracy: 0.9967 - val_loss: 0.0760 - val_accuracy: 0.9940 - lr: 3.0000e-04\n",
      "Epoch 14/20\n",
      "581/581 [==============================] - 49s 85ms/step - loss: 0.0758 - accuracy: 0.9929 - val_loss: 0.0776 - val_accuracy: 0.9910 - lr: 3.0000e-04\n",
      "Epoch 15/20\n",
      "581/581 [==============================] - 50s 86ms/step - loss: 0.0623 - accuracy: 0.9980 - val_loss: 0.0606 - val_accuracy: 0.9974 - lr: 6.0000e-05\n",
      "Epoch 16/20\n",
      "581/581 [==============================] - 50s 86ms/step - loss: 0.0574 - accuracy: 0.9988 - val_loss: 0.0579 - val_accuracy: 0.9983 - lr: 6.0000e-05\n",
      "Epoch 17/20\n",
      "581/581 [==============================] - 49s 85ms/step - loss: 0.0570 - accuracy: 0.9986 - val_loss: 0.0546 - val_accuracy: 0.9991 - lr: 6.0000e-05\n",
      "Epoch 18/20\n",
      "581/581 [==============================] - 49s 85ms/step - loss: 0.0540 - accuracy: 0.9989 - val_loss: 0.0541 - val_accuracy: 0.9983 - lr: 6.0000e-05\n",
      "Epoch 19/20\n",
      "581/581 [==============================] - 49s 85ms/step - loss: 0.0519 - accuracy: 0.9992 - val_loss: 0.0520 - val_accuracy: 0.9987 - lr: 6.0000e-05\n",
      "Epoch 20/20\n",
      "581/581 [==============================] - 49s 85ms/step - loss: 0.0495 - accuracy: 0.9995 - val_loss: 0.0510 - val_accuracy: 0.9979 - lr: 6.0000e-05\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "batch_size = 16\n",
    "\n",
    "val_acc_per_fold = []\n",
    "acc_per_fold = []\n",
    "val_loss_per_fold = []\n",
    "loss_per_fold = []\n",
    "sets = [(fold_1, fold_1_lab), (fold_2, fold_2_lab), (fold_3, fold_3_lab), (fold_4, fold_4_lab), (fold_5, fold_5_lab)]\n",
    "\n",
    "for n in range(len(sets)):\n",
    "    test_x = sets[n][0]\n",
    "    test_y = sets[n][1]\n",
    "    train_x = pd.DataFrame()\n",
    "    train_y = pd.DataFrame()\n",
    "    for s in sets[:n] + sets[n+1:]:\n",
    "        train_x = pd.concat([train_x, s[0]])\n",
    "        train_y = pd.concat([train_y, s[1]])\n",
    "\n",
    "    classifier_test_opt_bi = build_model_bidirectional()\n",
    "\n",
    "    history_test_opt_bi = classifier_test_opt_bi.fit(\n",
    "        np.array(train_x), np.array(train_y),\n",
    "        epochs=epochs,\n",
    "        batch_size = batch_size,\n",
    "        steps_per_epoch=train_x.shape[0] // batch_size,\n",
    "        validation_data=(np.array(test_x), np.array(test_y)),\n",
    "        callbacks=[tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=1e-8)]\n",
    "    )\n",
    "\n",
    "    val_acc_per_fold.append(history_test_opt_bi.history['val_accuracy'])\n",
    "    acc_per_fold.append(history_test_opt_bi.history['accuracy'])\n",
    "    val_loss_per_fold.append(history_test_opt_bi.history['val_loss'])\n",
    "    loss_per_fold.append(history_test_opt_bi.history['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[0.850386917591095,\n",
       "   0.9256234169006348,\n",
       "   0.8890799880027771,\n",
       "   0.9282029271125793,\n",
       "   0.97463458776474,\n",
       "   0.9630266427993774,\n",
       "   0.9806534647941589,\n",
       "   0.9871023297309875,\n",
       "   0.985382616519928,\n",
       "   0.9905416965484619,\n",
       "   0.9935511350631714,\n",
       "   0.9918314814567566,\n",
       "   0.9939810633659363,\n",
       "   0.9819432497024536,\n",
       "   0.9931212663650513,\n",
       "   0.9957007765769958,\n",
       "   0.9978503584861755,\n",
       "   0.9957007765769958,\n",
       "   0.9961307048797607,\n",
       "   0.9978503584861755],\n",
       "  [0.8817712664604187,\n",
       "   0.9393808841705322,\n",
       "   0.9079965353012085,\n",
       "   0.9135855436325073,\n",
       "   0.9742046594619751,\n",
       "   0.9823731780052185,\n",
       "   0.9785038828849792,\n",
       "   0.9862424731254578,\n",
       "   0.9918314814567566,\n",
       "   0.9888219833374023,\n",
       "   0.9492691159248352,\n",
       "   0.9961307048797607,\n",
       "   0.9914015531539917,\n",
       "   0.9961307048797607,\n",
       "   0.9965606331825256,\n",
       "   0.9948409199714661,\n",
       "   0.9961307048797607,\n",
       "   0.9957007765769958,\n",
       "   0.9961307048797607,\n",
       "   0.9969905614852905],\n",
       "  [0.869733452796936,\n",
       "   0.9273430705070496,\n",
       "   0.9411005973815918,\n",
       "   0.9664660096168518,\n",
       "   0.9716250896453857,\n",
       "   0.9926913380622864,\n",
       "   0.9914015531539917,\n",
       "   0.9922614097595215,\n",
       "   0.9957007765769958,\n",
       "   0.9978503584861755,\n",
       "   0.9939810633659363,\n",
       "   0.9888219833374023,\n",
       "   0.9987102150917053,\n",
       "   0.9982802867889404,\n",
       "   0.9987102150917053,\n",
       "   0.9987102150917053,\n",
       "   0.9978503584861755,\n",
       "   0.9991401433944702,\n",
       "   0.9995700716972351,\n",
       "   0.9991401433944702],\n",
       "  [0.8822011947631836,\n",
       "   0.8963886499404907,\n",
       "   0.9570077657699585,\n",
       "   0.9625967144966125,\n",
       "   0.9625967144966125,\n",
       "   0.9415305256843567,\n",
       "   0.9905416965484619,\n",
       "   0.9922614097595215,\n",
       "   0.9931212663650513,\n",
       "   0.9914015531539917,\n",
       "   0.9875322580337524,\n",
       "   0.9918314814567566,\n",
       "   0.9935511350631714,\n",
       "   0.9931212663650513,\n",
       "   0.9931212663650513,\n",
       "   0.9931212663650513,\n",
       "   0.9931212663650513,\n",
       "   0.9931212663650513,\n",
       "   0.9931212663650513,\n",
       "   0.9931212663650513],\n",
       "  [0.8456577658653259,\n",
       "   0.8637145161628723,\n",
       "   0.9428203105926514,\n",
       "   0.9888219833374023,\n",
       "   0.9914015531539917,\n",
       "   0.9840928912162781,\n",
       "   0.995270848274231,\n",
       "   0.9957007765769958,\n",
       "   0.9282029271125793,\n",
       "   0.9922614097595215,\n",
       "   0.969905436038971,\n",
       "   0.9965606331825256,\n",
       "   0.9939810633659363,\n",
       "   0.9909716248512268,\n",
       "   0.9974204897880554,\n",
       "   0.9982802867889404,\n",
       "   0.9991401433944702,\n",
       "   0.9982802867889404,\n",
       "   0.9987102150917053,\n",
       "   0.9978503584861755]],\n",
       " [[0.8184165358543396,\n",
       "   0.8763996362686157,\n",
       "   0.8871662616729736,\n",
       "   0.8989018201828003,\n",
       "   0.9415374398231506,\n",
       "   0.9435831308364868,\n",
       "   0.9476743936538696,\n",
       "   0.9792205095291138,\n",
       "   0.9868647456169128,\n",
       "   0.9891257286071777,\n",
       "   0.9861111044883728,\n",
       "   0.993863046169281,\n",
       "   0.9923557043075562,\n",
       "   0.9920327067375183,\n",
       "   0.9923557043075562,\n",
       "   0.9972006678581238,\n",
       "   0.9979543685913086,\n",
       "   0.9987080097198486,\n",
       "   0.9990310072898865,\n",
       "   0.9990310072898865],\n",
       "  [0.81830894947052,\n",
       "   0.8801679611206055,\n",
       "   0.8759689927101135,\n",
       "   0.9066537618637085,\n",
       "   0.9609172940254211,\n",
       "   0.9708225727081299,\n",
       "   0.9812661409378052,\n",
       "   0.9840654730796814,\n",
       "   0.9852497577667236,\n",
       "   0.986972451210022,\n",
       "   0.9874030947685242,\n",
       "   0.9923557043075562,\n",
       "   0.992678701877594,\n",
       "   0.9928940534591675,\n",
       "   0.9939706921577454,\n",
       "   0.9945090413093567,\n",
       "   0.9932170510292053,\n",
       "   0.9955856800079346,\n",
       "   0.995047390460968,\n",
       "   0.9959086775779724],\n",
       "  [0.8184165358543396,\n",
       "   0.8775839805603027,\n",
       "   0.9029930830001831,\n",
       "   0.9427217841148376,\n",
       "   0.961670994758606,\n",
       "   0.9728682041168213,\n",
       "   0.9774978756904602,\n",
       "   0.9844961166381836,\n",
       "   0.9874030947685242,\n",
       "   0.991925060749054,\n",
       "   0.991925060749054,\n",
       "   0.9932170510292053,\n",
       "   0.9977390170097351,\n",
       "   0.9983850121498108,\n",
       "   0.9984926581382751,\n",
       "   0.9986003637313843,\n",
       "   0.9996770024299622,\n",
       "   0.9982773661613464,\n",
       "   0.9987080097198486,\n",
       "   0.998815655708313],\n",
       "  [0.8174483776092529,\n",
       "   0.870047390460968,\n",
       "   0.9193583130836487,\n",
       "   0.9514427185058594,\n",
       "   0.9630706310272217,\n",
       "   0.9702842235565186,\n",
       "   0.9871877431869507,\n",
       "   0.9913867115974426,\n",
       "   0.9936476945877075,\n",
       "   0.9930016994476318,\n",
       "   0.9947243928909302,\n",
       "   0.9967700242996216,\n",
       "   0.9972006678581238,\n",
       "   0.998062014579773,\n",
       "   0.9982773661613464,\n",
       "   0.9984926581382751,\n",
       "   0.9976313710212708,\n",
       "   0.9987080097198486,\n",
       "   0.9984926581382751,\n",
       "   0.9984926581382751],\n",
       "  [0.8112091422080994,\n",
       "   0.8673557043075562,\n",
       "   0.8824289441108704,\n",
       "   0.9647932648658752,\n",
       "   0.9812661409378052,\n",
       "   0.9812661409378052,\n",
       "   0.9861111044883728,\n",
       "   0.9890180826187134,\n",
       "   0.9920327067375183,\n",
       "   0.9912790656089783,\n",
       "   0.9935400485992432,\n",
       "   0.9944013953208923,\n",
       "   0.9966623783111572,\n",
       "   0.9928940534591675,\n",
       "   0.9979543685913086,\n",
       "   0.998815655708313,\n",
       "   0.9986003637313843,\n",
       "   0.9989233613014221,\n",
       "   0.99924635887146,\n",
       "   0.9994616508483887]],\n",
       " [[0.4895152747631073,\n",
       "   0.31295663118362427,\n",
       "   0.3720548152923584,\n",
       "   0.2409757375717163,\n",
       "   0.1532256156206131,\n",
       "   0.1776055544614792,\n",
       "   0.1357613503932953,\n",
       "   0.11649508029222488,\n",
       "   0.11268515884876251,\n",
       "   0.10579489171504974,\n",
       "   0.09159594029188156,\n",
       "   0.10260684788227081,\n",
       "   0.08129232376813889,\n",
       "   0.11158426851034164,\n",
       "   0.08819926530122757,\n",
       "   0.07280687987804413,\n",
       "   0.06803169846534729,\n",
       "   0.07076122611761093,\n",
       "   0.07143773138523102,\n",
       "   0.06574917584657669],\n",
       "  [0.37618985772132874,\n",
       "   0.25278037786483765,\n",
       "   0.3089141547679901,\n",
       "   0.3505861759185791,\n",
       "   0.15263822674751282,\n",
       "   0.13387124240398407,\n",
       "   0.14517055451869965,\n",
       "   0.11928057670593262,\n",
       "   0.10097665339708328,\n",
       "   0.1120065450668335,\n",
       "   0.21963781118392944,\n",
       "   0.09179820865392685,\n",
       "   0.10008687525987625,\n",
       "   0.08889161050319672,\n",
       "   0.08822792023420334,\n",
       "   0.09485571831464767,\n",
       "   0.08818210661411285,\n",
       "   0.08709152042865753,\n",
       "   0.08619881421327591,\n",
       "   0.0858607068657875],\n",
       "  [0.4016284644603729,\n",
       "   0.3144812285900116,\n",
       "   0.22728316485881805,\n",
       "   0.1748325079679489,\n",
       "   0.1517421454191208,\n",
       "   0.10628396272659302,\n",
       "   0.10109567642211914,\n",
       "   0.0929277241230011,\n",
       "   0.07967948913574219,\n",
       "   0.0775633230805397,\n",
       "   0.08080515265464783,\n",
       "   0.09178350865840912,\n",
       "   0.06463652104139328,\n",
       "   0.06306341290473938,\n",
       "   0.060408297926187515,\n",
       "   0.059661123901605606,\n",
       "   0.059256117790937424,\n",
       "   0.052925124764442444,\n",
       "   0.052034806460142136,\n",
       "   0.04964762553572655],\n",
       "  [0.39366355538368225,\n",
       "   0.3557072579860687,\n",
       "   0.19392280280590057,\n",
       "   0.16916011273860931,\n",
       "   0.17657026648521423,\n",
       "   0.26112762093544006,\n",
       "   0.09946043789386749,\n",
       "   0.10033290833234787,\n",
       "   0.09378263354301453,\n",
       "   0.09666062891483307,\n",
       "   0.10743214935064316,\n",
       "   0.09411241114139557,\n",
       "   0.0940326452255249,\n",
       "   0.09339720010757446,\n",
       "   0.09362034499645233,\n",
       "   0.09391649067401886,\n",
       "   0.09350010007619858,\n",
       "   0.09337003529071808,\n",
       "   0.09354803711175919,\n",
       "   0.09317582100629807],\n",
       "  [0.46392759680747986,\n",
       "   0.4041872024536133,\n",
       "   0.2349163442850113,\n",
       "   0.11374136060476303,\n",
       "   0.10798130184412003,\n",
       "   0.12681035697460175,\n",
       "   0.08737727999687195,\n",
       "   0.0843842625617981,\n",
       "   0.2886696457862854,\n",
       "   0.0796235203742981,\n",
       "   0.157063290476799,\n",
       "   0.06784482300281525,\n",
       "   0.07599545270204544,\n",
       "   0.07756035029888153,\n",
       "   0.060628365725278854,\n",
       "   0.05792052671313286,\n",
       "   0.05455046519637108,\n",
       "   0.05411521717905998,\n",
       "   0.05198793485760689,\n",
       "   0.0509914830327034]],\n",
       " [[0.5316330194473267,\n",
       "   0.3936918079853058,\n",
       "   0.3570416271686554,\n",
       "   0.3270333409309387,\n",
       "   0.2321634590625763,\n",
       "   0.22506290674209595,\n",
       "   0.21586734056472778,\n",
       "   0.13135743141174316,\n",
       "   0.1146121695637703,\n",
       "   0.10751163959503174,\n",
       "   0.10923390835523605,\n",
       "   0.08906329423189163,\n",
       "   0.0866381824016571,\n",
       "   0.0849716067314148,\n",
       "   0.08335171639919281,\n",
       "   0.06715292483568192,\n",
       "   0.062370482832193375,\n",
       "   0.05992602929472923,\n",
       "   0.056567609310150146,\n",
       "   0.05633499473333359],\n",
       "  [0.5372376441955566,\n",
       "   0.396401584148407,\n",
       "   0.3788023293018341,\n",
       "   0.316771537065506,\n",
       "   0.1849774867296219,\n",
       "   0.15954488515853882,\n",
       "   0.13674296438694,\n",
       "   0.12601204216480255,\n",
       "   0.12038819491863251,\n",
       "   0.1162189394235611,\n",
       "   0.10982304811477661,\n",
       "   0.10028447210788727,\n",
       "   0.09824541956186295,\n",
       "   0.09735066443681717,\n",
       "   0.09689381718635559,\n",
       "   0.0923171192407608,\n",
       "   0.09429114311933517,\n",
       "   0.09033434838056564,\n",
       "   0.09000104665756226,\n",
       "   0.08860424160957336],\n",
       "  [0.5286402106285095,\n",
       "   0.3952344059944153,\n",
       "   0.32899948954582214,\n",
       "   0.23833109438419342,\n",
       "   0.1833551675081253,\n",
       "   0.15688557922840118,\n",
       "   0.1447828710079193,\n",
       "   0.12239503115415573,\n",
       "   0.10818450897932053,\n",
       "   0.09466376155614853,\n",
       "   0.09091681987047195,\n",
       "   0.08369509875774384,\n",
       "   0.06832899898290634,\n",
       "   0.06293587386608124,\n",
       "   0.06277736276388168,\n",
       "   0.0595361553132534,\n",
       "   0.056594401597976685,\n",
       "   0.05765235051512718,\n",
       "   0.05457577854394913,\n",
       "   0.05254937708377838],\n",
       "  [0.5386538505554199,\n",
       "   0.414713054895401,\n",
       "   0.28864315152168274,\n",
       "   0.21259519457817078,\n",
       "   0.18452990055084229,\n",
       "   0.15682880580425262,\n",
       "   0.11562157422304153,\n",
       "   0.1006881594657898,\n",
       "   0.0945959985256195,\n",
       "   0.09257221221923828,\n",
       "   0.08840913325548172,\n",
       "   0.08112049102783203,\n",
       "   0.0795150026679039,\n",
       "   0.07700876146554947,\n",
       "   0.07577864080667496,\n",
       "   0.0757831260561943,\n",
       "   0.07563502341508865,\n",
       "   0.07372445613145828,\n",
       "   0.0755847692489624,\n",
       "   0.0749262124300003],\n",
       "  [0.5514622926712036,\n",
       "   0.4074641466140747,\n",
       "   0.37865161895751953,\n",
       "   0.17970211803913116,\n",
       "   0.13872894644737244,\n",
       "   0.1309601217508316,\n",
       "   0.11459437012672424,\n",
       "   0.10592645406723022,\n",
       "   0.09424671530723572,\n",
       "   0.0941523090004921,\n",
       "   0.08735720813274384,\n",
       "   0.08039908856153488,\n",
       "   0.06888788193464279,\n",
       "   0.07581020146608353,\n",
       "   0.06228897348046303,\n",
       "   0.05736897885799408,\n",
       "   0.05704088136553764,\n",
       "   0.05399405583739281,\n",
       "   0.05186731368303299,\n",
       "   0.0494801290333271]])"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_acc_per_fold, acc_per_fold, val_loss_per_fold, loss_per_fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "results = {\"val_acc_per_fold\": val_acc_per_fold, \"acc_per_fold\": acc_per_fold, \"val_loss_per_fold\": val_loss_per_fold, \"loss_per_fold\": loss_per_fold}\n",
    "\n",
    "with open('results_model_bi_random_folds.pickle', 'wb') as file:\n",
    " pickle.dump(results, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9978503584861755\n",
      "0.9969905614852905\n",
      "0.9991401433944702\n",
      "0.9931212663650513\n",
      "0.9978503584861755\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9969905376434326"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 0\n",
    "for fold in val_acc_per_fold:\n",
    "    print(fold[-1])\n",
    "    s += fold[-1]\n",
    "s/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as gru_cell_38_layer_call_fn, gru_cell_38_layer_call_and_return_conditional_losses, gru_cell_39_layer_call_fn, gru_cell_39_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: classifier_test_opt_bi_random_folds\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: classifier_test_opt_bi_random_folds\\assets\n"
     ]
    }
   ],
   "source": [
    "# Calling `save('my_model')` creates a SavedModel folder `my_model`.\n",
    "classifier_test_opt_bi.save(\"classifier_test_opt_bi_random_folds\")\n",
    "\n",
    "# It can be used to reconstruct the model identically.\n",
    "#reconstructed_model = keras.models.load_model(\"classifier_RNN_random_folds\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simplified RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_RNN():\n",
    "    hidden_dim = 512\n",
    "    seq_len = fold_1.shape[1]\n",
    "    n_features = 1\n",
    "    learning_rate = 0.0003\n",
    "\n",
    "    in1 = tf.keras.layers.Input(shape=(seq_len, n_features))\n",
    "    gru1 = tf.keras.layers.GRU(hidden_dim, name='latent_layer')(in1)\n",
    "    bn = tf.keras.layers.BatchNormalization()(gru1)\n",
    "    dense = tf.keras.layers.Dense(128, activation=\"relu\")(bn)\n",
    "    drop = tf.keras.layers.Dropout(0.3)(dense)\n",
    "    d1   = tf.keras.layers.Dense(1, activation='sigmoid')(drop)\n",
    "\n",
    "    classifier_RNN = tf.keras.Model(\n",
    "        inputs=[in1], \n",
    "        outputs=[d1]\n",
    "    )\n",
    "\n",
    "    #print(classifier_RNN.summary())\n",
    "\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    classifier_RNN.compile(loss='binary_crossentropy', optimizer=opt, \n",
    "        metrics=['accuracy'])\n",
    "    \n",
    "    return classifier_RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "581/581 [==============================] - 33s 53ms/step - loss: 0.4237 - accuracy: 0.8343 - val_loss: 0.5230 - val_accuracy: 0.8401\n",
      "Epoch 2/20\n",
      "581/581 [==============================] - 30s 51ms/step - loss: 0.3276 - accuracy: 0.8767 - val_loss: 0.4277 - val_accuracy: 0.8826\n",
      "Epoch 3/20\n",
      "581/581 [==============================] - 30s 52ms/step - loss: 0.2105 - accuracy: 0.9184 - val_loss: 0.5029 - val_accuracy: 0.7971\n",
      "Epoch 4/20\n",
      "581/581 [==============================] - 30s 52ms/step - loss: 0.1621 - accuracy: 0.9397 - val_loss: 0.2906 - val_accuracy: 0.8938\n",
      "Epoch 5/20\n",
      "581/581 [==============================] - 31s 53ms/step - loss: 0.1620 - accuracy: 0.9360 - val_loss: 0.0864 - val_accuracy: 0.9647\n",
      "Epoch 6/20\n",
      "581/581 [==============================] - 30s 52ms/step - loss: 0.0810 - accuracy: 0.9727 - val_loss: 0.1456 - val_accuracy: 0.9342\n",
      "Epoch 7/20\n",
      "581/581 [==============================] - 30s 51ms/step - loss: 0.1324 - accuracy: 0.9540 - val_loss: 0.1261 - val_accuracy: 0.9523\n",
      "Epoch 8/20\n",
      "581/581 [==============================] - 30s 51ms/step - loss: 0.1120 - accuracy: 0.9621 - val_loss: 0.0904 - val_accuracy: 0.9746\n",
      "Epoch 9/20\n",
      "581/581 [==============================] - 30s 51ms/step - loss: 0.0411 - accuracy: 0.9882 - val_loss: 0.0446 - val_accuracy: 0.9845\n",
      "Epoch 10/20\n",
      "581/581 [==============================] - 30s 51ms/step - loss: 0.0286 - accuracy: 0.9904 - val_loss: 0.0401 - val_accuracy: 0.9858\n",
      "Epoch 11/20\n",
      "581/581 [==============================] - 30s 52ms/step - loss: 0.0243 - accuracy: 0.9913 - val_loss: 0.0309 - val_accuracy: 0.9914\n",
      "Epoch 12/20\n",
      "581/581 [==============================] - 30s 51ms/step - loss: 0.0238 - accuracy: 0.9939 - val_loss: 0.0374 - val_accuracy: 0.9893\n",
      "Epoch 13/20\n",
      "581/581 [==============================] - 29s 50ms/step - loss: 0.0155 - accuracy: 0.9950 - val_loss: 0.0530 - val_accuracy: 0.9815\n",
      "Epoch 14/20\n",
      "581/581 [==============================] - 30s 51ms/step - loss: 0.0169 - accuracy: 0.9950 - val_loss: 0.0381 - val_accuracy: 0.9880\n",
      "Epoch 15/20\n",
      "581/581 [==============================] - 31s 54ms/step - loss: 0.0174 - accuracy: 0.9952 - val_loss: 0.0337 - val_accuracy: 0.9888\n",
      "Epoch 16/20\n",
      "581/581 [==============================] - 30s 51ms/step - loss: 0.0131 - accuracy: 0.9956 - val_loss: 0.0641 - val_accuracy: 0.9807\n",
      "Epoch 17/20\n",
      "581/581 [==============================] - 31s 53ms/step - loss: 0.0116 - accuracy: 0.9956 - val_loss: 0.0562 - val_accuracy: 0.9901\n",
      "Epoch 18/20\n",
      "581/581 [==============================] - 29s 50ms/step - loss: 0.0104 - accuracy: 0.9970 - val_loss: 0.0276 - val_accuracy: 0.9893\n",
      "Epoch 19/20\n",
      "581/581 [==============================] - 29s 51ms/step - loss: 0.0250 - accuracy: 0.9916 - val_loss: 0.0941 - val_accuracy: 0.9751\n",
      "Epoch 20/20\n",
      "581/581 [==============================] - 29s 51ms/step - loss: 0.0136 - accuracy: 0.9960 - val_loss: 0.0469 - val_accuracy: 0.9850\n",
      "Epoch 1/20\n",
      "581/581 [==============================] - 31s 51ms/step - loss: 0.4448 - accuracy: 0.8180 - val_loss: 0.3039 - val_accuracy: 0.9050\n",
      "Epoch 2/20\n",
      "581/581 [==============================] - 28s 49ms/step - loss: 0.3344 - accuracy: 0.8749 - val_loss: 0.4310 - val_accuracy: 0.8822\n",
      "Epoch 3/20\n",
      "581/581 [==============================] - 29s 50ms/step - loss: 0.2625 - accuracy: 0.8984 - val_loss: 0.3124 - val_accuracy: 0.8947\n",
      "Epoch 4/20\n",
      "581/581 [==============================] - 29s 50ms/step - loss: 0.1029 - accuracy: 0.9624 - val_loss: 0.0921 - val_accuracy: 0.9703\n",
      "Epoch 5/20\n",
      "581/581 [==============================] - 30s 51ms/step - loss: 0.2574 - accuracy: 0.8969 - val_loss: 0.5571 - val_accuracy: 0.7666\n",
      "Epoch 6/20\n",
      "581/581 [==============================] - 30s 51ms/step - loss: 0.4565 - accuracy: 0.8049 - val_loss: 0.4408 - val_accuracy: 0.8078\n",
      "Epoch 7/20\n",
      "581/581 [==============================] - 29s 50ms/step - loss: 0.4288 - accuracy: 0.8157 - val_loss: 0.3648 - val_accuracy: 0.8452\n",
      "Epoch 8/20\n",
      "581/581 [==============================] - 28s 49ms/step - loss: 0.3532 - accuracy: 0.8538 - val_loss: 0.3476 - val_accuracy: 0.8478\n",
      "Epoch 9/20\n",
      "581/581 [==============================] - 28s 48ms/step - loss: 0.3503 - accuracy: 0.8516 - val_loss: 0.2297 - val_accuracy: 0.9187\n",
      "Epoch 10/20\n",
      "581/581 [==============================] - 28s 48ms/step - loss: 0.2738 - accuracy: 0.8867 - val_loss: 0.1781 - val_accuracy: 0.9355\n",
      "Epoch 11/20\n",
      "581/581 [==============================] - 28s 48ms/step - loss: 0.2417 - accuracy: 0.9011 - val_loss: 0.1586 - val_accuracy: 0.9428\n",
      "Epoch 12/20\n",
      "581/581 [==============================] - 28s 48ms/step - loss: 0.2119 - accuracy: 0.9100 - val_loss: 0.1443 - val_accuracy: 0.9441\n",
      "Epoch 13/20\n",
      "581/581 [==============================] - 28s 47ms/step - loss: 0.1856 - accuracy: 0.9252 - val_loss: 0.1167 - val_accuracy: 0.9592\n",
      "Epoch 14/20\n",
      "581/581 [==============================] - 28s 48ms/step - loss: 0.1564 - accuracy: 0.9391 - val_loss: 0.0801 - val_accuracy: 0.9699\n",
      "Epoch 15/20\n",
      "581/581 [==============================] - 28s 48ms/step - loss: 0.1735 - accuracy: 0.9341 - val_loss: 0.1049 - val_accuracy: 0.9549\n",
      "Epoch 16/20\n",
      "581/581 [==============================] - 28s 48ms/step - loss: 0.1254 - accuracy: 0.9519 - val_loss: 0.0549 - val_accuracy: 0.9854\n",
      "Epoch 17/20\n",
      "581/581 [==============================] - 28s 48ms/step - loss: 0.1109 - accuracy: 0.9585 - val_loss: 0.0763 - val_accuracy: 0.9738\n",
      "Epoch 18/20\n",
      "581/581 [==============================] - 28s 49ms/step - loss: 0.0996 - accuracy: 0.9611 - val_loss: 0.0346 - val_accuracy: 0.9888\n",
      "Epoch 19/20\n",
      "581/581 [==============================] - 28s 47ms/step - loss: 0.0891 - accuracy: 0.9673 - val_loss: 0.0375 - val_accuracy: 0.9841\n",
      "Epoch 20/20\n",
      "581/581 [==============================] - 28s 48ms/step - loss: 0.0771 - accuracy: 0.9713 - val_loss: 0.0235 - val_accuracy: 0.9957\n",
      "Epoch 1/20\n",
      "581/581 [==============================] - 29s 48ms/step - loss: 0.4429 - accuracy: 0.8170 - val_loss: 0.3453 - val_accuracy: 0.9003\n",
      "Epoch 2/20\n",
      "581/581 [==============================] - 28s 48ms/step - loss: 0.3760 - accuracy: 0.8495 - val_loss: 0.2250 - val_accuracy: 0.9144\n",
      "Epoch 3/20\n",
      "581/581 [==============================] - 28s 48ms/step - loss: 0.2500 - accuracy: 0.9068 - val_loss: 0.1138 - val_accuracy: 0.9579\n",
      "Epoch 4/20\n",
      "581/581 [==============================] - 28s 48ms/step - loss: 0.1792 - accuracy: 0.9380 - val_loss: 0.0518 - val_accuracy: 0.9854\n",
      "Epoch 5/20\n",
      "581/581 [==============================] - 28s 48ms/step - loss: 0.0674 - accuracy: 0.9780 - val_loss: 0.0551 - val_accuracy: 0.9815\n",
      "Epoch 6/20\n",
      "581/581 [==============================] - 28s 48ms/step - loss: 0.1455 - accuracy: 0.9468 - val_loss: 0.0585 - val_accuracy: 0.9781\n",
      "Epoch 7/20\n",
      "581/581 [==============================] - 28s 48ms/step - loss: 0.0690 - accuracy: 0.9764 - val_loss: 0.0206 - val_accuracy: 0.9936\n",
      "Epoch 8/20\n",
      "581/581 [==============================] - 28s 48ms/step - loss: 0.0412 - accuracy: 0.9856 - val_loss: 0.0104 - val_accuracy: 0.9948\n",
      "Epoch 9/20\n",
      "581/581 [==============================] - 28s 48ms/step - loss: 0.0340 - accuracy: 0.9889 - val_loss: 0.0242 - val_accuracy: 0.9923\n",
      "Epoch 10/20\n",
      "581/581 [==============================] - 28s 48ms/step - loss: 0.0236 - accuracy: 0.9929 - val_loss: 0.0054 - val_accuracy: 0.9983\n",
      "Epoch 11/20\n",
      "581/581 [==============================] - 28s 48ms/step - loss: 0.0268 - accuracy: 0.9910 - val_loss: 0.0194 - val_accuracy: 0.9936\n",
      "Epoch 12/20\n",
      "581/581 [==============================] - 28s 48ms/step - loss: 0.0236 - accuracy: 0.9928 - val_loss: 0.0079 - val_accuracy: 0.9983\n",
      "Epoch 13/20\n",
      "581/581 [==============================] - 28s 48ms/step - loss: 0.0177 - accuracy: 0.9934 - val_loss: 8.5939e-04 - val_accuracy: 1.0000\n",
      "Epoch 14/20\n",
      "581/581 [==============================] - 28s 47ms/step - loss: 0.0190 - accuracy: 0.9933 - val_loss: 0.0043 - val_accuracy: 0.9979\n",
      "Epoch 15/20\n",
      "581/581 [==============================] - 28s 48ms/step - loss: 0.0226 - accuracy: 0.9932 - val_loss: 0.0090 - val_accuracy: 0.9974\n",
      "Epoch 16/20\n",
      "581/581 [==============================] - 28s 48ms/step - loss: 0.0162 - accuracy: 0.9938 - val_loss: 0.0191 - val_accuracy: 0.9944\n",
      "Epoch 17/20\n",
      "581/581 [==============================] - 28s 48ms/step - loss: 0.0168 - accuracy: 0.9948 - val_loss: 0.0128 - val_accuracy: 0.9957\n",
      "Epoch 18/20\n",
      "581/581 [==============================] - 28s 48ms/step - loss: 0.1337 - accuracy: 0.9436 - val_loss: 0.1838 - val_accuracy: 0.9067\n",
      "Epoch 19/20\n",
      "581/581 [==============================] - 28s 48ms/step - loss: 0.4401 - accuracy: 0.8230 - val_loss: 0.4153 - val_accuracy: 0.8366\n",
      "Epoch 20/20\n",
      "581/581 [==============================] - 28s 48ms/step - loss: 0.4409 - accuracy: 0.8131 - val_loss: 0.3724 - val_accuracy: 0.8512\n",
      "Epoch 1/20\n",
      "581/581 [==============================] - 29s 48ms/step - loss: 0.4679 - accuracy: 0.8092 - val_loss: 0.2583 - val_accuracy: 0.9076\n",
      "Epoch 2/20\n",
      "581/581 [==============================] - 28s 48ms/step - loss: 0.3720 - accuracy: 0.8534 - val_loss: 0.2311 - val_accuracy: 0.9166\n",
      "Epoch 3/20\n",
      "581/581 [==============================] - 28s 48ms/step - loss: 0.2538 - accuracy: 0.8988 - val_loss: 0.2571 - val_accuracy: 0.8878\n",
      "Epoch 4/20\n",
      "581/581 [==============================] - 27s 47ms/step - loss: 0.1319 - accuracy: 0.9503 - val_loss: 0.1737 - val_accuracy: 0.9450\n",
      "Epoch 5/20\n",
      "581/581 [==============================] - 27s 46ms/step - loss: 0.0838 - accuracy: 0.9709 - val_loss: 0.4976 - val_accuracy: 0.8164\n",
      "Epoch 6/20\n",
      "581/581 [==============================] - 28s 48ms/step - loss: 0.1717 - accuracy: 0.9338 - val_loss: 0.2745 - val_accuracy: 0.8809\n",
      "Epoch 7/20\n",
      "581/581 [==============================] - 28s 48ms/step - loss: 0.1411 - accuracy: 0.9497 - val_loss: 0.5154 - val_accuracy: 0.7880\n",
      "Epoch 8/20\n",
      "581/581 [==============================] - 28s 48ms/step - loss: 0.1331 - accuracy: 0.9481 - val_loss: 0.1217 - val_accuracy: 0.9527\n",
      "Epoch 9/20\n",
      "581/581 [==============================] - 28s 48ms/step - loss: 0.0697 - accuracy: 0.9784 - val_loss: 0.1260 - val_accuracy: 0.9514\n",
      "Epoch 10/20\n",
      "581/581 [==============================] - 28s 48ms/step - loss: 0.0515 - accuracy: 0.9829 - val_loss: 0.1905 - val_accuracy: 0.9372\n",
      "Epoch 11/20\n",
      "581/581 [==============================] - 28s 48ms/step - loss: 0.0438 - accuracy: 0.9861 - val_loss: 0.1693 - val_accuracy: 0.9475\n",
      "Epoch 12/20\n",
      "581/581 [==============================] - 28s 48ms/step - loss: 0.0325 - accuracy: 0.9907 - val_loss: 0.0582 - val_accuracy: 0.9819\n",
      "Epoch 13/20\n",
      "581/581 [==============================] - 27s 47ms/step - loss: 0.0337 - accuracy: 0.9892 - val_loss: 0.2792 - val_accuracy: 0.8990\n",
      "Epoch 14/20\n",
      "581/581 [==============================] - 27s 46ms/step - loss: 0.0331 - accuracy: 0.9877 - val_loss: 0.0585 - val_accuracy: 0.9819\n",
      "Epoch 15/20\n",
      "581/581 [==============================] - 27s 47ms/step - loss: 0.0224 - accuracy: 0.9925 - val_loss: 0.3218 - val_accuracy: 0.9037\n",
      "Epoch 16/20\n",
      "581/581 [==============================] - 28s 48ms/step - loss: 0.0341 - accuracy: 0.9891 - val_loss: 0.0886 - val_accuracy: 0.9776\n",
      "Epoch 17/20\n",
      "581/581 [==============================] - 28s 47ms/step - loss: 0.0163 - accuracy: 0.9942 - val_loss: 0.2281 - val_accuracy: 0.9415\n",
      "Epoch 18/20\n",
      "581/581 [==============================] - 28s 48ms/step - loss: 0.0177 - accuracy: 0.9941 - val_loss: 0.0968 - val_accuracy: 0.9733\n",
      "Epoch 19/20\n",
      "581/581 [==============================] - 28s 48ms/step - loss: 0.0247 - accuracy: 0.9926 - val_loss: 0.1141 - val_accuracy: 0.9686\n",
      "Epoch 20/20\n",
      "581/581 [==============================] - 28s 47ms/step - loss: 0.0167 - accuracy: 0.9942 - val_loss: 0.0712 - val_accuracy: 0.9837\n",
      "Epoch 1/20\n",
      "581/581 [==============================] - 30s 49ms/step - loss: 0.3875 - accuracy: 0.8475 - val_loss: 0.5819 - val_accuracy: 0.8138\n",
      "Epoch 2/20\n",
      "581/581 [==============================] - 28s 48ms/step - loss: 0.3017 - accuracy: 0.8872 - val_loss: 0.4901 - val_accuracy: 0.8246\n",
      "Epoch 3/20\n",
      "581/581 [==============================] - 28s 48ms/step - loss: 0.1819 - accuracy: 0.9324 - val_loss: 0.2753 - val_accuracy: 0.8904\n",
      "Epoch 4/20\n",
      "581/581 [==============================] - 27s 47ms/step - loss: 0.1218 - accuracy: 0.9568 - val_loss: 0.3413 - val_accuracy: 0.8749\n",
      "Epoch 5/20\n",
      "581/581 [==============================] - 29s 51ms/step - loss: 0.0848 - accuracy: 0.9714 - val_loss: 0.1809 - val_accuracy: 0.9334\n",
      "Epoch 6/20\n",
      "581/581 [==============================] - 31s 54ms/step - loss: 0.0653 - accuracy: 0.9775 - val_loss: 0.1505 - val_accuracy: 0.9557\n",
      "Epoch 7/20\n",
      "581/581 [==============================] - 31s 52ms/step - loss: 0.0394 - accuracy: 0.9877 - val_loss: 0.1642 - val_accuracy: 0.9471\n",
      "Epoch 8/20\n",
      "581/581 [==============================] - 30s 51ms/step - loss: 0.0336 - accuracy: 0.9896 - val_loss: 0.1779 - val_accuracy: 0.9506\n",
      "Epoch 9/20\n",
      "581/581 [==============================] - 30s 52ms/step - loss: 0.0640 - accuracy: 0.9778 - val_loss: 0.2563 - val_accuracy: 0.9261\n",
      "Epoch 10/20\n",
      "581/581 [==============================] - 31s 53ms/step - loss: 0.0282 - accuracy: 0.9905 - val_loss: 0.1390 - val_accuracy: 0.9647\n",
      "Epoch 11/20\n",
      "581/581 [==============================] - 31s 53ms/step - loss: 0.0222 - accuracy: 0.9930 - val_loss: 0.1116 - val_accuracy: 0.9703\n",
      "Epoch 12/20\n",
      "581/581 [==============================] - 30s 52ms/step - loss: 0.0167 - accuracy: 0.9952 - val_loss: 0.1825 - val_accuracy: 0.9592\n",
      "Epoch 13/20\n",
      "581/581 [==============================] - 29s 51ms/step - loss: 0.0189 - accuracy: 0.9938 - val_loss: 0.1733 - val_accuracy: 0.9626\n",
      "Epoch 14/20\n",
      "581/581 [==============================] - 30s 52ms/step - loss: 0.0118 - accuracy: 0.9971 - val_loss: 0.1304 - val_accuracy: 0.9690\n",
      "Epoch 15/20\n",
      "581/581 [==============================] - 31s 53ms/step - loss: 0.0093 - accuracy: 0.9976 - val_loss: 0.1856 - val_accuracy: 0.9570\n",
      "Epoch 16/20\n",
      "581/581 [==============================] - 30s 52ms/step - loss: 0.0266 - accuracy: 0.9911 - val_loss: 0.1827 - val_accuracy: 0.9557\n",
      "Epoch 17/20\n",
      "581/581 [==============================] - 29s 51ms/step - loss: 0.0064 - accuracy: 0.9978 - val_loss: 0.2610 - val_accuracy: 0.9437\n",
      "Epoch 18/20\n",
      "581/581 [==============================] - 30s 52ms/step - loss: 0.0130 - accuracy: 0.9966 - val_loss: 0.2689 - val_accuracy: 0.9415\n",
      "Epoch 19/20\n",
      "581/581 [==============================] - 30s 52ms/step - loss: 0.0128 - accuracy: 0.9960 - val_loss: 0.1970 - val_accuracy: 0.9617\n",
      "Epoch 20/20\n",
      "581/581 [==============================] - 30s 52ms/step - loss: 0.0042 - accuracy: 0.9995 - val_loss: 0.2076 - val_accuracy: 0.9656\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "batch_size = 16\n",
    "\n",
    "val_acc_per_fold = []\n",
    "acc_per_fold = []\n",
    "val_loss_per_fold = []\n",
    "loss_per_fold = []\n",
    "sets = [(fold_1, fold_1_lab), (fold_2, fold_2_lab), (fold_3, fold_3_lab), (fold_4, fold_4_lab), (fold_5, fold_5_lab)]\n",
    "\n",
    "for n in range(len(sets)):\n",
    "    test_x = sets[n][0]\n",
    "    test_y = sets[n][1]\n",
    "    train_x = pd.DataFrame()\n",
    "    train_y = pd.DataFrame()\n",
    "    for s in sets[:n] + sets[n+1:]:\n",
    "        train_x = pd.concat([train_x, s[0]])\n",
    "        train_y = pd.concat([train_y, s[1]])\n",
    "\n",
    "    classifier_RNN = build_model_RNN()\n",
    "\n",
    "    history_RNN = classifier_RNN.fit(\n",
    "        np.array(train_x), np.array(train_y),\n",
    "        epochs=epochs,\n",
    "        batch_size = batch_size,\n",
    "        steps_per_epoch=train_x.shape[0] // batch_size,\n",
    "        validation_data=(np.array(test_x), np.array(test_y))\n",
    "    )\n",
    "\n",
    "    val_acc_per_fold.append(history_RNN.history['val_accuracy'])\n",
    "    acc_per_fold.append(history_RNN.history['accuracy'])\n",
    "    val_loss_per_fold.append(history_RNN.history['val_loss'])\n",
    "    loss_per_fold.append(history_RNN.history['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[0.8400688171386719,\n",
       "   0.8826311230659485,\n",
       "   0.7970765233039856,\n",
       "   0.8938091397285461,\n",
       "   0.964746356010437,\n",
       "   0.9342218637466431,\n",
       "   0.9522786140441895,\n",
       "   0.97463458776474,\n",
       "   0.9845227599143982,\n",
       "   0.9858125448226929,\n",
       "   0.9914015531539917,\n",
       "   0.9892519116401672,\n",
       "   0.9815133213996887,\n",
       "   0.9879621863365173,\n",
       "   0.9888219833374023,\n",
       "   0.9806534647941589,\n",
       "   0.990111768245697,\n",
       "   0.9892519116401672,\n",
       "   0.9750645160675049,\n",
       "   0.9849526882171631],\n",
       "  [0.904987096786499,\n",
       "   0.8822011947631836,\n",
       "   0.8946689367294312,\n",
       "   0.9703353643417358,\n",
       "   0.7665520310401917,\n",
       "   0.8078246116638184,\n",
       "   0.845227837562561,\n",
       "   0.8478074073791504,\n",
       "   0.9187446236610413,\n",
       "   0.935511589050293,\n",
       "   0.9428203105926514,\n",
       "   0.9441100358963013,\n",
       "   0.9591573476791382,\n",
       "   0.969905436038971,\n",
       "   0.954858124256134,\n",
       "   0.985382616519928,\n",
       "   0.9737747311592102,\n",
       "   0.9888219833374023,\n",
       "   0.9840928912162781,\n",
       "   0.9957007765769958],\n",
       "  [0.90025794506073,\n",
       "   0.9144454002380371,\n",
       "   0.9578675627708435,\n",
       "   0.985382616519928,\n",
       "   0.9815133213996887,\n",
       "   0.9780739545822144,\n",
       "   0.9935511350631714,\n",
       "   0.9948409199714661,\n",
       "   0.9922614097595215,\n",
       "   0.9982802867889404,\n",
       "   0.9935511350631714,\n",
       "   0.9982802867889404,\n",
       "   1.0,\n",
       "   0.9978503584861755,\n",
       "   0.9974204897880554,\n",
       "   0.9944109916687012,\n",
       "   0.9957007765769958,\n",
       "   0.9067068099975586,\n",
       "   0.8366293907165527,\n",
       "   0.8512467741966248],\n",
       "  [0.9075666666030884,\n",
       "   0.9165950417518616,\n",
       "   0.8877902030944824,\n",
       "   0.944969892501831,\n",
       "   0.8164230585098267,\n",
       "   0.8809114098548889,\n",
       "   0.7880481481552124,\n",
       "   0.9527084827423096,\n",
       "   0.9514187574386597,\n",
       "   0.9372313022613525,\n",
       "   0.9475494623184204,\n",
       "   0.9819432497024536,\n",
       "   0.8989681601524353,\n",
       "   0.9819432497024536,\n",
       "   0.9036973118782043,\n",
       "   0.9776440262794495,\n",
       "   0.9415305256843567,\n",
       "   0.9733448028564453,\n",
       "   0.9686156511306763,\n",
       "   0.9836629629135132],\n",
       "  [0.8138434886932373,\n",
       "   0.8245915770530701,\n",
       "   0.890369713306427,\n",
       "   0.87489253282547,\n",
       "   0.9333620071411133,\n",
       "   0.9557179808616638,\n",
       "   0.9471195340156555,\n",
       "   0.9505589008331299,\n",
       "   0.9260532855987549,\n",
       "   0.964746356010437,\n",
       "   0.9703353643417358,\n",
       "   0.9591573476791382,\n",
       "   0.9625967144966125,\n",
       "   0.9690455794334412,\n",
       "   0.9570077657699585,\n",
       "   0.9557179808616638,\n",
       "   0.9436801671981812,\n",
       "   0.9415305256843567,\n",
       "   0.9617368578910828,\n",
       "   0.9656062126159668]],\n",
       " [[0.8343373537063599,\n",
       "   0.8767226338386536,\n",
       "   0.9183893203735352,\n",
       "   0.9397071599960327,\n",
       "   0.9360465407371521,\n",
       "   0.9726529121398926,\n",
       "   0.9540266990661621,\n",
       "   0.9621016383171082,\n",
       "   0.9881567358970642,\n",
       "   0.9904177188873291,\n",
       "   0.9912790656089783,\n",
       "   0.993863046169281,\n",
       "   0.995047390460968,\n",
       "   0.995047390460968,\n",
       "   0.9951550364494324,\n",
       "   0.9955856800079346,\n",
       "   0.9955856800079346,\n",
       "   0.9969853758811951,\n",
       "   0.9916020631790161,\n",
       "   0.9960163831710815],\n",
       "  [0.817986249923706,\n",
       "   0.8748923540115356,\n",
       "   0.898363471031189,\n",
       "   0.962424635887146,\n",
       "   0.8968561291694641,\n",
       "   0.8049095869064331,\n",
       "   0.8156761527061462,\n",
       "   0.8537898659706116,\n",
       "   0.851636528968811,\n",
       "   0.8867355585098267,\n",
       "   0.901055097579956,\n",
       "   0.9099913835525513,\n",
       "   0.9251722693443298,\n",
       "   0.939061164855957,\n",
       "   0.934108555316925,\n",
       "   0.9518733620643616,\n",
       "   0.9585486650466919,\n",
       "   0.9611326456069946,\n",
       "   0.9672695994377136,\n",
       "   0.9712532162666321],\n",
       "  [0.8170180916786194,\n",
       "   0.8494831919670105,\n",
       "   0.9067614078521729,\n",
       "   0.9379844665527344,\n",
       "   0.9780361652374268,\n",
       "   0.9468131065368652,\n",
       "   0.9764211773872375,\n",
       "   0.9855727553367615,\n",
       "   0.988910436630249,\n",
       "   0.9928940534591675,\n",
       "   0.9909560680389404,\n",
       "   0.9927864074707031,\n",
       "   0.9934324026107788,\n",
       "   0.9933246970176697,\n",
       "   0.9932170510292053,\n",
       "   0.9937554001808167,\n",
       "   0.9948320388793945,\n",
       "   0.9435831308364868,\n",
       "   0.8229973912239075,\n",
       "   0.8130921721458435],\n",
       "  [0.8091652393341064,\n",
       "   0.8533591628074646,\n",
       "   0.8987941145896912,\n",
       "   0.9502583742141724,\n",
       "   0.9709302186965942,\n",
       "   0.9337855577468872,\n",
       "   0.9497200846672058,\n",
       "   0.9481050968170166,\n",
       "   0.9783591628074646,\n",
       "   0.9828811287879944,\n",
       "   0.9861111044883728,\n",
       "   0.9907407164573669,\n",
       "   0.9892334342002869,\n",
       "   0.987726092338562,\n",
       "   0.9924634099006653,\n",
       "   0.9891257286071777,\n",
       "   0.9941860437393188,\n",
       "   0.9940783977508545,\n",
       "   0.9925710558891296,\n",
       "   0.9941860437393188],\n",
       "  [0.8474612832069397,\n",
       "   0.8871662616729736,\n",
       "   0.9323858618736267,\n",
       "   0.9568260312080383,\n",
       "   0.9713609218597412,\n",
       "   0.9774978756904602,\n",
       "   0.987726092338562,\n",
       "   0.9895564317703247,\n",
       "   0.977820873260498,\n",
       "   0.9905254244804382,\n",
       "   0.9930016994476318,\n",
       "   0.9951550364494324,\n",
       "   0.9937554001808167,\n",
       "   0.9970930218696594,\n",
       "   0.9976313710212708,\n",
       "   0.9910637140274048,\n",
       "   0.9978466629981995,\n",
       "   0.9965546727180481,\n",
       "   0.9960163831710815,\n",
       "   0.9994616508483887]],\n",
       " [[0.5229824781417847,\n",
       "   0.4277411699295044,\n",
       "   0.5028591156005859,\n",
       "   0.2905687391757965,\n",
       "   0.08641129732131958,\n",
       "   0.1455664485692978,\n",
       "   0.12611649930477142,\n",
       "   0.09037826210260391,\n",
       "   0.0445760041475296,\n",
       "   0.04012080281972885,\n",
       "   0.03085952065885067,\n",
       "   0.03743673861026764,\n",
       "   0.052986644208431244,\n",
       "   0.038066279143095016,\n",
       "   0.03373504430055618,\n",
       "   0.06410741806030273,\n",
       "   0.056235894560813904,\n",
       "   0.02764694206416607,\n",
       "   0.09414254128932953,\n",
       "   0.04690011963248253],\n",
       "  [0.3038581907749176,\n",
       "   0.4310455620288849,\n",
       "   0.31238436698913574,\n",
       "   0.0921492725610733,\n",
       "   0.5570570230484009,\n",
       "   0.44078782200813293,\n",
       "   0.3648202419281006,\n",
       "   0.34760376811027527,\n",
       "   0.22973759472370148,\n",
       "   0.17811590433120728,\n",
       "   0.15857857465744019,\n",
       "   0.1442956030368805,\n",
       "   0.11665387451648712,\n",
       "   0.08011601865291595,\n",
       "   0.10487576574087143,\n",
       "   0.05486442893743515,\n",
       "   0.07626044005155563,\n",
       "   0.03460557386279106,\n",
       "   0.03745844587683678,\n",
       "   0.02352752722799778],\n",
       "  [0.3453095853328705,\n",
       "   0.22503383457660675,\n",
       "   0.11378761380910873,\n",
       "   0.051793817430734634,\n",
       "   0.05514530837535858,\n",
       "   0.05852612853050232,\n",
       "   0.020584682002663612,\n",
       "   0.010413667187094688,\n",
       "   0.02420349419116974,\n",
       "   0.005356263369321823,\n",
       "   0.019373958930373192,\n",
       "   0.007855761796236038,\n",
       "   0.0008593854145146906,\n",
       "   0.004296949598938227,\n",
       "   0.008978589437901974,\n",
       "   0.01908414252102375,\n",
       "   0.012775345705449581,\n",
       "   0.18378308415412903,\n",
       "   0.4152938425540924,\n",
       "   0.37236660718917847],\n",
       "  [0.258262038230896,\n",
       "   0.23111337423324585,\n",
       "   0.2570782005786896,\n",
       "   0.17372092604637146,\n",
       "   0.49764561653137207,\n",
       "   0.27451297640800476,\n",
       "   0.5153589248657227,\n",
       "   0.12165673822164536,\n",
       "   0.12595681846141815,\n",
       "   0.19052179157733917,\n",
       "   0.16926056146621704,\n",
       "   0.05823404714465141,\n",
       "   0.2792159616947174,\n",
       "   0.05846570432186127,\n",
       "   0.32180681824684143,\n",
       "   0.08861242234706879,\n",
       "   0.2281114161014557,\n",
       "   0.09677648544311523,\n",
       "   0.1141231432557106,\n",
       "   0.0711774080991745],\n",
       "  [0.5818850994110107,\n",
       "   0.4901016056537628,\n",
       "   0.27534791827201843,\n",
       "   0.3412838578224182,\n",
       "   0.18087393045425415,\n",
       "   0.15048182010650635,\n",
       "   0.1641511172056198,\n",
       "   0.17786157131195068,\n",
       "   0.25630730390548706,\n",
       "   0.13903935253620148,\n",
       "   0.11163254082202911,\n",
       "   0.18251916766166687,\n",
       "   0.17333649098873138,\n",
       "   0.13043822348117828,\n",
       "   0.18560750782489777,\n",
       "   0.18267682194709778,\n",
       "   0.2610226571559906,\n",
       "   0.26885274052619934,\n",
       "   0.19700294733047485,\n",
       "   0.2075882852077484]],\n",
       " [[0.423659086227417,\n",
       "   0.3275652527809143,\n",
       "   0.21051090955734253,\n",
       "   0.1621111035346985,\n",
       "   0.16203603148460388,\n",
       "   0.08102118968963623,\n",
       "   0.13240322470664978,\n",
       "   0.11201731860637665,\n",
       "   0.04108358174562454,\n",
       "   0.028637832030653954,\n",
       "   0.02427239529788494,\n",
       "   0.023782765492796898,\n",
       "   0.015519478358328342,\n",
       "   0.016886360943317413,\n",
       "   0.017427917569875717,\n",
       "   0.013123289681971073,\n",
       "   0.01159391924738884,\n",
       "   0.010445760563015938,\n",
       "   0.0249573215842247,\n",
       "   0.013576886616647243],\n",
       "  [0.4447691738605499,\n",
       "   0.3344349265098572,\n",
       "   0.2624641954898834,\n",
       "   0.10285812616348267,\n",
       "   0.2573753595352173,\n",
       "   0.4565258324146271,\n",
       "   0.4287789762020111,\n",
       "   0.35319241881370544,\n",
       "   0.3502703905105591,\n",
       "   0.2738015353679657,\n",
       "   0.241705060005188,\n",
       "   0.2118813544511795,\n",
       "   0.18560072779655457,\n",
       "   0.15639227628707886,\n",
       "   0.17346274852752686,\n",
       "   0.1253754049539566,\n",
       "   0.11089534312486649,\n",
       "   0.09956544637680054,\n",
       "   0.08905976265668869,\n",
       "   0.07712602615356445],\n",
       "  [0.44290482997894287,\n",
       "   0.37602582573890686,\n",
       "   0.25002381205558777,\n",
       "   0.17915315926074982,\n",
       "   0.06742570549249649,\n",
       "   0.14553560316562653,\n",
       "   0.0690087378025055,\n",
       "   0.041193071752786636,\n",
       "   0.034028299152851105,\n",
       "   0.023594273254275322,\n",
       "   0.02681858092546463,\n",
       "   0.023629341274499893,\n",
       "   0.01774125173687935,\n",
       "   0.019037002697587013,\n",
       "   0.022644175216555595,\n",
       "   0.01620083674788475,\n",
       "   0.016774995252490044,\n",
       "   0.1336812525987625,\n",
       "   0.44013071060180664,\n",
       "   0.44092175364494324],\n",
       "  [0.4678910970687866,\n",
       "   0.37203389406204224,\n",
       "   0.25379055738449097,\n",
       "   0.13190627098083496,\n",
       "   0.08379782736301422,\n",
       "   0.17174586653709412,\n",
       "   0.14105738699436188,\n",
       "   0.13306720554828644,\n",
       "   0.06973797082901001,\n",
       "   0.051478102803230286,\n",
       "   0.04375509172677994,\n",
       "   0.032536160200834274,\n",
       "   0.03372340649366379,\n",
       "   0.03308434784412384,\n",
       "   0.02237393520772457,\n",
       "   0.034139230847358704,\n",
       "   0.01630926877260208,\n",
       "   0.017701281234622,\n",
       "   0.024742599576711655,\n",
       "   0.016723977401852608],\n",
       "  [0.3875312805175781,\n",
       "   0.30165261030197144,\n",
       "   0.18186843395233154,\n",
       "   0.12181974202394485,\n",
       "   0.0848408117890358,\n",
       "   0.06530564278364182,\n",
       "   0.03938662260770798,\n",
       "   0.03355478495359421,\n",
       "   0.06403771042823792,\n",
       "   0.028155528008937836,\n",
       "   0.022175459191203117,\n",
       "   0.0166939590126276,\n",
       "   0.018887000158429146,\n",
       "   0.01182155217975378,\n",
       "   0.009302246384322643,\n",
       "   0.02661852352321148,\n",
       "   0.006424157414585352,\n",
       "   0.013039520010352135,\n",
       "   0.012750296853482723,\n",
       "   0.0042359414510428905]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_acc_per_fold, acc_per_fold, val_loss_per_fold, loss_per_fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "results = {\"val_acc_per_fold\": val_acc_per_fold, \"acc_per_fold\": acc_per_fold, \"val_loss_per_fold\": val_loss_per_fold, \"loss_per_fold\": loss_per_fold}\n",
    "\n",
    "with open('results_model_rnn.pickle', 'wb') as file:\n",
    " pickle.dump(results, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9849526882171631\n",
      "0.9957007765769958\n",
      "0.8512467741966248\n",
      "0.9836629629135132\n",
      "0.9656062126159668\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9562338829040528"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 0\n",
    "for fold in val_acc_per_fold:\n",
    "    print(fold[-1])\n",
    "    s += fold[-1]\n",
    "s/5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Simplified RNN model - randomly created folds, regular lr**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "581/581 [==============================] - 26s 42ms/step - loss: 0.4345 - accuracy: 0.8234 - val_loss: 0.3714 - val_accuracy: 0.8654\n",
      "Epoch 2/20\n",
      "581/581 [==============================] - 24s 42ms/step - loss: 0.3121 - accuracy: 0.8814 - val_loss: 0.3268 - val_accuracy: 0.8629\n",
      "Epoch 3/20\n",
      "581/581 [==============================] - 25s 42ms/step - loss: 0.1752 - accuracy: 0.9385 - val_loss: 0.1379 - val_accuracy: 0.9467\n",
      "Epoch 4/20\n",
      "581/581 [==============================] - 24s 42ms/step - loss: 0.1173 - accuracy: 0.9591 - val_loss: 0.0788 - val_accuracy: 0.9699\n",
      "Epoch 5/20\n",
      "581/581 [==============================] - 24s 42ms/step - loss: 0.0784 - accuracy: 0.9714 - val_loss: 0.0370 - val_accuracy: 0.9880\n",
      "Epoch 6/20\n",
      "581/581 [==============================] - 24s 42ms/step - loss: 0.0594 - accuracy: 0.9811 - val_loss: 0.0347 - val_accuracy: 0.9888\n",
      "Epoch 7/20\n",
      "581/581 [==============================] - 24s 42ms/step - loss: 0.0511 - accuracy: 0.9836 - val_loss: 0.0505 - val_accuracy: 0.9832\n",
      "Epoch 8/20\n",
      "581/581 [==============================] - 25s 42ms/step - loss: 0.0410 - accuracy: 0.9870 - val_loss: 0.0474 - val_accuracy: 0.9901\n",
      "Epoch 9/20\n",
      "581/581 [==============================] - 24s 42ms/step - loss: 0.0283 - accuracy: 0.9910 - val_loss: 0.0222 - val_accuracy: 0.9940\n",
      "Epoch 10/20\n",
      "581/581 [==============================] - 25s 42ms/step - loss: 0.0364 - accuracy: 0.9863 - val_loss: 0.0265 - val_accuracy: 0.9914\n",
      "Epoch 11/20\n",
      "581/581 [==============================] - 24s 42ms/step - loss: 0.0394 - accuracy: 0.9880 - val_loss: 0.0566 - val_accuracy: 0.9815\n",
      "Epoch 12/20\n",
      "581/581 [==============================] - 24s 42ms/step - loss: 0.0159 - accuracy: 0.9949 - val_loss: 0.0272 - val_accuracy: 0.9936\n",
      "Epoch 13/20\n",
      "581/581 [==============================] - 25s 42ms/step - loss: 0.0185 - accuracy: 0.9944 - val_loss: 0.0277 - val_accuracy: 0.9905\n",
      "Epoch 14/20\n",
      "581/581 [==============================] - 25s 43ms/step - loss: 0.0114 - accuracy: 0.9955 - val_loss: 0.0267 - val_accuracy: 0.9940\n",
      "Epoch 15/20\n",
      "581/581 [==============================] - 25s 43ms/step - loss: 0.0190 - accuracy: 0.9945 - val_loss: 0.0686 - val_accuracy: 0.9819\n",
      "Epoch 16/20\n",
      "581/581 [==============================] - 25s 43ms/step - loss: 0.0111 - accuracy: 0.9963 - val_loss: 0.0305 - val_accuracy: 0.9948\n",
      "Epoch 17/20\n",
      "581/581 [==============================] - 25s 42ms/step - loss: 0.0142 - accuracy: 0.9960 - val_loss: 0.0414 - val_accuracy: 0.9910\n",
      "Epoch 18/20\n",
      "581/581 [==============================] - 25s 43ms/step - loss: 0.0110 - accuracy: 0.9966 - val_loss: 0.0298 - val_accuracy: 0.9957\n",
      "Epoch 19/20\n",
      "581/581 [==============================] - 25s 43ms/step - loss: 0.0139 - accuracy: 0.9964 - val_loss: 0.0338 - val_accuracy: 0.9901\n",
      "Epoch 20/20\n",
      "581/581 [==============================] - 25s 43ms/step - loss: 0.0122 - accuracy: 0.9967 - val_loss: 0.0245 - val_accuracy: 0.9961\n",
      "Epoch 1/20\n",
      "581/581 [==============================] - 26s 43ms/step - loss: 0.4529 - accuracy: 0.8162 - val_loss: 0.3396 - val_accuracy: 0.8818\n",
      "Epoch 2/20\n",
      "581/581 [==============================] - 25s 42ms/step - loss: 0.3370 - accuracy: 0.8692 - val_loss: 0.4415 - val_accuracy: 0.7945\n",
      "Epoch 3/20\n",
      "581/581 [==============================] - 25s 42ms/step - loss: 0.2239 - accuracy: 0.9146 - val_loss: 0.1261 - val_accuracy: 0.9497\n",
      "Epoch 4/20\n",
      "581/581 [==============================] - 25s 42ms/step - loss: 0.1625 - accuracy: 0.9408 - val_loss: 0.0754 - val_accuracy: 0.9708\n",
      "Epoch 5/20\n",
      "581/581 [==============================] - 25s 42ms/step - loss: 0.0915 - accuracy: 0.9674 - val_loss: 0.0659 - val_accuracy: 0.9768\n",
      "Epoch 6/20\n",
      "581/581 [==============================] - 25s 43ms/step - loss: 0.0746 - accuracy: 0.9748 - val_loss: 0.0705 - val_accuracy: 0.9759\n",
      "Epoch 7/20\n",
      "581/581 [==============================] - 25s 43ms/step - loss: 0.0560 - accuracy: 0.9818 - val_loss: 0.0714 - val_accuracy: 0.9768\n",
      "Epoch 8/20\n",
      "581/581 [==============================] - 25s 42ms/step - loss: 0.0775 - accuracy: 0.9753 - val_loss: 0.0406 - val_accuracy: 0.9841\n",
      "Epoch 9/20\n",
      "581/581 [==============================] - 25s 43ms/step - loss: 0.0428 - accuracy: 0.9852 - val_loss: 0.0242 - val_accuracy: 0.9918\n",
      "Epoch 10/20\n",
      "581/581 [==============================] - 25s 42ms/step - loss: 0.0335 - accuracy: 0.9900 - val_loss: 0.0156 - val_accuracy: 0.9953\n",
      "Epoch 11/20\n",
      "581/581 [==============================] - 25s 42ms/step - loss: 0.0219 - accuracy: 0.9941 - val_loss: 0.0405 - val_accuracy: 0.9862\n",
      "Epoch 12/20\n",
      "581/581 [==============================] - 25s 42ms/step - loss: 0.0222 - accuracy: 0.9939 - val_loss: 0.0213 - val_accuracy: 0.9936\n",
      "Epoch 13/20\n",
      "581/581 [==============================] - 25s 42ms/step - loss: 0.0195 - accuracy: 0.9936 - val_loss: 0.0142 - val_accuracy: 0.9966\n",
      "Epoch 14/20\n",
      "581/581 [==============================] - 25s 42ms/step - loss: 0.0271 - accuracy: 0.9917 - val_loss: 0.0313 - val_accuracy: 0.9923\n",
      "Epoch 15/20\n",
      "581/581 [==============================] - 24s 42ms/step - loss: 0.0130 - accuracy: 0.9961 - val_loss: 0.0123 - val_accuracy: 0.9983\n",
      "Epoch 16/20\n",
      "581/581 [==============================] - 24s 42ms/step - loss: 0.0175 - accuracy: 0.9954 - val_loss: 0.0351 - val_accuracy: 0.9880\n",
      "Epoch 17/20\n",
      "581/581 [==============================] - 24s 42ms/step - loss: 0.0126 - accuracy: 0.9971 - val_loss: 0.0153 - val_accuracy: 0.9961\n",
      "Epoch 18/20\n",
      "581/581 [==============================] - 25s 42ms/step - loss: 0.0187 - accuracy: 0.9945 - val_loss: 0.0179 - val_accuracy: 0.9940\n",
      "Epoch 19/20\n",
      "581/581 [==============================] - 25s 42ms/step - loss: 0.0126 - accuracy: 0.9963 - val_loss: 0.0184 - val_accuracy: 0.9970\n",
      "Epoch 20/20\n",
      "581/581 [==============================] - 25s 42ms/step - loss: 0.0047 - accuracy: 0.9985 - val_loss: 0.0096 - val_accuracy: 0.9979\n",
      "Epoch 1/20\n",
      "581/581 [==============================] - 26s 42ms/step - loss: 0.4349 - accuracy: 0.8266 - val_loss: 0.3309 - val_accuracy: 0.8719\n",
      "Epoch 2/20\n",
      "581/581 [==============================] - 24s 42ms/step - loss: 0.3381 - accuracy: 0.8693 - val_loss: 0.2367 - val_accuracy: 0.9071\n",
      "Epoch 3/20\n",
      "581/581 [==============================] - 25s 42ms/step - loss: 0.1958 - accuracy: 0.9294 - val_loss: 0.2933 - val_accuracy: 0.8895\n",
      "Epoch 4/20\n",
      "581/581 [==============================] - 24s 42ms/step - loss: 0.1597 - accuracy: 0.9405 - val_loss: 0.2128 - val_accuracy: 0.9230\n",
      "Epoch 5/20\n",
      "581/581 [==============================] - 25s 42ms/step - loss: 0.1034 - accuracy: 0.9651 - val_loss: 0.0639 - val_accuracy: 0.9738\n",
      "Epoch 6/20\n",
      "581/581 [==============================] - 25s 42ms/step - loss: 0.0645 - accuracy: 0.9778 - val_loss: 0.0753 - val_accuracy: 0.9738\n",
      "Epoch 7/20\n",
      "581/581 [==============================] - 25s 42ms/step - loss: 0.0480 - accuracy: 0.9846 - val_loss: 0.0392 - val_accuracy: 0.9841\n",
      "Epoch 8/20\n",
      "581/581 [==============================] - 25s 42ms/step - loss: 0.0507 - accuracy: 0.9836 - val_loss: 0.0277 - val_accuracy: 0.9914\n",
      "Epoch 9/20\n",
      "581/581 [==============================] - 25s 42ms/step - loss: 0.0327 - accuracy: 0.9894 - val_loss: 0.0411 - val_accuracy: 0.9845\n",
      "Epoch 10/20\n",
      "581/581 [==============================] - 25s 42ms/step - loss: 0.0373 - accuracy: 0.9884 - val_loss: 0.0442 - val_accuracy: 0.9828\n",
      "Epoch 11/20\n",
      "581/581 [==============================] - 24s 42ms/step - loss: 0.0324 - accuracy: 0.9886 - val_loss: 0.0112 - val_accuracy: 0.9974\n",
      "Epoch 12/20\n",
      "581/581 [==============================] - 25s 42ms/step - loss: 0.0239 - accuracy: 0.9930 - val_loss: 0.0167 - val_accuracy: 0.9944\n",
      "Epoch 13/20\n",
      "581/581 [==============================] - 25s 43ms/step - loss: 0.0130 - accuracy: 0.9966 - val_loss: 0.0543 - val_accuracy: 0.9884\n",
      "Epoch 14/20\n",
      "581/581 [==============================] - 25s 42ms/step - loss: 0.0195 - accuracy: 0.9942 - val_loss: 0.0075 - val_accuracy: 0.9979\n",
      "Epoch 15/20\n",
      "581/581 [==============================] - 25s 42ms/step - loss: 0.0144 - accuracy: 0.9953 - val_loss: 0.0048 - val_accuracy: 0.9987\n",
      "Epoch 16/20\n",
      "581/581 [==============================] - 25s 42ms/step - loss: 0.0178 - accuracy: 0.9946 - val_loss: 0.0270 - val_accuracy: 0.9888\n",
      "Epoch 17/20\n",
      "581/581 [==============================] - 25s 42ms/step - loss: 0.0129 - accuracy: 0.9964 - val_loss: 0.0092 - val_accuracy: 0.9953\n",
      "Epoch 18/20\n",
      "581/581 [==============================] - 25s 42ms/step - loss: 0.0110 - accuracy: 0.9967 - val_loss: 0.0035 - val_accuracy: 0.9991\n",
      "Epoch 19/20\n",
      "581/581 [==============================] - 25s 42ms/step - loss: 0.0134 - accuracy: 0.9960 - val_loss: 0.0048 - val_accuracy: 0.9987\n",
      "Epoch 20/20\n",
      "581/581 [==============================] - 25s 42ms/step - loss: 0.0129 - accuracy: 0.9969 - val_loss: 0.0019 - val_accuracy: 0.9991\n",
      "Epoch 1/20\n",
      "581/581 [==============================] - 26s 43ms/step - loss: 0.4359 - accuracy: 0.8236 - val_loss: 0.3275 - val_accuracy: 0.8796\n",
      "Epoch 2/20\n",
      "581/581 [==============================] - 25s 43ms/step - loss: 0.3291 - accuracy: 0.8762 - val_loss: 0.2877 - val_accuracy: 0.8848\n",
      "Epoch 3/20\n",
      "581/581 [==============================] - 25s 43ms/step - loss: 0.2162 - accuracy: 0.9168 - val_loss: 0.1420 - val_accuracy: 0.9433\n",
      "Epoch 4/20\n",
      "581/581 [==============================] - 25s 43ms/step - loss: 0.1365 - accuracy: 0.9504 - val_loss: 0.0987 - val_accuracy: 0.9613\n",
      "Epoch 5/20\n",
      "581/581 [==============================] - 25s 42ms/step - loss: 0.1016 - accuracy: 0.9646 - val_loss: 0.0635 - val_accuracy: 0.9832\n",
      "Epoch 6/20\n",
      "581/581 [==============================] - 25s 42ms/step - loss: 0.0770 - accuracy: 0.9738 - val_loss: 0.0401 - val_accuracy: 0.9888\n",
      "Epoch 7/20\n",
      "581/581 [==============================] - 25s 42ms/step - loss: 0.0488 - accuracy: 0.9850 - val_loss: 0.0420 - val_accuracy: 0.9905\n",
      "Epoch 8/20\n",
      "581/581 [==============================] - 25s 42ms/step - loss: 0.0642 - accuracy: 0.9812 - val_loss: 0.0372 - val_accuracy: 0.9914\n",
      "Epoch 9/20\n",
      "581/581 [==============================] - 25s 42ms/step - loss: 0.0430 - accuracy: 0.9869 - val_loss: 0.0487 - val_accuracy: 0.9862\n",
      "Epoch 10/20\n",
      "581/581 [==============================] - 25s 42ms/step - loss: 0.0492 - accuracy: 0.9862 - val_loss: 0.0910 - val_accuracy: 0.9738\n",
      "Epoch 11/20\n",
      "581/581 [==============================] - 25s 42ms/step - loss: 0.0466 - accuracy: 0.9850 - val_loss: 0.0262 - val_accuracy: 0.9918\n",
      "Epoch 12/20\n",
      "581/581 [==============================] - 25s 42ms/step - loss: 0.0271 - accuracy: 0.9912 - val_loss: 0.0324 - val_accuracy: 0.9893\n",
      "Epoch 13/20\n",
      "581/581 [==============================] - 25s 43ms/step - loss: 0.0204 - accuracy: 0.9941 - val_loss: 0.0182 - val_accuracy: 0.9957\n",
      "Epoch 14/20\n",
      "581/581 [==============================] - 25s 43ms/step - loss: 0.0226 - accuracy: 0.9935 - val_loss: 0.0176 - val_accuracy: 0.9948\n",
      "Epoch 15/20\n",
      "581/581 [==============================] - 25s 43ms/step - loss: 0.0156 - accuracy: 0.9958 - val_loss: 0.0636 - val_accuracy: 0.9854\n",
      "Epoch 16/20\n",
      "581/581 [==============================] - 25s 43ms/step - loss: 0.0198 - accuracy: 0.9945 - val_loss: 0.0494 - val_accuracy: 0.9871\n",
      "Epoch 17/20\n",
      "581/581 [==============================] - 25s 43ms/step - loss: 0.0278 - accuracy: 0.9924 - val_loss: 0.0235 - val_accuracy: 0.9948\n",
      "Epoch 18/20\n",
      "581/581 [==============================] - 25s 43ms/step - loss: 0.0090 - accuracy: 0.9970 - val_loss: 0.0229 - val_accuracy: 0.9953\n",
      "Epoch 19/20\n",
      "581/581 [==============================] - 25s 43ms/step - loss: 0.0153 - accuracy: 0.9950 - val_loss: 0.0148 - val_accuracy: 0.9948\n",
      "Epoch 20/20\n",
      "581/581 [==============================] - 25s 42ms/step - loss: 0.0095 - accuracy: 0.9973 - val_loss: 0.0203 - val_accuracy: 0.9953\n",
      "Epoch 1/20\n",
      "581/581 [==============================] - 26s 43ms/step - loss: 0.4405 - accuracy: 0.8201 - val_loss: 0.3813 - val_accuracy: 0.8564\n",
      "Epoch 2/20\n",
      "581/581 [==============================] - 25s 42ms/step - loss: 0.3528 - accuracy: 0.8642 - val_loss: 0.3513 - val_accuracy: 0.8594\n",
      "Epoch 3/20\n",
      "581/581 [==============================] - 25s 42ms/step - loss: 0.2220 - accuracy: 0.9124 - val_loss: 0.1577 - val_accuracy: 0.9437\n",
      "Epoch 4/20\n",
      "581/581 [==============================] - 25s 42ms/step - loss: 0.1484 - accuracy: 0.9467 - val_loss: 0.0992 - val_accuracy: 0.9678\n",
      "Epoch 5/20\n",
      "581/581 [==============================] - 25s 42ms/step - loss: 0.0927 - accuracy: 0.9675 - val_loss: 0.0518 - val_accuracy: 0.9802\n",
      "Epoch 6/20\n",
      "581/581 [==============================] - 25s 42ms/step - loss: 0.1047 - accuracy: 0.9643 - val_loss: 0.4078 - val_accuracy: 0.8998\n",
      "Epoch 7/20\n",
      "581/581 [==============================] - 25s 42ms/step - loss: 0.0752 - accuracy: 0.9739 - val_loss: 0.0375 - val_accuracy: 0.9862\n",
      "Epoch 8/20\n",
      "581/581 [==============================] - 25s 42ms/step - loss: 0.0488 - accuracy: 0.9831 - val_loss: 0.0953 - val_accuracy: 0.9686\n",
      "Epoch 9/20\n",
      "581/581 [==============================] - 25s 42ms/step - loss: 0.0431 - accuracy: 0.9878 - val_loss: 0.0503 - val_accuracy: 0.9789\n",
      "Epoch 10/20\n",
      "581/581 [==============================] - 25s 42ms/step - loss: 0.0340 - accuracy: 0.9894 - val_loss: 0.0651 - val_accuracy: 0.9802\n",
      "Epoch 11/20\n",
      "581/581 [==============================] - 25s 42ms/step - loss: 0.0246 - accuracy: 0.9918 - val_loss: 0.0266 - val_accuracy: 0.9893\n",
      "Epoch 12/20\n",
      "581/581 [==============================] - 25s 42ms/step - loss: 0.0221 - accuracy: 0.9931 - val_loss: 0.0215 - val_accuracy: 0.9953\n",
      "Epoch 13/20\n",
      "581/581 [==============================] - 25s 42ms/step - loss: 0.0298 - accuracy: 0.9918 - val_loss: 0.1608 - val_accuracy: 0.9647\n",
      "Epoch 14/20\n",
      "581/581 [==============================] - 25s 42ms/step - loss: 0.0232 - accuracy: 0.9946 - val_loss: 0.0063 - val_accuracy: 0.9987\n",
      "Epoch 15/20\n",
      "581/581 [==============================] - 25s 42ms/step - loss: 0.0179 - accuracy: 0.9943 - val_loss: 0.0167 - val_accuracy: 0.9948\n",
      "Epoch 16/20\n",
      "581/581 [==============================] - 25s 43ms/step - loss: 0.0333 - accuracy: 0.9894 - val_loss: 0.0234 - val_accuracy: 0.9936\n",
      "Epoch 17/20\n",
      "581/581 [==============================] - 25s 43ms/step - loss: 0.0156 - accuracy: 0.9959 - val_loss: 0.0128 - val_accuracy: 0.9966\n",
      "Epoch 18/20\n",
      "581/581 [==============================] - 25s 43ms/step - loss: 0.0196 - accuracy: 0.9940 - val_loss: 0.0294 - val_accuracy: 0.9905\n",
      "Epoch 19/20\n",
      "581/581 [==============================] - 25s 43ms/step - loss: 0.0129 - accuracy: 0.9971 - val_loss: 0.6279 - val_accuracy: 0.9003\n",
      "Epoch 20/20\n",
      "581/581 [==============================] - 25s 42ms/step - loss: 0.0209 - accuracy: 0.9939 - val_loss: 0.0110 - val_accuracy: 0.9961\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "batch_size = 16\n",
    "\n",
    "val_acc_per_fold = []\n",
    "acc_per_fold = []\n",
    "val_loss_per_fold = []\n",
    "loss_per_fold = []\n",
    "sets = [(fold_1, fold_1_lab), (fold_2, fold_2_lab), (fold_3, fold_3_lab), (fold_4, fold_4_lab), (fold_5, fold_5_lab)]\n",
    "\n",
    "for n in range(len(sets)):\n",
    "    test_x = sets[n][0]\n",
    "    test_y = sets[n][1]\n",
    "    train_x = pd.DataFrame()\n",
    "    train_y = pd.DataFrame()\n",
    "    for s in sets[:n] + sets[n+1:]:\n",
    "        train_x = pd.concat([train_x, s[0]])\n",
    "        train_y = pd.concat([train_y, s[1]])\n",
    "\n",
    "    classifier_RNN = build_model_RNN()\n",
    "\n",
    "    history_RNN = classifier_RNN.fit(\n",
    "        np.array(train_x), np.array(train_y),\n",
    "        epochs=epochs,\n",
    "        batch_size = batch_size,\n",
    "        steps_per_epoch=train_x.shape[0] // batch_size,\n",
    "        validation_data=(np.array(test_x), np.array(test_y))\n",
    "    )\n",
    "\n",
    "    val_acc_per_fold.append(history_RNN.history['val_accuracy'])\n",
    "    acc_per_fold.append(history_RNN.history['accuracy'])\n",
    "    val_loss_per_fold.append(history_RNN.history['val_loss'])\n",
    "    loss_per_fold.append(history_RNN.history['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[0.8654342293739319,\n",
       "   0.8628546595573425,\n",
       "   0.9466896057128906,\n",
       "   0.969905436038971,\n",
       "   0.9879621863365173,\n",
       "   0.9888219833374023,\n",
       "   0.9832330346107483,\n",
       "   0.990111768245697,\n",
       "   0.9939810633659363,\n",
       "   0.9914015531539917,\n",
       "   0.9815133213996887,\n",
       "   0.9935511350631714,\n",
       "   0.9905416965484619,\n",
       "   0.9939810633659363,\n",
       "   0.9819432497024536,\n",
       "   0.9948409199714661,\n",
       "   0.9909716248512268,\n",
       "   0.9957007765769958,\n",
       "   0.990111768245697,\n",
       "   0.9961307048797607],\n",
       "  [0.8817712664604187,\n",
       "   0.794497013092041,\n",
       "   0.9496990442276001,\n",
       "   0.970765233039856,\n",
       "   0.9767841696739197,\n",
       "   0.9759243130683899,\n",
       "   0.9767841696739197,\n",
       "   0.9840928912162781,\n",
       "   0.9918314814567566,\n",
       "   0.995270848274231,\n",
       "   0.9862424731254578,\n",
       "   0.9935511350631714,\n",
       "   0.9965606331825256,\n",
       "   0.9922614097595215,\n",
       "   0.9982802867889404,\n",
       "   0.9879621863365173,\n",
       "   0.9961307048797607,\n",
       "   0.9939810633659363,\n",
       "   0.9969905614852905,\n",
       "   0.9978503584861755],\n",
       "  [0.8718830347061157,\n",
       "   0.9071367383003235,\n",
       "   0.889509916305542,\n",
       "   0.9230438470840454,\n",
       "   0.9737747311592102,\n",
       "   0.9737747311592102,\n",
       "   0.9840928912162781,\n",
       "   0.9914015531539917,\n",
       "   0.9845227599143982,\n",
       "   0.9828031063079834,\n",
       "   0.9974204897880554,\n",
       "   0.9944109916687012,\n",
       "   0.9883921146392822,\n",
       "   0.9978503584861755,\n",
       "   0.9987102150917053,\n",
       "   0.9888219833374023,\n",
       "   0.995270848274231,\n",
       "   0.9991401433944702,\n",
       "   0.9987102150917053,\n",
       "   0.9991401433944702],\n",
       "  [0.879621684551239,\n",
       "   0.884780764579773,\n",
       "   0.9432502388954163,\n",
       "   0.9613069891929626,\n",
       "   0.9832330346107483,\n",
       "   0.9888219833374023,\n",
       "   0.9905416965484619,\n",
       "   0.9914015531539917,\n",
       "   0.9862424731254578,\n",
       "   0.9737747311592102,\n",
       "   0.9918314814567566,\n",
       "   0.9892519116401672,\n",
       "   0.9957007765769958,\n",
       "   0.9948409199714661,\n",
       "   0.985382616519928,\n",
       "   0.9871023297309875,\n",
       "   0.9948409199714661,\n",
       "   0.995270848274231,\n",
       "   0.9948409199714661,\n",
       "   0.995270848274231],\n",
       "  [0.8564058542251587,\n",
       "   0.8594152927398682,\n",
       "   0.9436801671981812,\n",
       "   0.9677557945251465,\n",
       "   0.980223536491394,\n",
       "   0.8998280167579651,\n",
       "   0.9862424731254578,\n",
       "   0.9686156511306763,\n",
       "   0.9789338111877441,\n",
       "   0.980223536491394,\n",
       "   0.9892519116401672,\n",
       "   0.995270848274231,\n",
       "   0.964746356010437,\n",
       "   0.9987102150917053,\n",
       "   0.9948409199714661,\n",
       "   0.9935511350631714,\n",
       "   0.9965606331825256,\n",
       "   0.9905416965484619,\n",
       "   0.90025794506073,\n",
       "   0.9961307048797607]],\n",
       " [[0.8233649134635925,\n",
       "   0.8813523054122925,\n",
       "   0.9385228157043457,\n",
       "   0.9590870141983032,\n",
       "   0.9713609218597412,\n",
       "   0.9810507893562317,\n",
       "   0.9836347699165344,\n",
       "   0.986972451210022,\n",
       "   0.9909560680389404,\n",
       "   0.9863264560699463,\n",
       "   0.9880490899085999,\n",
       "   0.9949396848678589,\n",
       "   0.9944013953208923,\n",
       "   0.9954780340194702,\n",
       "   0.9945090413093567,\n",
       "   0.9963393807411194,\n",
       "   0.9960163831710815,\n",
       "   0.9965546727180481,\n",
       "   0.9964470267295837,\n",
       "   0.9966623783111572],\n",
       "  [0.8161574602127075,\n",
       "   0.8691860437393188,\n",
       "   0.9146209955215454,\n",
       "   0.9407837986946106,\n",
       "   0.967377245426178,\n",
       "   0.9748061895370483,\n",
       "   0.9818044900894165,\n",
       "   0.9753445386886597,\n",
       "   0.9852497577667236,\n",
       "   0.9899870753288269,\n",
       "   0.9940783977508545,\n",
       "   0.993863046169281,\n",
       "   0.9936476945877075,\n",
       "   0.9917097091674805,\n",
       "   0.9961240291595459,\n",
       "   0.9953703880310059,\n",
       "   0.9970930218696594,\n",
       "   0.9945090413093567,\n",
       "   0.9963393807411194,\n",
       "   0.9984926581382751],\n",
       "  [0.8265920877456665,\n",
       "   0.8692936897277832,\n",
       "   0.9293712377548218,\n",
       "   0.9404608011245728,\n",
       "   0.9651162624359131,\n",
       "   0.977820873260498,\n",
       "   0.984603762626648,\n",
       "   0.9836347699165344,\n",
       "   0.9894487261772156,\n",
       "   0.9883720874786377,\n",
       "   0.9885874390602112,\n",
       "   0.9930016994476318,\n",
       "   0.9965546727180481,\n",
       "   0.9941860437393188,\n",
       "   0.9952626824378967,\n",
       "   0.994616687297821,\n",
       "   0.9964470267295837,\n",
       "   0.9966623783111572,\n",
       "   0.9960163831710815,\n",
       "   0.9968776702880859],\n",
       "  [0.8235800266265869,\n",
       "   0.876184344291687,\n",
       "   0.916774332523346,\n",
       "   0.9503660798072815,\n",
       "   0.9645779728889465,\n",
       "   0.9738371968269348,\n",
       "   0.9850344657897949,\n",
       "   0.9811584949493408,\n",
       "   0.9868647456169128,\n",
       "   0.9862187504768372,\n",
       "   0.9850344657897949,\n",
       "   0.9911714196205139,\n",
       "   0.9940783977508545,\n",
       "   0.9935400485992432,\n",
       "   0.9958010315895081,\n",
       "   0.9945090413093567,\n",
       "   0.9923557043075562,\n",
       "   0.9969853758811951,\n",
       "   0.995047390460968,\n",
       "   0.9973083734512329],\n",
       "  [0.8201376795768738,\n",
       "   0.8642334342002869,\n",
       "   0.9123600125312805,\n",
       "   0.9467054009437561,\n",
       "   0.9674849510192871,\n",
       "   0.9642549753189087,\n",
       "   0.973944902420044,\n",
       "   0.9830964803695679,\n",
       "   0.9878337383270264,\n",
       "   0.9894487261772156,\n",
       "   0.9918174147605896,\n",
       "   0.993109405040741,\n",
       "   0.9918174147605896,\n",
       "   0.994616687297821,\n",
       "   0.9942936897277832,\n",
       "   0.9894487261772156,\n",
       "   0.9959086775779724,\n",
       "   0.9939706921577454,\n",
       "   0.9970930218696594,\n",
       "   0.993863046169281]],\n",
       " [[0.37138763070106506,\n",
       "   0.32682499289512634,\n",
       "   0.13785390555858612,\n",
       "   0.07882673293352127,\n",
       "   0.0370100736618042,\n",
       "   0.03471654653549194,\n",
       "   0.05054930970072746,\n",
       "   0.04740077257156372,\n",
       "   0.022161442786455154,\n",
       "   0.02646270953118801,\n",
       "   0.05656345188617706,\n",
       "   0.027241945266723633,\n",
       "   0.027696190401911736,\n",
       "   0.026749614626169205,\n",
       "   0.06864731013774872,\n",
       "   0.030537357553839684,\n",
       "   0.04138895869255066,\n",
       "   0.02982475981116295,\n",
       "   0.033812228590250015,\n",
       "   0.024507945403456688],\n",
       "  [0.3395538330078125,\n",
       "   0.4414988160133362,\n",
       "   0.12610189616680145,\n",
       "   0.07537657767534256,\n",
       "   0.06587810069322586,\n",
       "   0.07045186311006546,\n",
       "   0.07139250636100769,\n",
       "   0.04055479168891907,\n",
       "   0.024167317897081375,\n",
       "   0.01564234495162964,\n",
       "   0.04051106050610542,\n",
       "   0.021318992599844933,\n",
       "   0.014234265312552452,\n",
       "   0.03133530542254448,\n",
       "   0.012289674952626228,\n",
       "   0.035068780183792114,\n",
       "   0.015261459164321423,\n",
       "   0.01787179708480835,\n",
       "   0.01844603940844536,\n",
       "   0.009576711803674698],\n",
       "  [0.3308830261230469,\n",
       "   0.23667654395103455,\n",
       "   0.2933046221733093,\n",
       "   0.21277180314064026,\n",
       "   0.06388088315725327,\n",
       "   0.07530029863119125,\n",
       "   0.039183568209409714,\n",
       "   0.027726365253329277,\n",
       "   0.041115712374448776,\n",
       "   0.04424336925148964,\n",
       "   0.011152314953505993,\n",
       "   0.01669367589056492,\n",
       "   0.05431441590189934,\n",
       "   0.007535153068602085,\n",
       "   0.0047677443362772465,\n",
       "   0.027015667408704758,\n",
       "   0.009177724830806255,\n",
       "   0.0035346699878573418,\n",
       "   0.004786586854606867,\n",
       "   0.0019081810023635626],\n",
       "  [0.32748863101005554,\n",
       "   0.2876516282558441,\n",
       "   0.14200255274772644,\n",
       "   0.0986732468008995,\n",
       "   0.06354089826345444,\n",
       "   0.04009344428777695,\n",
       "   0.04200486093759537,\n",
       "   0.03719019144773483,\n",
       "   0.04871131852269173,\n",
       "   0.09102922677993774,\n",
       "   0.02623608149588108,\n",
       "   0.032381895929574966,\n",
       "   0.01815836876630783,\n",
       "   0.017593806609511375,\n",
       "   0.06356213241815567,\n",
       "   0.04940897598862648,\n",
       "   0.023467782884836197,\n",
       "   0.022857366129755974,\n",
       "   0.014761354774236679,\n",
       "   0.02032923884689808],\n",
       "  [0.3813396990299225,\n",
       "   0.351310133934021,\n",
       "   0.15773995220661163,\n",
       "   0.09921539574861526,\n",
       "   0.05180661752820015,\n",
       "   0.4077753722667694,\n",
       "   0.0375058688223362,\n",
       "   0.09534840285778046,\n",
       "   0.05030658468604088,\n",
       "   0.06513994187116623,\n",
       "   0.026626374572515488,\n",
       "   0.02151307463645935,\n",
       "   0.16081373393535614,\n",
       "   0.0063208602368831635,\n",
       "   0.01673530414700508,\n",
       "   0.023371584713459015,\n",
       "   0.012822994962334633,\n",
       "   0.029397858306765556,\n",
       "   0.6278805136680603,\n",
       "   0.011034281924366951]],\n",
       " [[0.43452244997024536,\n",
       "   0.3120560348033905,\n",
       "   0.17521429061889648,\n",
       "   0.11728455871343613,\n",
       "   0.07837648689746857,\n",
       "   0.05940096825361252,\n",
       "   0.051072511821985245,\n",
       "   0.04096775874495506,\n",
       "   0.02831234224140644,\n",
       "   0.03636668995022774,\n",
       "   0.03935474157333374,\n",
       "   0.015877170488238335,\n",
       "   0.018549028784036636,\n",
       "   0.011354811489582062,\n",
       "   0.01904192753136158,\n",
       "   0.01112834457308054,\n",
       "   0.014203743077814579,\n",
       "   0.010995813645422459,\n",
       "   0.013887416571378708,\n",
       "   0.012211746536195278],\n",
       "  [0.45293906331062317,\n",
       "   0.3369837999343872,\n",
       "   0.22385844588279724,\n",
       "   0.16252170503139496,\n",
       "   0.09154189378023148,\n",
       "   0.07458098977804184,\n",
       "   0.05604814738035202,\n",
       "   0.07749653607606888,\n",
       "   0.04277916997671127,\n",
       "   0.03349930793046951,\n",
       "   0.02187969535589218,\n",
       "   0.022205539047718048,\n",
       "   0.01950281299650669,\n",
       "   0.02707490511238575,\n",
       "   0.013010363094508648,\n",
       "   0.017464803531765938,\n",
       "   0.012594202533364296,\n",
       "   0.018655020743608475,\n",
       "   0.012649394571781158,\n",
       "   0.004718966782093048],\n",
       "  [0.4349265396595001,\n",
       "   0.3381335437297821,\n",
       "   0.19578570127487183,\n",
       "   0.1597178727388382,\n",
       "   0.10339611023664474,\n",
       "   0.0644664466381073,\n",
       "   0.048006877303123474,\n",
       "   0.0506533682346344,\n",
       "   0.032684385776519775,\n",
       "   0.03732668608427048,\n",
       "   0.03239409253001213,\n",
       "   0.023944206535816193,\n",
       "   0.01296460721641779,\n",
       "   0.019462987780570984,\n",
       "   0.014398773200809956,\n",
       "   0.017811084166169167,\n",
       "   0.012904795818030834,\n",
       "   0.010982269421219826,\n",
       "   0.013364582322537899,\n",
       "   0.012857385911047459],\n",
       "  [0.43592822551727295,\n",
       "   0.3290950655937195,\n",
       "   0.21621210873126984,\n",
       "   0.13646413385868073,\n",
       "   0.10158207267522812,\n",
       "   0.0770062506198883,\n",
       "   0.04880017787218094,\n",
       "   0.06421677023172379,\n",
       "   0.043047819286584854,\n",
       "   0.049237366765737534,\n",
       "   0.04662833735346794,\n",
       "   0.0271354541182518,\n",
       "   0.020364901050925255,\n",
       "   0.022625811398029327,\n",
       "   0.015614804811775684,\n",
       "   0.019802050665020943,\n",
       "   0.02783322148025036,\n",
       "   0.00897300522774458,\n",
       "   0.015294261276721954,\n",
       "   0.009486625902354717],\n",
       "  [0.44051438570022583,\n",
       "   0.35280269384384155,\n",
       "   0.22203069925308228,\n",
       "   0.14836879074573517,\n",
       "   0.09274736791849136,\n",
       "   0.10471586138010025,\n",
       "   0.07523950189352036,\n",
       "   0.04878951609134674,\n",
       "   0.04308580979704857,\n",
       "   0.034011952579021454,\n",
       "   0.024625837802886963,\n",
       "   0.022081656381487846,\n",
       "   0.029813287779688835,\n",
       "   0.023155419155955315,\n",
       "   0.01789720170199871,\n",
       "   0.03331834822893143,\n",
       "   0.015611317940056324,\n",
       "   0.01955442875623703,\n",
       "   0.012913846410810947,\n",
       "   0.02089095674455166]])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_acc_per_fold, acc_per_fold, val_loss_per_fold, loss_per_fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "results = {\"val_acc_per_fold\": val_acc_per_fold, \"acc_per_fold\": acc_per_fold, \"val_loss_per_fold\": val_loss_per_fold, \"loss_per_fold\": loss_per_fold}\n",
    "\n",
    "with open('results_model_rnn_random_folds.pickle', 'wb') as file:\n",
    " pickle.dump(results, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9961307048797607\n",
      "0.9978503584861755\n",
      "0.9991401433944702\n",
      "0.995270848274231\n",
      "0.9961307048797607\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9969045519828796"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 0\n",
    "for fold in val_acc_per_fold:\n",
    "    print(fold[-1])\n",
    "    s += fold[-1]\n",
    "s/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as gru_cell_9_layer_call_fn, gru_cell_9_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: classifier_RNN_random_folds\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: classifier_RNN_random_folds\\assets\n"
     ]
    }
   ],
   "source": [
    "# Calling `save('my_model')` creates a SavedModel folder `my_model`.\n",
    "classifier_RNN.save(\"classifier_RNN_random_folds\")\n",
    "\n",
    "# It can be used to reconstruct the model identically.\n",
    "#reconstructed_model = keras.models.load_model(\"classifier_RNN_random_folds\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Second approach - Leave One Out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN-infused model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_model():\n",
    "#     hidden_dim = 512\n",
    "#     seq_len = train_x.shape[1]\n",
    "#     n_features = 1\n",
    "#     learning_rate = 0.0003\n",
    "\n",
    "#     regularizers = {\n",
    "#         'kernel_regularizer':tf.keras.regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
    "#         'bias_regularizer':tf.keras.regularizers.l2(1e-4),\n",
    "#         'activity_regularizer':tf.keras.regularizers.l2(1e-5)\n",
    "#     }\n",
    "\n",
    "#     in1 = tf.keras.layers.Input(shape=(seq_len, n_features))\n",
    "#     gru1 = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(hidden_dim, return_sequences=True, **regularizers))(in1)\n",
    "#     gru2 = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(hidden_dim, name='latent_layer', **regularizers))(gru1)\n",
    "#     rsh1 = tf.keras.layers.Reshape( (hidden_dim*2, 1) )(gru2)\n",
    "#     cov1 = tf.keras.layers.Dropout(0.1)(tf.keras.layers.Conv1D(seq_len, 3, activation='relu', **regularizers)(rsh1))\n",
    "#     max1 = tf.keras.layers.MaxPool1D(pool_size=3, strides=1)(cov1)\n",
    "#     cov2 = tf.keras.layers.Dropout(0.1)(tf.keras.layers.Conv1D(seq_len, 5, activation='relu', **regularizers)(max1))\n",
    "#     max2 = tf.keras.layers.MaxPool1D(pool_size=3, strides=1)(cov2)\n",
    "#     cov3 = tf.keras.layers.Dropout(0.1)(tf.keras.layers.Conv1D(seq_len, 3, activation='relu', **regularizers)(max2))\n",
    "#     max3 = tf.keras.layers.MaxPool1D(pool_size=3, strides=1)(cov3)\n",
    "#     rsh2 = tf.keras.layers.Reshape( (seq_len, 1010) )(max3)\n",
    "#     tdd1 = tf.keras.layers.Dropout(0.1)(tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(1,**regularizers))(rsh2))\n",
    "#     f1   = tf.keras.layers.Flatten()(tdd1)\n",
    "#     d1   = tf.keras.layers.Dense(1, activation='sigmoid', **regularizers)(f1)\n",
    "\n",
    "#     classifier = tf.keras.Model(\n",
    "#         inputs=[in1], \n",
    "#         outputs=[d1]\n",
    "#     )\n",
    "\n",
    "#     opt = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "#     classifier.compile(loss='binary_crossentropy', optimizer=opt, \n",
    "#         metrics=['accuracy'])\n",
    "    \n",
    "#     return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs = 20\n",
    "# batch_size = 16\n",
    "\n",
    "# val_acc_per_fold = []\n",
    "# acc_per_fold = []\n",
    "# val_loss_per_fold = []\n",
    "# loss_per_fold = []\n",
    "\n",
    "# for n in range(1, 31):\n",
    "#     test_x = pd.DataFrame()\n",
    "#     test_y = pd.DataFrame()\n",
    "\n",
    "#     idxs_test = df_fall[df_fall['subject'] == n].index.to_list()\n",
    "#     idxs_test_2 = df_adl[df_adl['subject'] == n].index.to_list()\n",
    "#     test_x = pd.concat([test_x, fall_data.loc[idxs_test], adl_data.loc[idxs_test_2]])\n",
    "#     test_y = pd.concat([test_y, df_fall.loc[idxs_test]['label'], df_adl.loc[idxs_test_2]['label']])\n",
    "\n",
    "#     train_x = pd.DataFrame()\n",
    "#     train_y = pd.DataFrame()\n",
    "\n",
    "#     fall_data_c = fall_data.copy()\n",
    "#     adl_data_c = adl_data.copy()\n",
    "#     fall_data_c.drop(idxs_test, axis=0, inplace=True)\n",
    "#     adl_data_c.drop(idxs_test_2, axis=0, inplace=True)\n",
    "#     train_x = pd.concat([train_x, fall_data_c, adl_data_c])\n",
    "\n",
    "#     df_fall_c = df_fall.copy()\n",
    "#     df_adl_c = df_adl.copy()\n",
    "#     df_fall_c.drop(idxs_test, axis=0, inplace=True)\n",
    "#     df_adl_c.drop(idxs_test_2, axis=0, inplace=True)\n",
    "#     train_y = pd.concat([train_y, df_fall_c['label'], df_adl_c['label']])\n",
    "\n",
    "#     classifier = build_model()\n",
    "    \n",
    "#     history = classifier.fit(\n",
    "#         np.array(train_x), np.array(train_y),\n",
    "#         epochs=epochs,\n",
    "#         batch_size = batch_size,\n",
    "#         steps_per_epoch=(train_x[0]) // batch_size,\n",
    "#         validation_data=(np.array(test_x), np.array(test_y)),\n",
    "#         callbacks=[tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=10, min_lr=0.00003)]\n",
    "#     )\n",
    "\n",
    "#     val_acc_per_fold.append(history.history['val_accuracy'])\n",
    "#     acc_per_fold.append(history.history['accuracy'])\n",
    "#     val_loss_per_fold.append(history.history['val_loss'])\n",
    "#     loss_per_fold.append(history.history['loss'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bidirectional RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_bidirectional():\n",
    "    hidden_dim = 512\n",
    "    seq_len = train_x.shape[1]\n",
    "    n_features = 1\n",
    "    learning_rate = 0.0003\n",
    "\n",
    "    regularizers = {\n",
    "        'kernel_regularizer':tf.keras.regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
    "        'bias_regularizer':tf.keras.regularizers.l2(1e-4),\n",
    "        'activity_regularizer':tf.keras.regularizers.l2(1e-5)\n",
    "    }\n",
    "\n",
    "    regularizers_GRU = {\n",
    "        'kernel_regularizer':tf.keras.regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
    "        'bias_regularizer':tf.keras.regularizers.l2(1e-4),\n",
    "        'activity_regularizer':tf.keras.regularizers.l2(1e-5),\n",
    "        'recurrent_regularizer':tf.keras.regularizers.l2(1e-5)\n",
    "    }\n",
    "\n",
    "    in1 = tf.keras.layers.Input(shape=(seq_len, n_features))\n",
    "    gru1 = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(hidden_dim, name='latent_layer', **regularizers_GRU))(in1)\n",
    "    bn = tf.keras.layers.BatchNormalization()(gru1)\n",
    "    dense = tf.keras.layers.Dense(128, activation=\"relu\", **regularizers)(bn)\n",
    "    drop = tf.keras.layers.Dropout(0.3)(dense)\n",
    "    d1   = tf.keras.layers.Dense(1, activation='sigmoid', **regularizers)(drop)\n",
    "\n",
    "    classifier_test_opt_bi = tf.keras.Model(\n",
    "        inputs=[in1], \n",
    "        outputs=[d1]\n",
    "    )\n",
    "\n",
    "    #print(classifier_test_opt_bi.summary())\n",
    "\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    classifier_test_opt_bi.compile(loss='binary_crossentropy', optimizer=opt, \n",
    "        metrics=['accuracy'])\n",
    "    \n",
    "    return classifier_test_opt_bi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(384, 453) (384, 1)\n",
      "Epoch 1/20\n",
      "711/711 [==============================] - 59s 76ms/step - loss: 0.5197 - accuracy: 0.8223 - val_loss: 0.4512 - val_accuracy: 0.8438 - lr: 3.0000e-04\n",
      "Epoch 2/20\n",
      "711/711 [==============================] - 54s 76ms/step - loss: 0.3699 - accuracy: 0.8858 - val_loss: 0.2559 - val_accuracy: 0.9349 - lr: 3.0000e-04\n",
      "Epoch 3/20\n",
      "711/711 [==============================] - 54s 76ms/step - loss: 0.2569 - accuracy: 0.9363 - val_loss: 0.2123 - val_accuracy: 0.9635 - lr: 3.0000e-04\n",
      "Epoch 4/20\n",
      "711/711 [==============================] - 54s 76ms/step - loss: 0.2384 - accuracy: 0.9439 - val_loss: 0.1278 - val_accuracy: 0.9896 - lr: 3.0000e-04\n",
      "Epoch 5/20\n",
      "711/711 [==============================] - 54s 76ms/step - loss: 0.1414 - accuracy: 0.9811 - val_loss: 0.1288 - val_accuracy: 0.9844 - lr: 3.0000e-04\n",
      "Epoch 6/20\n",
      "711/711 [==============================] - 54s 76ms/step - loss: 0.1263 - accuracy: 0.9850 - val_loss: 0.4077 - val_accuracy: 0.9089 - lr: 3.0000e-04\n",
      "Epoch 7/20\n",
      "711/711 [==============================] - 60s 84ms/step - loss: 0.1187 - accuracy: 0.9845 - val_loss: 0.0830 - val_accuracy: 0.9974 - lr: 3.0000e-04\n",
      "Epoch 8/20\n",
      "711/711 [==============================] - 57s 80ms/step - loss: 0.1088 - accuracy: 0.9877 - val_loss: 0.1535 - val_accuracy: 0.9531 - lr: 3.0000e-04\n",
      "Epoch 9/20\n",
      "711/711 [==============================] - 62s 87ms/step - loss: 0.0963 - accuracy: 0.9903 - val_loss: 0.1081 - val_accuracy: 0.9818 - lr: 3.0000e-04\n",
      "Epoch 10/20\n",
      "711/711 [==============================] - 103s 145ms/step - loss: 0.0870 - accuracy: 0.9925 - val_loss: 0.1005 - val_accuracy: 0.9792 - lr: 3.0000e-04\n",
      "Epoch 11/20\n",
      "711/711 [==============================] - 78s 110ms/step - loss: 0.0855 - accuracy: 0.9919 - val_loss: 0.1752 - val_accuracy: 0.9505 - lr: 3.0000e-04\n",
      "Epoch 12/20\n",
      "711/711 [==============================] - 72s 101ms/step - loss: 0.0795 - accuracy: 0.9939 - val_loss: 0.0672 - val_accuracy: 0.9974 - lr: 3.0000e-04\n",
      "Epoch 13/20\n",
      "711/711 [==============================] - 102s 143ms/step - loss: 0.0694 - accuracy: 0.9962 - val_loss: 0.0690 - val_accuracy: 0.9948 - lr: 3.0000e-04\n",
      "Epoch 14/20\n",
      "711/711 [==============================] - 115s 162ms/step - loss: 0.0745 - accuracy: 0.9935 - val_loss: 0.0509 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 15/20\n",
      "711/711 [==============================] - 111s 156ms/step - loss: 0.0643 - accuracy: 0.9950 - val_loss: 0.0646 - val_accuracy: 0.9948 - lr: 3.0000e-04\n",
      "Epoch 16/20\n",
      "711/711 [==============================] - 105s 147ms/step - loss: 0.0538 - accuracy: 0.9975 - val_loss: 0.0482 - val_accuracy: 0.9948 - lr: 3.0000e-04\n",
      "Epoch 17/20\n",
      "711/711 [==============================] - 107s 150ms/step - loss: 0.0658 - accuracy: 0.9928 - val_loss: 0.0432 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 18/20\n",
      "711/711 [==============================] - 119s 168ms/step - loss: 0.0503 - accuracy: 0.9966 - val_loss: 0.1674 - val_accuracy: 0.9583 - lr: 3.0000e-04\n",
      "Epoch 19/20\n",
      "711/711 [==============================] - 122s 172ms/step - loss: 0.0495 - accuracy: 0.9962 - val_loss: 0.0384 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 20/20\n",
      "711/711 [==============================] - 126s 177ms/step - loss: 0.0467 - accuracy: 0.9968 - val_loss: 0.0380 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "(583, 453) (583, 1)\n",
      "Epoch 1/20\n",
      "699/699 [==============================] - 114s 154ms/step - loss: 0.5278 - accuracy: 0.8191 - val_loss: 0.4478 - val_accuracy: 0.8233 - lr: 3.0000e-04\n",
      "Epoch 2/20\n",
      "699/699 [==============================] - 115s 165ms/step - loss: 0.4001 - accuracy: 0.8751 - val_loss: 0.3727 - val_accuracy: 0.8748 - lr: 3.0000e-04\n",
      "Epoch 3/20\n",
      "699/699 [==============================] - 104s 149ms/step - loss: 0.2807 - accuracy: 0.9222 - val_loss: 0.1042 - val_accuracy: 0.9914 - lr: 3.0000e-04\n",
      "Epoch 4/20\n",
      "699/699 [==============================] - 103s 147ms/step - loss: 0.1675 - accuracy: 0.9706 - val_loss: 0.1237 - val_accuracy: 0.9863 - lr: 3.0000e-04\n",
      "Epoch 5/20\n",
      "699/699 [==============================] - 83s 118ms/step - loss: 0.1713 - accuracy: 0.9657 - val_loss: 0.1906 - val_accuracy: 0.9554 - lr: 3.0000e-04\n",
      "Epoch 6/20\n",
      "699/699 [==============================] - 65s 93ms/step - loss: 0.1340 - accuracy: 0.9808 - val_loss: 0.5910 - val_accuracy: 0.8010 - lr: 3.0000e-04\n",
      "Epoch 7/20\n",
      "699/699 [==============================] - 65s 93ms/step - loss: 0.1264 - accuracy: 0.9825 - val_loss: 0.1060 - val_accuracy: 0.9931 - lr: 3.0000e-04\n",
      "Epoch 8/20\n",
      "699/699 [==============================] - 65s 93ms/step - loss: 0.1192 - accuracy: 0.9824 - val_loss: 0.0727 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 9/20\n",
      "699/699 [==============================] - 65s 92ms/step - loss: 0.0932 - accuracy: 0.9914 - val_loss: 0.3298 - val_accuracy: 0.9074 - lr: 3.0000e-04\n",
      "Epoch 10/20\n",
      "699/699 [==============================] - 64s 92ms/step - loss: 0.0901 - accuracy: 0.9914 - val_loss: 0.0659 - val_accuracy: 0.9983 - lr: 3.0000e-04\n",
      "Epoch 11/20\n",
      "699/699 [==============================] - 66s 95ms/step - loss: 0.0827 - accuracy: 0.9927 - val_loss: 0.0767 - val_accuracy: 0.9949 - lr: 3.0000e-04\n",
      "Epoch 12/20\n",
      "699/699 [==============================] - 66s 94ms/step - loss: 0.0768 - accuracy: 0.9948 - val_loss: 0.0611 - val_accuracy: 0.9949 - lr: 3.0000e-04\n",
      "Epoch 13/20\n",
      "699/699 [==============================] - 65s 94ms/step - loss: 0.0705 - accuracy: 0.9945 - val_loss: 0.0720 - val_accuracy: 0.9880 - lr: 3.0000e-04\n",
      "Epoch 14/20\n",
      "699/699 [==============================] - 67s 95ms/step - loss: 0.0725 - accuracy: 0.9930 - val_loss: 0.0514 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 15/20\n",
      "699/699 [==============================] - 65s 93ms/step - loss: 0.0616 - accuracy: 0.9956 - val_loss: 0.0568 - val_accuracy: 0.9949 - lr: 3.0000e-04\n",
      "Epoch 16/20\n",
      "699/699 [==============================] - 86s 123ms/step - loss: 0.0563 - accuracy: 0.9957 - val_loss: 0.0766 - val_accuracy: 0.9880 - lr: 3.0000e-04\n",
      "Epoch 17/20\n",
      "699/699 [==============================] - 8683s 12s/step - loss: 0.0592 - accuracy: 0.9942 - val_loss: 0.0868 - val_accuracy: 0.9794 - lr: 3.0000e-04\n",
      "Epoch 18/20\n",
      "699/699 [==============================] - 63s 90ms/step - loss: 0.0487 - accuracy: 0.9978 - val_loss: 0.0460 - val_accuracy: 0.9949 - lr: 3.0000e-04\n",
      "Epoch 19/20\n",
      "699/699 [==============================] - 62s 89ms/step - loss: 0.0467 - accuracy: 0.9973 - val_loss: 0.0543 - val_accuracy: 0.9949 - lr: 3.0000e-04\n",
      "Epoch 20/20\n",
      "699/699 [==============================] - 62s 89ms/step - loss: 0.0470 - accuracy: 0.9970 - val_loss: 0.0425 - val_accuracy: 0.9983 - lr: 3.0000e-04\n",
      "(304, 453) (304, 1)\n",
      "Epoch 1/20\n",
      "716/716 [==============================] - 65s 88ms/step - loss: 0.5201 - accuracy: 0.8318 - val_loss: 0.3282 - val_accuracy: 0.9178 - lr: 3.0000e-04\n",
      "Epoch 2/20\n",
      "716/716 [==============================] - 63s 88ms/step - loss: 0.3554 - accuracy: 0.8924 - val_loss: 0.3128 - val_accuracy: 0.9145 - lr: 3.0000e-04\n",
      "Epoch 3/20\n",
      "716/716 [==============================] - 60s 84ms/step - loss: 0.2619 - accuracy: 0.9322 - val_loss: 0.1692 - val_accuracy: 0.9671 - lr: 3.0000e-04\n",
      "Epoch 4/20\n",
      "716/716 [==============================] - 60s 84ms/step - loss: 0.1616 - accuracy: 0.9712 - val_loss: 0.1399 - val_accuracy: 0.9704 - lr: 3.0000e-04\n",
      "Epoch 5/20\n",
      "716/716 [==============================] - 60s 84ms/step - loss: 0.2067 - accuracy: 0.9517 - val_loss: 0.3057 - val_accuracy: 0.9145 - lr: 3.0000e-04\n",
      "Epoch 6/20\n",
      "716/716 [==============================] - 60s 84ms/step - loss: 0.2246 - accuracy: 0.9452 - val_loss: 0.1114 - val_accuracy: 0.9803 - lr: 3.0000e-04\n",
      "Epoch 7/20\n",
      "716/716 [==============================] - 62s 86ms/step - loss: 0.1470 - accuracy: 0.9762 - val_loss: 0.0978 - val_accuracy: 0.9934 - lr: 3.0000e-04\n",
      "Epoch 8/20\n",
      "716/716 [==============================] - 63s 87ms/step - loss: 0.1553 - accuracy: 0.9714 - val_loss: 0.1155 - val_accuracy: 0.9934 - lr: 3.0000e-04\n",
      "Epoch 9/20\n",
      "716/716 [==============================] - 63s 88ms/step - loss: 0.1458 - accuracy: 0.9733 - val_loss: 0.1289 - val_accuracy: 0.9737 - lr: 3.0000e-04\n",
      "Epoch 10/20\n",
      "716/716 [==============================] - 63s 88ms/step - loss: 0.1289 - accuracy: 0.9818 - val_loss: 0.1749 - val_accuracy: 0.9770 - lr: 3.0000e-04\n",
      "Epoch 11/20\n",
      "716/716 [==============================] - 63s 87ms/step - loss: 0.1120 - accuracy: 0.9852 - val_loss: 0.0928 - val_accuracy: 0.9901 - lr: 3.0000e-04\n",
      "Epoch 12/20\n",
      "716/716 [==============================] - 61s 86ms/step - loss: 0.1005 - accuracy: 0.9887 - val_loss: 0.1279 - val_accuracy: 0.9836 - lr: 3.0000e-04\n",
      "Epoch 13/20\n",
      "716/716 [==============================] - 60s 84ms/step - loss: 0.1041 - accuracy: 0.9871 - val_loss: 0.1955 - val_accuracy: 0.9605 - lr: 3.0000e-04\n",
      "Epoch 14/20\n",
      "716/716 [==============================] - 60s 84ms/step - loss: 0.0909 - accuracy: 0.9913 - val_loss: 0.0819 - val_accuracy: 0.9934 - lr: 3.0000e-04\n",
      "Epoch 15/20\n",
      "716/716 [==============================] - 60s 83ms/step - loss: 0.0869 - accuracy: 0.9910 - val_loss: 0.1692 - val_accuracy: 0.9638 - lr: 3.0000e-04\n",
      "Epoch 16/20\n",
      "716/716 [==============================] - 60s 84ms/step - loss: 0.0771 - accuracy: 0.9932 - val_loss: 0.0920 - val_accuracy: 0.9803 - lr: 3.0000e-04\n",
      "Epoch 17/20\n",
      "716/716 [==============================] - 61s 85ms/step - loss: 0.0740 - accuracy: 0.9944 - val_loss: 0.0799 - val_accuracy: 0.9901 - lr: 3.0000e-04\n",
      "Epoch 18/20\n",
      "716/716 [==============================] - 63s 88ms/step - loss: 0.0737 - accuracy: 0.9930 - val_loss: 0.1366 - val_accuracy: 0.9737 - lr: 3.0000e-04\n",
      "Epoch 19/20\n",
      "716/716 [==============================] - 63s 88ms/step - loss: 0.0728 - accuracy: 0.9938 - val_loss: 0.0932 - val_accuracy: 0.9901 - lr: 3.0000e-04\n",
      "Epoch 20/20\n",
      "716/716 [==============================] - 63s 88ms/step - loss: 0.0629 - accuracy: 0.9955 - val_loss: 0.0515 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "(311, 453) (311, 1)\n",
      "Epoch 1/20\n",
      "716/716 [==============================] - 67s 88ms/step - loss: 0.5102 - accuracy: 0.8291 - val_loss: 1.2781 - val_accuracy: 0.6431 - lr: 3.0000e-04\n",
      "Epoch 2/20\n",
      "716/716 [==============================] - 63s 87ms/step - loss: 0.3364 - accuracy: 0.9015 - val_loss: 0.3180 - val_accuracy: 0.9068 - lr: 3.0000e-04\n",
      "Epoch 3/20\n",
      "716/716 [==============================] - 62s 87ms/step - loss: 0.2184 - accuracy: 0.9512 - val_loss: 0.3099 - val_accuracy: 0.9100 - lr: 3.0000e-04\n",
      "Epoch 4/20\n",
      "716/716 [==============================] - 63s 87ms/step - loss: 0.1658 - accuracy: 0.9699 - val_loss: 0.2467 - val_accuracy: 0.9357 - lr: 3.0000e-04\n",
      "Epoch 5/20\n",
      "716/716 [==============================] - 62s 87ms/step - loss: 0.1376 - accuracy: 0.9789 - val_loss: 0.4465 - val_accuracy: 0.8650 - lr: 3.0000e-04\n",
      "Epoch 6/20\n",
      "716/716 [==============================] - 62s 87ms/step - loss: 0.1288 - accuracy: 0.9824 - val_loss: 0.7251 - val_accuracy: 0.8778 - lr: 3.0000e-04\n",
      "Epoch 7/20\n",
      "716/716 [==============================] - 62s 87ms/step - loss: 0.1117 - accuracy: 0.9862 - val_loss: 0.2137 - val_accuracy: 0.9325 - lr: 3.0000e-04\n",
      "Epoch 8/20\n",
      "716/716 [==============================] - 63s 87ms/step - loss: 0.1023 - accuracy: 0.9887 - val_loss: 0.3263 - val_accuracy: 0.9357 - lr: 3.0000e-04\n",
      "Epoch 9/20\n",
      "716/716 [==============================] - 62s 87ms/step - loss: 0.0898 - accuracy: 0.9936 - val_loss: 0.2813 - val_accuracy: 0.9260 - lr: 3.0000e-04\n",
      "Epoch 10/20\n",
      "716/716 [==============================] - 63s 88ms/step - loss: 0.0920 - accuracy: 0.9918 - val_loss: 0.2951 - val_accuracy: 0.9260 - lr: 3.0000e-04\n",
      "Epoch 11/20\n",
      "716/716 [==============================] - 64s 89ms/step - loss: 0.0828 - accuracy: 0.9929 - val_loss: 0.1600 - val_accuracy: 0.9486 - lr: 3.0000e-04\n",
      "Epoch 12/20\n",
      "716/716 [==============================] - 62s 87ms/step - loss: 0.0701 - accuracy: 0.9958 - val_loss: 0.2409 - val_accuracy: 0.9421 - lr: 3.0000e-04\n",
      "Epoch 13/20\n",
      "716/716 [==============================] - 63s 87ms/step - loss: 0.0723 - accuracy: 0.9941 - val_loss: 0.1472 - val_accuracy: 0.9711 - lr: 3.0000e-04\n",
      "Epoch 14/20\n",
      "716/716 [==============================] - 63s 88ms/step - loss: 0.0631 - accuracy: 0.9964 - val_loss: 0.3633 - val_accuracy: 0.9164 - lr: 3.0000e-04\n",
      "Epoch 15/20\n",
      "716/716 [==============================] - 63s 88ms/step - loss: 0.0577 - accuracy: 0.9958 - val_loss: 0.1699 - val_accuracy: 0.9678 - lr: 3.0000e-04\n",
      "Epoch 16/20\n",
      "716/716 [==============================] - 63s 88ms/step - loss: 0.0562 - accuracy: 0.9967 - val_loss: 0.3876 - val_accuracy: 0.9068 - lr: 3.0000e-04\n",
      "Epoch 17/20\n",
      "716/716 [==============================] - 61s 86ms/step - loss: 0.0555 - accuracy: 0.9958 - val_loss: 0.4571 - val_accuracy: 0.9035 - lr: 3.0000e-04\n",
      "Epoch 18/20\n",
      "716/716 [==============================] - 61s 86ms/step - loss: 0.0561 - accuracy: 0.9948 - val_loss: 0.4352 - val_accuracy: 0.9068 - lr: 3.0000e-04\n",
      "Epoch 19/20\n",
      "716/716 [==============================] - 62s 87ms/step - loss: 0.0434 - accuracy: 0.9982 - val_loss: 0.1936 - val_accuracy: 0.9453 - lr: 3.0000e-04\n",
      "Epoch 20/20\n",
      "716/716 [==============================] - 63s 87ms/step - loss: 0.0401 - accuracy: 0.9983 - val_loss: 0.3429 - val_accuracy: 0.9228 - lr: 3.0000e-04\n",
      "(298, 453) (298, 1)\n",
      "Epoch 1/20\n",
      "717/717 [==============================] - 66s 89ms/step - loss: 0.5355 - accuracy: 0.8170 - val_loss: 0.5245 - val_accuracy: 0.7383 - lr: 3.0000e-04\n",
      "Epoch 2/20\n",
      "717/717 [==============================] - 63s 88ms/step - loss: 0.3677 - accuracy: 0.8922 - val_loss: 0.2735 - val_accuracy: 0.9128 - lr: 3.0000e-04\n",
      "Epoch 3/20\n",
      "717/717 [==============================] - 63s 88ms/step - loss: 0.2517 - accuracy: 0.9345 - val_loss: 0.1792 - val_accuracy: 0.9631 - lr: 3.0000e-04\n",
      "Epoch 4/20\n",
      "717/717 [==============================] - 61s 86ms/step - loss: 0.2000 - accuracy: 0.9563 - val_loss: 0.1657 - val_accuracy: 0.9832 - lr: 3.0000e-04\n",
      "Epoch 5/20\n",
      "717/717 [==============================] - 61s 85ms/step - loss: 0.1603 - accuracy: 0.9706 - val_loss: 0.1852 - val_accuracy: 0.9631 - lr: 3.0000e-04\n",
      "Epoch 6/20\n",
      "717/717 [==============================] - 61s 86ms/step - loss: 0.2330 - accuracy: 0.9427 - val_loss: 0.2709 - val_accuracy: 0.9060 - lr: 3.0000e-04\n",
      "Epoch 7/20\n",
      "717/717 [==============================] - 61s 85ms/step - loss: 0.2262 - accuracy: 0.9459 - val_loss: 0.2566 - val_accuracy: 0.9530 - lr: 3.0000e-04\n",
      "Epoch 8/20\n",
      "717/717 [==============================] - 61s 85ms/step - loss: 0.1815 - accuracy: 0.9588 - val_loss: 0.3905 - val_accuracy: 0.8758 - lr: 3.0000e-04\n",
      "Epoch 9/20\n",
      "717/717 [==============================] - 61s 86ms/step - loss: 0.2452 - accuracy: 0.9345 - val_loss: 0.2867 - val_accuracy: 0.9027 - lr: 3.0000e-04\n",
      "Epoch 10/20\n",
      "717/717 [==============================] - 62s 86ms/step - loss: 0.2431 - accuracy: 0.9374 - val_loss: 0.1886 - val_accuracy: 0.9597 - lr: 3.0000e-04\n",
      "Epoch 11/20\n",
      "717/717 [==============================] - 61s 85ms/step - loss: 0.2142 - accuracy: 0.9468 - val_loss: 0.1151 - val_accuracy: 0.9799 - lr: 3.0000e-04\n",
      "Epoch 12/20\n",
      "717/717 [==============================] - 61s 86ms/step - loss: 0.1691 - accuracy: 0.9640 - val_loss: 0.1033 - val_accuracy: 0.9866 - lr: 3.0000e-04\n",
      "Epoch 13/20\n",
      "717/717 [==============================] - 61s 85ms/step - loss: 0.1463 - accuracy: 0.9729 - val_loss: 0.1594 - val_accuracy: 0.9597 - lr: 3.0000e-04\n",
      "Epoch 14/20\n",
      "717/717 [==============================] - 61s 85ms/step - loss: 0.1460 - accuracy: 0.9736 - val_loss: 0.0865 - val_accuracy: 0.9966 - lr: 3.0000e-04\n",
      "Epoch 15/20\n",
      "717/717 [==============================] - 62s 86ms/step - loss: 0.1330 - accuracy: 0.9781 - val_loss: 0.1169 - val_accuracy: 0.9732 - lr: 3.0000e-04\n",
      "Epoch 16/20\n",
      "717/717 [==============================] - 65s 90ms/step - loss: 0.1235 - accuracy: 0.9804 - val_loss: 0.0709 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 17/20\n",
      "717/717 [==============================] - 64s 90ms/step - loss: 0.1213 - accuracy: 0.9819 - val_loss: 0.0989 - val_accuracy: 0.9899 - lr: 3.0000e-04\n",
      "Epoch 18/20\n",
      "717/717 [==============================] - 65s 90ms/step - loss: 0.1112 - accuracy: 0.9845 - val_loss: 0.0705 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 19/20\n",
      "717/717 [==============================] - 66s 91ms/step - loss: 0.1112 - accuracy: 0.9838 - val_loss: 0.0765 - val_accuracy: 0.9966 - lr: 3.0000e-04\n",
      "Epoch 20/20\n",
      "717/717 [==============================] - 66s 92ms/step - loss: 0.1034 - accuracy: 0.9854 - val_loss: 0.0637 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "(412, 453) (412, 1)\n",
      "Epoch 1/20\n",
      "709/709 [==============================] - 67s 91ms/step - loss: 0.5143 - accuracy: 0.8276 - val_loss: 0.4396 - val_accuracy: 0.8908 - lr: 3.0000e-04\n",
      "Epoch 2/20\n",
      "709/709 [==============================] - 63s 89ms/step - loss: 0.3759 - accuracy: 0.8842 - val_loss: 0.2354 - val_accuracy: 0.9417 - lr: 3.0000e-04\n",
      "Epoch 3/20\n",
      "709/709 [==============================] - 64s 90ms/step - loss: 0.2453 - accuracy: 0.9369 - val_loss: 0.1006 - val_accuracy: 0.9927 - lr: 3.0000e-04\n",
      "Epoch 4/20\n",
      "709/709 [==============================] - 64s 90ms/step - loss: 0.1687 - accuracy: 0.9676 - val_loss: 0.1173 - val_accuracy: 0.9806 - lr: 3.0000e-04\n",
      "Epoch 5/20\n",
      "709/709 [==============================] - 64s 90ms/step - loss: 0.1711 - accuracy: 0.9664 - val_loss: 0.2331 - val_accuracy: 0.9296 - lr: 3.0000e-04\n",
      "Epoch 6/20\n",
      "709/709 [==============================] - 63s 90ms/step - loss: 0.1340 - accuracy: 0.9805 - val_loss: 0.1017 - val_accuracy: 0.9903 - lr: 3.0000e-04\n",
      "Epoch 7/20\n",
      "709/709 [==============================] - 64s 90ms/step - loss: 0.1137 - accuracy: 0.9882 - val_loss: 0.0838 - val_accuracy: 0.9976 - lr: 3.0000e-04\n",
      "Epoch 8/20\n",
      "709/709 [==============================] - 64s 91ms/step - loss: 0.1019 - accuracy: 0.9910 - val_loss: 0.0775 - val_accuracy: 0.9976 - lr: 3.0000e-04\n",
      "Epoch 9/20\n",
      "709/709 [==============================] - 64s 90ms/step - loss: 0.0970 - accuracy: 0.9914 - val_loss: 0.1699 - val_accuracy: 0.9709 - lr: 3.0000e-04\n",
      "Epoch 10/20\n",
      "709/709 [==============================] - 64s 90ms/step - loss: 0.0845 - accuracy: 0.9937 - val_loss: 0.0631 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 11/20\n",
      "709/709 [==============================] - 63s 89ms/step - loss: 0.0833 - accuracy: 0.9927 - val_loss: 0.0612 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 12/20\n",
      "709/709 [==============================] - 63s 89ms/step - loss: 0.0710 - accuracy: 0.9963 - val_loss: 0.0619 - val_accuracy: 0.9976 - lr: 3.0000e-04\n",
      "Epoch 13/20\n",
      "709/709 [==============================] - 63s 89ms/step - loss: 0.0685 - accuracy: 0.9947 - val_loss: 0.0607 - val_accuracy: 0.9951 - lr: 3.0000e-04\n",
      "Epoch 14/20\n",
      "709/709 [==============================] - 63s 89ms/step - loss: 0.0723 - accuracy: 0.9934 - val_loss: 0.0498 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 15/20\n",
      "709/709 [==============================] - 64s 90ms/step - loss: 0.0583 - accuracy: 0.9967 - val_loss: 0.0476 - val_accuracy: 0.9976 - lr: 3.0000e-04\n",
      "Epoch 16/20\n",
      "709/709 [==============================] - 63s 88ms/step - loss: 0.0560 - accuracy: 0.9962 - val_loss: 0.0772 - val_accuracy: 0.9854 - lr: 3.0000e-04\n",
      "Epoch 17/20\n",
      "709/709 [==============================] - 62s 87ms/step - loss: 0.0564 - accuracy: 0.9964 - val_loss: 0.0445 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 18/20\n",
      "709/709 [==============================] - 62s 88ms/step - loss: 0.0523 - accuracy: 0.9970 - val_loss: 0.0813 - val_accuracy: 0.9806 - lr: 3.0000e-04\n",
      "Epoch 19/20\n",
      "709/709 [==============================] - 62s 88ms/step - loss: 0.0554 - accuracy: 0.9956 - val_loss: 0.0514 - val_accuracy: 0.9951 - lr: 3.0000e-04\n",
      "Epoch 20/20\n",
      "709/709 [==============================] - 62s 88ms/step - loss: 0.0486 - accuracy: 0.9971 - val_loss: 0.0392 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "(330, 453) (330, 1)\n",
      "Epoch 1/20\n",
      "715/715 [==============================] - 65s 86ms/step - loss: 0.5365 - accuracy: 0.8154 - val_loss: 0.3010 - val_accuracy: 0.9091 - lr: 3.0000e-04\n",
      "Epoch 2/20\n",
      "715/715 [==============================] - 61s 85ms/step - loss: 0.3694 - accuracy: 0.8852 - val_loss: 0.4283 - val_accuracy: 0.8758 - lr: 3.0000e-04\n",
      "Epoch 3/20\n",
      "715/715 [==============================] - 61s 85ms/step - loss: 0.2264 - accuracy: 0.9470 - val_loss: 0.1714 - val_accuracy: 0.9697 - lr: 3.0000e-04\n",
      "Epoch 4/20\n",
      "715/715 [==============================] - 61s 86ms/step - loss: 0.1533 - accuracy: 0.9775 - val_loss: 0.1741 - val_accuracy: 0.9606 - lr: 3.0000e-04\n",
      "Epoch 5/20\n",
      "715/715 [==============================] - 62s 87ms/step - loss: 0.1795 - accuracy: 0.9654 - val_loss: 0.1434 - val_accuracy: 0.9818 - lr: 3.0000e-04\n",
      "Epoch 6/20\n",
      "715/715 [==============================] - 64s 89ms/step - loss: 0.1322 - accuracy: 0.9840 - val_loss: 0.0963 - val_accuracy: 0.9939 - lr: 3.0000e-04\n",
      "Epoch 7/20\n",
      "715/715 [==============================] - 62s 87ms/step - loss: 0.1160 - accuracy: 0.9875 - val_loss: 0.1039 - val_accuracy: 0.9848 - lr: 3.0000e-04\n",
      "Epoch 8/20\n",
      "715/715 [==============================] - 62s 87ms/step - loss: 0.1032 - accuracy: 0.9902 - val_loss: 0.0796 - val_accuracy: 0.9970 - lr: 3.0000e-04\n",
      "Epoch 9/20\n",
      "715/715 [==============================] - 63s 88ms/step - loss: 0.1023 - accuracy: 0.9900 - val_loss: 0.0859 - val_accuracy: 0.9939 - lr: 3.0000e-04\n",
      "Epoch 10/20\n",
      "715/715 [==============================] - 63s 88ms/step - loss: 0.0907 - accuracy: 0.9925 - val_loss: 0.0915 - val_accuracy: 0.9939 - lr: 3.0000e-04\n",
      "Epoch 11/20\n",
      "715/715 [==============================] - 63s 88ms/step - loss: 0.0859 - accuracy: 0.9937 - val_loss: 0.0830 - val_accuracy: 0.9970 - lr: 3.0000e-04\n",
      "Epoch 12/20\n",
      "715/715 [==============================] - 61s 86ms/step - loss: 0.0778 - accuracy: 0.9940 - val_loss: 0.0606 - val_accuracy: 0.9970 - lr: 3.0000e-04\n",
      "Epoch 13/20\n",
      "715/715 [==============================] - 62s 87ms/step - loss: 0.0720 - accuracy: 0.9957 - val_loss: 0.0547 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 14/20\n",
      "715/715 [==============================] - 63s 88ms/step - loss: 0.0626 - accuracy: 0.9972 - val_loss: 0.0587 - val_accuracy: 0.9939 - lr: 3.0000e-04\n",
      "Epoch 15/20\n",
      "715/715 [==============================] - 62s 87ms/step - loss: 0.0653 - accuracy: 0.9947 - val_loss: 0.0469 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 16/20\n",
      "715/715 [==============================] - 62s 87ms/step - loss: 0.0574 - accuracy: 0.9967 - val_loss: 0.0433 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 17/20\n",
      "715/715 [==============================] - 63s 88ms/step - loss: 0.0514 - accuracy: 0.9972 - val_loss: 0.0406 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 18/20\n",
      "715/715 [==============================] - 62s 87ms/step - loss: 0.0590 - accuracy: 0.9941 - val_loss: 0.0417 - val_accuracy: 0.9970 - lr: 3.0000e-04\n",
      "Epoch 19/20\n",
      "715/715 [==============================] - 62s 87ms/step - loss: 0.0494 - accuracy: 0.9972 - val_loss: 0.1073 - val_accuracy: 0.9758 - lr: 3.0000e-04\n",
      "Epoch 20/20\n",
      "715/715 [==============================] - 64s 90ms/step - loss: 0.0409 - accuracy: 0.9982 - val_loss: 0.0348 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "(397, 453) (397, 1)\n",
      "Epoch 1/20\n",
      "710/710 [==============================] - 65s 88ms/step - loss: 0.5224 - accuracy: 0.8216 - val_loss: 0.3700 - val_accuracy: 0.9018 - lr: 3.0000e-04\n",
      "Epoch 2/20\n",
      "710/710 [==============================] - 63s 88ms/step - loss: 0.3858 - accuracy: 0.8786 - val_loss: 0.3622 - val_accuracy: 0.8816 - lr: 3.0000e-04\n",
      "Epoch 3/20\n",
      "710/710 [==============================] - 62s 87ms/step - loss: 0.2190 - accuracy: 0.9476 - val_loss: 0.2246 - val_accuracy: 0.9673 - lr: 3.0000e-04\n",
      "Epoch 4/20\n",
      "710/710 [==============================] - 63s 88ms/step - loss: 0.2663 - accuracy: 0.9302 - val_loss: 0.8320 - val_accuracy: 0.8262 - lr: 3.0000e-04\n",
      "Epoch 5/20\n",
      "710/710 [==============================] - 63s 88ms/step - loss: 0.1908 - accuracy: 0.9573 - val_loss: 0.2046 - val_accuracy: 0.9647 - lr: 3.0000e-04\n",
      "Epoch 6/20\n",
      "710/710 [==============================] - 61s 85ms/step - loss: 0.1993 - accuracy: 0.9542 - val_loss: 0.1835 - val_accuracy: 0.9698 - lr: 3.0000e-04\n",
      "Epoch 7/20\n",
      "710/710 [==============================] - 61s 86ms/step - loss: 0.1370 - accuracy: 0.9777 - val_loss: 0.2642 - val_accuracy: 0.9622 - lr: 3.0000e-04\n",
      "Epoch 8/20\n",
      "710/710 [==============================] - 62s 88ms/step - loss: 0.1174 - accuracy: 0.9849 - val_loss: 0.2358 - val_accuracy: 0.9673 - lr: 3.0000e-04\n",
      "Epoch 9/20\n",
      "710/710 [==============================] - 63s 89ms/step - loss: 0.1077 - accuracy: 0.9867 - val_loss: 0.1960 - val_accuracy: 0.9748 - lr: 3.0000e-04\n",
      "Epoch 10/20\n",
      "710/710 [==============================] - 63s 88ms/step - loss: 0.0954 - accuracy: 0.9905 - val_loss: 0.1944 - val_accuracy: 0.9698 - lr: 3.0000e-04\n",
      "Epoch 11/20\n",
      "710/710 [==============================] - 63s 89ms/step - loss: 0.0843 - accuracy: 0.9929 - val_loss: 0.1743 - val_accuracy: 0.9773 - lr: 3.0000e-04\n",
      "Epoch 12/20\n",
      "710/710 [==============================] - 63s 89ms/step - loss: 0.0832 - accuracy: 0.9916 - val_loss: 0.2485 - val_accuracy: 0.9723 - lr: 3.0000e-04\n",
      "Epoch 13/20\n",
      "710/710 [==============================] - 62s 87ms/step - loss: 0.0783 - accuracy: 0.9923 - val_loss: 0.3581 - val_accuracy: 0.9521 - lr: 3.0000e-04\n",
      "Epoch 14/20\n",
      "710/710 [==============================] - 62s 88ms/step - loss: 0.0714 - accuracy: 0.9933 - val_loss: 0.2130 - val_accuracy: 0.9723 - lr: 3.0000e-04\n",
      "Epoch 15/20\n",
      "710/710 [==============================] - 60s 85ms/step - loss: 0.0682 - accuracy: 0.9933 - val_loss: 0.1836 - val_accuracy: 0.9773 - lr: 3.0000e-04\n",
      "Epoch 16/20\n",
      "710/710 [==============================] - 60s 85ms/step - loss: 0.0726 - accuracy: 0.9920 - val_loss: 0.1799 - val_accuracy: 0.9698 - lr: 3.0000e-04\n",
      "Epoch 17/20\n",
      "710/710 [==============================] - 60s 84ms/step - loss: 0.0621 - accuracy: 0.9949 - val_loss: 0.1546 - val_accuracy: 0.9748 - lr: 3.0000e-04\n",
      "Epoch 18/20\n",
      "710/710 [==============================] - 60s 84ms/step - loss: 0.0531 - accuracy: 0.9971 - val_loss: 0.2427 - val_accuracy: 0.9673 - lr: 3.0000e-04\n",
      "Epoch 19/20\n",
      "710/710 [==============================] - 60s 84ms/step - loss: 0.0569 - accuracy: 0.9942 - val_loss: 0.1449 - val_accuracy: 0.9773 - lr: 3.0000e-04\n",
      "Epoch 20/20\n",
      "710/710 [==============================] - 60s 84ms/step - loss: 0.0556 - accuracy: 0.9949 - val_loss: 0.2959 - val_accuracy: 0.9295 - lr: 3.0000e-04\n",
      "(414, 453) (414, 1)\n",
      "Epoch 1/20\n",
      "709/709 [==============================] - 63s 85ms/step - loss: 0.5137 - accuracy: 0.8241 - val_loss: 0.6806 - val_accuracy: 0.8116 - lr: 3.0000e-04\n",
      "Epoch 2/20\n",
      "709/709 [==============================] - 60s 85ms/step - loss: 0.3327 - accuracy: 0.9034 - val_loss: 0.3965 - val_accuracy: 0.9106 - lr: 3.0000e-04\n",
      "Epoch 3/20\n",
      "709/709 [==============================] - 60s 85ms/step - loss: 0.2026 - accuracy: 0.9568 - val_loss: 0.2258 - val_accuracy: 0.9372 - lr: 3.0000e-04\n",
      "Epoch 4/20\n",
      "709/709 [==============================] - 62s 88ms/step - loss: 0.1858 - accuracy: 0.9622 - val_loss: 0.2111 - val_accuracy: 0.9589 - lr: 3.0000e-04\n",
      "Epoch 5/20\n",
      "709/709 [==============================] - 63s 89ms/step - loss: 0.1802 - accuracy: 0.9619 - val_loss: 0.6003 - val_accuracy: 0.7343 - lr: 3.0000e-04\n",
      "Epoch 6/20\n",
      "709/709 [==============================] - 62s 88ms/step - loss: 0.2693 - accuracy: 0.9237 - val_loss: 0.1864 - val_accuracy: 0.9565 - lr: 3.0000e-04\n",
      "Epoch 7/20\n",
      "709/709 [==============================] - 63s 88ms/step - loss: 0.1424 - accuracy: 0.9748 - val_loss: 0.1684 - val_accuracy: 0.9565 - lr: 3.0000e-04\n",
      "Epoch 8/20\n",
      "709/709 [==============================] - 62s 88ms/step - loss: 0.1209 - accuracy: 0.9857 - val_loss: 0.1031 - val_accuracy: 0.9831 - lr: 3.0000e-04\n",
      "Epoch 9/20\n",
      "709/709 [==============================] - 62s 88ms/step - loss: 0.1021 - accuracy: 0.9889 - val_loss: 0.0936 - val_accuracy: 0.9855 - lr: 3.0000e-04\n",
      "Epoch 10/20\n",
      "709/709 [==============================] - 62s 88ms/step - loss: 0.1038 - accuracy: 0.9877 - val_loss: 0.2190 - val_accuracy: 0.9565 - lr: 3.0000e-04\n",
      "Epoch 11/20\n",
      "709/709 [==============================] - 62s 88ms/step - loss: 0.0875 - accuracy: 0.9923 - val_loss: 0.1291 - val_accuracy: 0.9783 - lr: 3.0000e-04\n",
      "Epoch 12/20\n",
      "709/709 [==============================] - 62s 88ms/step - loss: 0.0809 - accuracy: 0.9926 - val_loss: 0.0769 - val_accuracy: 0.9903 - lr: 3.0000e-04\n",
      "Epoch 13/20\n",
      "709/709 [==============================] - 62s 88ms/step - loss: 0.0785 - accuracy: 0.9923 - val_loss: 0.1071 - val_accuracy: 0.9831 - lr: 3.0000e-04\n",
      "Epoch 14/20\n",
      "709/709 [==============================] - 62s 88ms/step - loss: 0.0685 - accuracy: 0.9953 - val_loss: 0.0908 - val_accuracy: 0.9879 - lr: 3.0000e-04\n",
      "Epoch 15/20\n",
      "709/709 [==============================] - 61s 86ms/step - loss: 0.0663 - accuracy: 0.9946 - val_loss: 0.0640 - val_accuracy: 0.9903 - lr: 3.0000e-04\n",
      "Epoch 16/20\n",
      "709/709 [==============================] - 60s 85ms/step - loss: 0.0644 - accuracy: 0.9943 - val_loss: 0.0658 - val_accuracy: 0.9928 - lr: 3.0000e-04\n",
      "Epoch 17/20\n",
      "709/709 [==============================] - 60s 85ms/step - loss: 0.0627 - accuracy: 0.9949 - val_loss: 0.1141 - val_accuracy: 0.9783 - lr: 3.0000e-04\n",
      "Epoch 18/20\n",
      "709/709 [==============================] - 60s 85ms/step - loss: 0.0566 - accuracy: 0.9960 - val_loss: 0.1251 - val_accuracy: 0.9783 - lr: 3.0000e-04\n",
      "Epoch 19/20\n",
      "709/709 [==============================] - 60s 85ms/step - loss: 0.0506 - accuracy: 0.9974 - val_loss: 0.0483 - val_accuracy: 0.9952 - lr: 3.0000e-04\n",
      "Epoch 20/20\n",
      "709/709 [==============================] - 61s 85ms/step - loss: 0.0480 - accuracy: 0.9972 - val_loss: 0.0648 - val_accuracy: 0.9879 - lr: 3.0000e-04\n",
      "(313, 453) (313, 1)\n",
      "Epoch 1/20\n",
      "716/716 [==============================] - 64s 85ms/step - loss: 0.5163 - accuracy: 0.8242 - val_loss: 0.2781 - val_accuracy: 0.9393 - lr: 3.0000e-04\n",
      "Epoch 2/20\n",
      "716/716 [==============================] - 60s 84ms/step - loss: 0.3451 - accuracy: 0.8989 - val_loss: 0.1408 - val_accuracy: 0.9808 - lr: 3.0000e-04\n",
      "Epoch 3/20\n",
      "716/716 [==============================] - 61s 85ms/step - loss: 0.3649 - accuracy: 0.8901 - val_loss: 0.2398 - val_accuracy: 0.9233 - lr: 3.0000e-04\n",
      "Epoch 4/20\n",
      "716/716 [==============================] - 61s 85ms/step - loss: 0.3177 - accuracy: 0.9071 - val_loss: 0.1052 - val_accuracy: 0.9936 - lr: 3.0000e-04\n",
      "Epoch 5/20\n",
      "716/716 [==============================] - 61s 85ms/step - loss: 0.1796 - accuracy: 0.9648 - val_loss: 0.0832 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 6/20\n",
      "716/716 [==============================] - 61s 85ms/step - loss: 0.1548 - accuracy: 0.9740 - val_loss: 0.0795 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 7/20\n",
      "716/716 [==============================] - 61s 85ms/step - loss: 0.1218 - accuracy: 0.9837 - val_loss: 0.1115 - val_accuracy: 0.9872 - lr: 3.0000e-04\n",
      "Epoch 8/20\n",
      "716/716 [==============================] - 62s 86ms/step - loss: 0.1108 - accuracy: 0.9863 - val_loss: 0.1225 - val_accuracy: 0.9776 - lr: 3.0000e-04\n",
      "Epoch 9/20\n",
      "716/716 [==============================] - 63s 88ms/step - loss: 0.1062 - accuracy: 0.9857 - val_loss: 0.0776 - val_accuracy: 0.9936 - lr: 3.0000e-04\n",
      "Epoch 10/20\n",
      "716/716 [==============================] - 63s 88ms/step - loss: 0.0983 - accuracy: 0.9882 - val_loss: 0.0640 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 11/20\n",
      "716/716 [==============================] - 63s 88ms/step - loss: 0.0927 - accuracy: 0.9893 - val_loss: 0.0751 - val_accuracy: 0.9904 - lr: 3.0000e-04\n",
      "Epoch 12/20\n",
      "716/716 [==============================] - 63s 88ms/step - loss: 0.0884 - accuracy: 0.9893 - val_loss: 0.0597 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 13/20\n",
      "716/716 [==============================] - 62s 87ms/step - loss: 0.0799 - accuracy: 0.9921 - val_loss: 0.0721 - val_accuracy: 0.9936 - lr: 3.0000e-04\n",
      "Epoch 14/20\n",
      "716/716 [==============================] - 61s 85ms/step - loss: 0.0937 - accuracy: 0.9865 - val_loss: 0.0539 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 15/20\n",
      "716/716 [==============================] - 60s 84ms/step - loss: 0.0760 - accuracy: 0.9927 - val_loss: 0.0506 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 16/20\n",
      "716/716 [==============================] - 62s 87ms/step - loss: 0.0692 - accuracy: 0.9939 - val_loss: 0.0518 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 17/20\n",
      "716/716 [==============================] - 63s 88ms/step - loss: 0.0665 - accuracy: 0.9950 - val_loss: 0.0570 - val_accuracy: 0.9936 - lr: 3.0000e-04\n",
      "Epoch 18/20\n",
      "716/716 [==============================] - 63s 87ms/step - loss: 0.0598 - accuracy: 0.9965 - val_loss: 0.0728 - val_accuracy: 0.9904 - lr: 3.0000e-04\n",
      "Epoch 19/20\n",
      "716/716 [==============================] - 63s 88ms/step - loss: 0.0577 - accuracy: 0.9967 - val_loss: 0.0575 - val_accuracy: 0.9968 - lr: 3.0000e-04\n",
      "Epoch 20/20\n",
      "716/716 [==============================] - 63s 88ms/step - loss: 0.0629 - accuracy: 0.9945 - val_loss: 0.0420 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "(392, 453) (392, 1)\n",
      "Epoch 1/20\n",
      "711/711 [==============================] - 64s 86ms/step - loss: 0.5370 - accuracy: 0.8162 - val_loss: 0.2744 - val_accuracy: 0.9362 - lr: 3.0000e-04\n",
      "Epoch 2/20\n",
      "711/711 [==============================] - 61s 85ms/step - loss: 0.3948 - accuracy: 0.8746 - val_loss: 0.1891 - val_accuracy: 0.9694 - lr: 3.0000e-04\n",
      "Epoch 3/20\n",
      "711/711 [==============================] - 60s 85ms/step - loss: 0.3645 - accuracy: 0.8948 - val_loss: 0.1868 - val_accuracy: 0.9668 - lr: 3.0000e-04\n",
      "Epoch 4/20\n",
      "711/711 [==============================] - 61s 85ms/step - loss: 0.2386 - accuracy: 0.9430 - val_loss: 0.1358 - val_accuracy: 0.9770 - lr: 3.0000e-04\n",
      "Epoch 5/20\n",
      "711/711 [==============================] - 61s 85ms/step - loss: 0.1819 - accuracy: 0.9645 - val_loss: 0.0925 - val_accuracy: 0.9974 - lr: 3.0000e-04\n",
      "Epoch 6/20\n",
      "711/711 [==============================] - 60s 85ms/step - loss: 0.1455 - accuracy: 0.9772 - val_loss: 0.0830 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 7/20\n",
      "711/711 [==============================] - 62s 87ms/step - loss: 0.1186 - accuracy: 0.9864 - val_loss: 0.2064 - val_accuracy: 0.9362 - lr: 3.0000e-04\n",
      "Epoch 8/20\n",
      "711/711 [==============================] - 62s 88ms/step - loss: 0.1187 - accuracy: 0.9847 - val_loss: 0.1075 - val_accuracy: 0.9821 - lr: 3.0000e-04\n",
      "Epoch 9/20\n",
      "711/711 [==============================] - 63s 88ms/step - loss: 0.1100 - accuracy: 0.9888 - val_loss: 0.1000 - val_accuracy: 0.9898 - lr: 3.0000e-04\n",
      "Epoch 10/20\n",
      "711/711 [==============================] - 62s 88ms/step - loss: 0.1198 - accuracy: 0.9831 - val_loss: 0.0886 - val_accuracy: 0.9923 - lr: 3.0000e-04\n",
      "Epoch 11/20\n",
      "711/711 [==============================] - 62s 87ms/step - loss: 0.1118 - accuracy: 0.9845 - val_loss: 0.0740 - val_accuracy: 0.9949 - lr: 3.0000e-04\n",
      "Epoch 12/20\n",
      "711/711 [==============================] - 61s 86ms/step - loss: 0.0894 - accuracy: 0.9932 - val_loss: 0.0659 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 13/20\n",
      "711/711 [==============================] - 62s 88ms/step - loss: 0.0778 - accuracy: 0.9949 - val_loss: 0.0644 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 14/20\n",
      "711/711 [==============================] - 63s 89ms/step - loss: 0.0802 - accuracy: 0.9933 - val_loss: 0.4404 - val_accuracy: 0.9133 - lr: 3.0000e-04\n",
      "Epoch 15/20\n",
      "711/711 [==============================] - 63s 89ms/step - loss: 0.0748 - accuracy: 0.9944 - val_loss: 0.0608 - val_accuracy: 0.9974 - lr: 3.0000e-04\n",
      "Epoch 16/20\n",
      "711/711 [==============================] - 64s 89ms/step - loss: 0.0688 - accuracy: 0.9952 - val_loss: 0.0613 - val_accuracy: 0.9923 - lr: 3.0000e-04\n",
      "Epoch 17/20\n",
      "711/711 [==============================] - 63s 88ms/step - loss: 0.0647 - accuracy: 0.9952 - val_loss: 0.0483 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 18/20\n",
      "711/711 [==============================] - 63s 88ms/step - loss: 0.0607 - accuracy: 0.9953 - val_loss: 0.1025 - val_accuracy: 0.9745 - lr: 3.0000e-04\n",
      "Epoch 19/20\n",
      "711/711 [==============================] - 61s 86ms/step - loss: 0.0572 - accuracy: 0.9957 - val_loss: 0.0536 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 20/20\n",
      "711/711 [==============================] - 62s 87ms/step - loss: 0.0568 - accuracy: 0.9952 - val_loss: 0.0622 - val_accuracy: 0.9872 - lr: 3.0000e-04\n",
      "(500, 453) (500, 1)\n",
      "Epoch 1/20\n",
      "704/704 [==============================] - 65s 89ms/step - loss: 0.5167 - accuracy: 0.8263 - val_loss: 0.5234 - val_accuracy: 0.8060 - lr: 3.0000e-04\n",
      "Epoch 2/20\n",
      "704/704 [==============================] - 62s 87ms/step - loss: 0.3906 - accuracy: 0.8807 - val_loss: 0.2291 - val_accuracy: 0.9520 - lr: 3.0000e-04\n",
      "Epoch 3/20\n",
      "704/704 [==============================] - 62s 87ms/step - loss: 0.2078 - accuracy: 0.9544 - val_loss: 0.1624 - val_accuracy: 0.9740 - lr: 3.0000e-04\n",
      "Epoch 4/20\n",
      "704/704 [==============================] - 62s 88ms/step - loss: 0.1621 - accuracy: 0.9716 - val_loss: 0.1118 - val_accuracy: 0.9840 - lr: 3.0000e-04\n",
      "Epoch 5/20\n",
      "704/704 [==============================] - 62s 88ms/step - loss: 0.1422 - accuracy: 0.9773 - val_loss: 0.1532 - val_accuracy: 0.9780 - lr: 3.0000e-04\n",
      "Epoch 6/20\n",
      "704/704 [==============================] - 61s 87ms/step - loss: 0.1297 - accuracy: 0.9812 - val_loss: 0.1153 - val_accuracy: 0.9880 - lr: 3.0000e-04\n",
      "Epoch 7/20\n",
      "704/704 [==============================] - 62s 87ms/step - loss: 0.1143 - accuracy: 0.9845 - val_loss: 0.0801 - val_accuracy: 0.9980 - lr: 3.0000e-04\n",
      "Epoch 8/20\n",
      "704/704 [==============================] - 62s 87ms/step - loss: 0.1101 - accuracy: 0.9861 - val_loss: 0.0728 - val_accuracy: 0.9980 - lr: 3.0000e-04\n",
      "Epoch 9/20\n",
      "704/704 [==============================] - 64s 91ms/step - loss: 0.0944 - accuracy: 0.9910 - val_loss: 0.0676 - val_accuracy: 0.9980 - lr: 3.0000e-04\n",
      "Epoch 10/20\n",
      "704/704 [==============================] - 62s 88ms/step - loss: 0.0950 - accuracy: 0.9892 - val_loss: 0.0705 - val_accuracy: 0.9960 - lr: 3.0000e-04\n",
      "Epoch 11/20\n",
      "704/704 [==============================] - 62s 88ms/step - loss: 0.0864 - accuracy: 0.9901 - val_loss: 0.0620 - val_accuracy: 0.9980 - lr: 3.0000e-04\n",
      "Epoch 12/20\n",
      "704/704 [==============================] - 62s 88ms/step - loss: 0.0807 - accuracy: 0.9907 - val_loss: 0.0672 - val_accuracy: 0.9960 - lr: 3.0000e-04\n",
      "Epoch 13/20\n",
      "704/704 [==============================] - 64s 90ms/step - loss: 0.0798 - accuracy: 0.9923 - val_loss: 0.0620 - val_accuracy: 0.9940 - lr: 3.0000e-04\n",
      "Epoch 14/20\n",
      "704/704 [==============================] - 64s 91ms/step - loss: 0.0731 - accuracy: 0.9923 - val_loss: 0.0520 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 15/20\n",
      "704/704 [==============================] - 64s 91ms/step - loss: 0.0724 - accuracy: 0.9916 - val_loss: 0.0569 - val_accuracy: 0.9920 - lr: 3.0000e-04\n",
      "Epoch 16/20\n",
      "704/704 [==============================] - 63s 90ms/step - loss: 0.0606 - accuracy: 0.9946 - val_loss: 0.0641 - val_accuracy: 0.9960 - lr: 3.0000e-04\n",
      "Epoch 17/20\n",
      "704/704 [==============================] - 62s 88ms/step - loss: 0.0628 - accuracy: 0.9941 - val_loss: 0.0507 - val_accuracy: 0.9960 - lr: 3.0000e-04\n",
      "Epoch 18/20\n",
      "704/704 [==============================] - 62s 89ms/step - loss: 0.0604 - accuracy: 0.9945 - val_loss: 0.0454 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 19/20\n",
      "704/704 [==============================] - 62s 88ms/step - loss: 0.0588 - accuracy: 0.9957 - val_loss: 0.0445 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 20/20\n",
      "704/704 [==============================] - 63s 89ms/step - loss: 0.0558 - accuracy: 0.9957 - val_loss: 0.0403 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "(377, 453) (377, 1)\n",
      "Epoch 1/20\n",
      "712/712 [==============================] - 67s 90ms/step - loss: 0.5208 - accuracy: 0.8212 - val_loss: 0.3877 - val_accuracy: 0.8939 - lr: 3.0000e-04\n",
      "Epoch 2/20\n",
      "712/712 [==============================] - 63s 88ms/step - loss: 0.4184 - accuracy: 0.8662 - val_loss: 0.5473 - val_accuracy: 0.8355 - lr: 3.0000e-04\n",
      "Epoch 3/20\n",
      "712/712 [==============================] - 63s 88ms/step - loss: 0.2743 - accuracy: 0.9263 - val_loss: 0.3106 - val_accuracy: 0.8647 - lr: 3.0000e-04\n",
      "Epoch 4/20\n",
      "712/712 [==============================] - 62s 87ms/step - loss: 0.2347 - accuracy: 0.9445 - val_loss: 0.3036 - val_accuracy: 0.9151 - lr: 3.0000e-04\n",
      "Epoch 5/20\n",
      "712/712 [==============================] - 62s 88ms/step - loss: 0.1832 - accuracy: 0.9632 - val_loss: 0.2345 - val_accuracy: 0.9496 - lr: 3.0000e-04\n",
      "Epoch 6/20\n",
      "712/712 [==============================] - 62s 87ms/step - loss: 0.1505 - accuracy: 0.9744 - val_loss: 0.1997 - val_accuracy: 0.9708 - lr: 3.0000e-04\n",
      "Epoch 7/20\n",
      "712/712 [==============================] - 63s 89ms/step - loss: 0.1226 - accuracy: 0.9845 - val_loss: 0.2883 - val_accuracy: 0.9390 - lr: 3.0000e-04\n",
      "Epoch 8/20\n",
      "712/712 [==============================] - 62s 87ms/step - loss: 0.1130 - accuracy: 0.9873 - val_loss: 0.1683 - val_accuracy: 0.9708 - lr: 3.0000e-04\n",
      "Epoch 9/20\n",
      "712/712 [==============================] - 62s 87ms/step - loss: 0.1086 - accuracy: 0.9884 - val_loss: 0.2112 - val_accuracy: 0.9416 - lr: 3.0000e-04\n",
      "Epoch 10/20\n",
      "712/712 [==============================] - 62s 88ms/step - loss: 0.0966 - accuracy: 0.9904 - val_loss: 0.2836 - val_accuracy: 0.9469 - lr: 3.0000e-04\n",
      "Epoch 11/20\n",
      "712/712 [==============================] - 62s 87ms/step - loss: 0.0811 - accuracy: 0.9945 - val_loss: 0.1201 - val_accuracy: 0.9947 - lr: 3.0000e-04\n",
      "Epoch 12/20\n",
      "712/712 [==============================] - 62s 87ms/step - loss: 0.0841 - accuracy: 0.9924 - val_loss: 0.1661 - val_accuracy: 0.9814 - lr: 3.0000e-04\n",
      "Epoch 13/20\n",
      "712/712 [==============================] - 61s 85ms/step - loss: 0.0772 - accuracy: 0.9952 - val_loss: 0.1353 - val_accuracy: 0.9735 - lr: 3.0000e-04\n",
      "Epoch 14/20\n",
      "712/712 [==============================] - 64s 89ms/step - loss: 0.0647 - accuracy: 0.9963 - val_loss: 0.1525 - val_accuracy: 0.9761 - lr: 3.0000e-04\n",
      "Epoch 15/20\n",
      "712/712 [==============================] - 63s 88ms/step - loss: 0.0733 - accuracy: 0.9931 - val_loss: 0.0963 - val_accuracy: 0.9947 - lr: 3.0000e-04\n",
      "Epoch 16/20\n",
      "712/712 [==============================] - 62s 87ms/step - loss: 0.0667 - accuracy: 0.9950 - val_loss: 0.1007 - val_accuracy: 0.9947 - lr: 3.0000e-04\n",
      "Epoch 17/20\n",
      "712/712 [==============================] - 63s 88ms/step - loss: 0.0604 - accuracy: 0.9959 - val_loss: 0.1323 - val_accuracy: 0.9867 - lr: 3.0000e-04\n",
      "Epoch 18/20\n",
      "712/712 [==============================] - 63s 88ms/step - loss: 0.0585 - accuracy: 0.9960 - val_loss: 0.0792 - val_accuracy: 0.9947 - lr: 3.0000e-04\n",
      "Epoch 19/20\n",
      "712/712 [==============================] - 63s 88ms/step - loss: 0.0468 - accuracy: 0.9982 - val_loss: 0.1240 - val_accuracy: 0.9814 - lr: 3.0000e-04\n",
      "Epoch 20/20\n",
      "712/712 [==============================] - 64s 89ms/step - loss: 0.0495 - accuracy: 0.9968 - val_loss: 0.2824 - val_accuracy: 0.9629 - lr: 3.0000e-04\n",
      "(372, 453) (372, 1)\n",
      "Epoch 1/20\n",
      "712/712 [==============================] - 66s 89ms/step - loss: 0.5281 - accuracy: 0.8201 - val_loss: 0.3641 - val_accuracy: 0.9140 - lr: 3.0000e-04\n",
      "Epoch 2/20\n",
      "712/712 [==============================] - 63s 89ms/step - loss: 0.3729 - accuracy: 0.8852 - val_loss: 0.3109 - val_accuracy: 0.9059 - lr: 3.0000e-04\n",
      "Epoch 3/20\n",
      "712/712 [==============================] - 62s 87ms/step - loss: 0.1780 - accuracy: 0.9657 - val_loss: 0.1030 - val_accuracy: 0.9946 - lr: 3.0000e-04\n",
      "Epoch 4/20\n",
      "712/712 [==============================] - 60s 85ms/step - loss: 0.1298 - accuracy: 0.9831 - val_loss: 0.1011 - val_accuracy: 0.9946 - lr: 3.0000e-04\n",
      "Epoch 5/20\n",
      "712/712 [==============================] - 60s 85ms/step - loss: 0.1267 - accuracy: 0.9831 - val_loss: 0.0837 - val_accuracy: 0.9946 - lr: 3.0000e-04\n",
      "Epoch 6/20\n",
      "712/712 [==============================] - 60s 84ms/step - loss: 0.1007 - accuracy: 0.9913 - val_loss: 0.0798 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 7/20\n",
      "712/712 [==============================] - 60s 84ms/step - loss: 0.0979 - accuracy: 0.9904 - val_loss: 0.0697 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 8/20\n",
      "712/712 [==============================] - 60s 84ms/step - loss: 0.0923 - accuracy: 0.9912 - val_loss: 0.0643 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 9/20\n",
      "712/712 [==============================] - 60s 84ms/step - loss: 0.0830 - accuracy: 0.9926 - val_loss: 0.0611 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 10/20\n",
      "712/712 [==============================] - 60s 84ms/step - loss: 0.0788 - accuracy: 0.9936 - val_loss: 0.0655 - val_accuracy: 0.9973 - lr: 3.0000e-04\n",
      "Epoch 11/20\n",
      "712/712 [==============================] - 60s 84ms/step - loss: 0.0671 - accuracy: 0.9956 - val_loss: 0.0591 - val_accuracy: 0.9973 - lr: 3.0000e-04\n",
      "Epoch 12/20\n",
      "712/712 [==============================] - 60s 84ms/step - loss: 0.0643 - accuracy: 0.9958 - val_loss: 0.0662 - val_accuracy: 0.9919 - lr: 3.0000e-04\n",
      "Epoch 13/20\n",
      "712/712 [==============================] - 60s 84ms/step - loss: 0.0671 - accuracy: 0.9944 - val_loss: 0.0674 - val_accuracy: 0.9973 - lr: 3.0000e-04\n",
      "Epoch 14/20\n",
      "712/712 [==============================] - 58s 82ms/step - loss: 0.0602 - accuracy: 0.9955 - val_loss: 0.0442 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 15/20\n",
      "712/712 [==============================] - 59s 83ms/step - loss: 0.0485 - accuracy: 0.9990 - val_loss: 0.0435 - val_accuracy: 0.9973 - lr: 3.0000e-04\n",
      "Epoch 16/20\n",
      "712/712 [==============================] - 60s 84ms/step - loss: 0.0520 - accuracy: 0.9974 - val_loss: 0.0408 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 17/20\n",
      "712/712 [==============================] - 60s 84ms/step - loss: 0.0501 - accuracy: 0.9968 - val_loss: 0.0471 - val_accuracy: 0.9973 - lr: 3.0000e-04\n",
      "Epoch 18/20\n",
      "712/712 [==============================] - 60s 84ms/step - loss: 0.0464 - accuracy: 0.9978 - val_loss: 0.0397 - val_accuracy: 0.9973 - lr: 3.0000e-04\n",
      "Epoch 19/20\n",
      "712/712 [==============================] - 60s 84ms/step - loss: 0.0507 - accuracy: 0.9951 - val_loss: 0.0347 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 20/20\n",
      "712/712 [==============================] - 60s 84ms/step - loss: 0.0433 - accuracy: 0.9970 - val_loss: 0.0471 - val_accuracy: 0.9973 - lr: 3.0000e-04\n",
      "(395, 453) (395, 1)\n",
      "Epoch 1/20\n",
      "711/711 [==============================] - 58s 79ms/step - loss: 0.5207 - accuracy: 0.8281 - val_loss: 0.5058 - val_accuracy: 0.8684 - lr: 3.0000e-04\n",
      "Epoch 2/20\n",
      "711/711 [==============================] - 56s 79ms/step - loss: 0.3641 - accuracy: 0.8884 - val_loss: 1.0107 - val_accuracy: 0.8025 - lr: 3.0000e-04\n",
      "Epoch 3/20\n",
      "711/711 [==============================] - 56s 79ms/step - loss: 0.2957 - accuracy: 0.9187 - val_loss: 0.2785 - val_accuracy: 0.9392 - lr: 3.0000e-04\n",
      "Epoch 4/20\n",
      "711/711 [==============================] - 56s 79ms/step - loss: 0.2894 - accuracy: 0.9211 - val_loss: 0.2649 - val_accuracy: 0.9241 - lr: 3.0000e-04\n",
      "Epoch 5/20\n",
      "711/711 [==============================] - 57s 80ms/step - loss: 0.1771 - accuracy: 0.9633 - val_loss: 0.2438 - val_accuracy: 0.9468 - lr: 3.0000e-04\n",
      "Epoch 6/20\n",
      "711/711 [==============================] - 58s 82ms/step - loss: 0.1201 - accuracy: 0.9850 - val_loss: 0.2481 - val_accuracy: 0.9519 - lr: 3.0000e-04\n",
      "Epoch 7/20\n",
      "711/711 [==============================] - 58s 82ms/step - loss: 0.1054 - accuracy: 0.9885 - val_loss: 0.1243 - val_accuracy: 0.9722 - lr: 3.0000e-04\n",
      "Epoch 8/20\n",
      "711/711 [==============================] - 58s 82ms/step - loss: 0.1112 - accuracy: 0.9866 - val_loss: 0.1103 - val_accuracy: 0.9899 - lr: 3.0000e-04\n",
      "Epoch 9/20\n",
      "711/711 [==============================] - 57s 81ms/step - loss: 0.0924 - accuracy: 0.9922 - val_loss: 0.3125 - val_accuracy: 0.9570 - lr: 3.0000e-04\n",
      "Epoch 10/20\n",
      "711/711 [==============================] - 58s 81ms/step - loss: 0.0930 - accuracy: 0.9909 - val_loss: 0.3221 - val_accuracy: 0.9494 - lr: 3.0000e-04\n",
      "Epoch 11/20\n",
      "711/711 [==============================] - 58s 82ms/step - loss: 0.0788 - accuracy: 0.9947 - val_loss: 0.1930 - val_accuracy: 0.9646 - lr: 3.0000e-04\n",
      "Epoch 12/20\n",
      "711/711 [==============================] - 58s 81ms/step - loss: 0.0723 - accuracy: 0.9960 - val_loss: 0.1731 - val_accuracy: 0.9620 - lr: 3.0000e-04\n",
      "Epoch 13/20\n",
      "711/711 [==============================] - 58s 82ms/step - loss: 0.0732 - accuracy: 0.9949 - val_loss: 0.1117 - val_accuracy: 0.9823 - lr: 3.0000e-04\n",
      "Epoch 14/20\n",
      "711/711 [==============================] - 58s 82ms/step - loss: 0.0635 - accuracy: 0.9971 - val_loss: 0.0925 - val_accuracy: 0.9823 - lr: 3.0000e-04\n",
      "Epoch 15/20\n",
      "711/711 [==============================] - 58s 81ms/step - loss: 0.0653 - accuracy: 0.9957 - val_loss: 0.0922 - val_accuracy: 0.9873 - lr: 3.0000e-04\n",
      "Epoch 16/20\n",
      "711/711 [==============================] - 58s 81ms/step - loss: 0.0570 - accuracy: 0.9969 - val_loss: 0.0598 - val_accuracy: 0.9949 - lr: 3.0000e-04\n",
      "Epoch 17/20\n",
      "711/711 [==============================] - 58s 81ms/step - loss: 0.0496 - accuracy: 0.9982 - val_loss: 0.1883 - val_accuracy: 0.9620 - lr: 3.0000e-04\n",
      "Epoch 18/20\n",
      "711/711 [==============================] - 58s 81ms/step - loss: 0.0501 - accuracy: 0.9961 - val_loss: 0.0522 - val_accuracy: 0.9924 - lr: 3.0000e-04\n",
      "Epoch 19/20\n",
      "711/711 [==============================] - 58s 81ms/step - loss: 0.0473 - accuracy: 0.9970 - val_loss: 0.0917 - val_accuracy: 0.9873 - lr: 3.0000e-04\n",
      "Epoch 20/20\n",
      "711/711 [==============================] - 58s 81ms/step - loss: 0.0430 - accuracy: 0.9977 - val_loss: 0.0470 - val_accuracy: 0.9975 - lr: 3.0000e-04\n",
      "(468, 453) (468, 1)\n",
      "Epoch 1/20\n",
      "706/706 [==============================] - 65s 88ms/step - loss: 0.5255 - accuracy: 0.8262 - val_loss: 0.4278 - val_accuracy: 0.8440 - lr: 3.0000e-04\n",
      "Epoch 2/20\n",
      "706/706 [==============================] - 61s 87ms/step - loss: 0.3949 - accuracy: 0.8820 - val_loss: 0.5584 - val_accuracy: 0.8120 - lr: 3.0000e-04\n",
      "Epoch 3/20\n",
      "706/706 [==============================] - 62s 87ms/step - loss: 0.3153 - accuracy: 0.9106 - val_loss: 0.4196 - val_accuracy: 0.8547 - lr: 3.0000e-04\n",
      "Epoch 4/20\n",
      "706/706 [==============================] - 62s 88ms/step - loss: 0.2018 - accuracy: 0.9592 - val_loss: 0.1667 - val_accuracy: 0.9615 - lr: 3.0000e-04\n",
      "Epoch 5/20\n",
      "706/706 [==============================] - 61s 87ms/step - loss: 0.1585 - accuracy: 0.9726 - val_loss: 0.1192 - val_accuracy: 0.9829 - lr: 3.0000e-04\n",
      "Epoch 6/20\n",
      "706/706 [==============================] - 61s 87ms/step - loss: 0.1507 - accuracy: 0.9760 - val_loss: 0.1338 - val_accuracy: 0.9872 - lr: 3.0000e-04\n",
      "Epoch 7/20\n",
      "706/706 [==============================] - 61s 87ms/step - loss: 0.1273 - accuracy: 0.9826 - val_loss: 0.1649 - val_accuracy: 0.9594 - lr: 3.0000e-04\n",
      "Epoch 8/20\n",
      "706/706 [==============================] - 61s 87ms/step - loss: 0.1229 - accuracy: 0.9837 - val_loss: 0.1917 - val_accuracy: 0.9658 - lr: 3.0000e-04\n",
      "Epoch 9/20\n",
      "706/706 [==============================] - 61s 87ms/step - loss: 0.1110 - accuracy: 0.9864 - val_loss: 0.1407 - val_accuracy: 0.9722 - lr: 3.0000e-04\n",
      "Epoch 10/20\n",
      "706/706 [==============================] - 60s 84ms/step - loss: 0.0970 - accuracy: 0.9912 - val_loss: 0.1208 - val_accuracy: 0.9786 - lr: 3.0000e-04\n",
      "Epoch 11/20\n",
      "706/706 [==============================] - 60s 85ms/step - loss: 0.1012 - accuracy: 0.9887 - val_loss: 0.0897 - val_accuracy: 0.9936 - lr: 3.0000e-04\n",
      "Epoch 12/20\n",
      "706/706 [==============================] - 60s 85ms/step - loss: 0.0860 - accuracy: 0.9921 - val_loss: 0.1119 - val_accuracy: 0.9765 - lr: 3.0000e-04\n",
      "Epoch 13/20\n",
      "706/706 [==============================] - 60s 85ms/step - loss: 0.0834 - accuracy: 0.9905 - val_loss: 0.1310 - val_accuracy: 0.9765 - lr: 3.0000e-04\n",
      "Epoch 14/20\n",
      "706/706 [==============================] - 60s 85ms/step - loss: 0.0820 - accuracy: 0.9908 - val_loss: 0.1518 - val_accuracy: 0.9722 - lr: 3.0000e-04\n",
      "Epoch 15/20\n",
      "706/706 [==============================] - 60s 85ms/step - loss: 0.0696 - accuracy: 0.9931 - val_loss: 0.5843 - val_accuracy: 0.8269 - lr: 3.0000e-04\n",
      "Epoch 16/20\n",
      "706/706 [==============================] - 60s 85ms/step - loss: 0.0681 - accuracy: 0.9942 - val_loss: 0.0617 - val_accuracy: 0.9936 - lr: 3.0000e-04\n",
      "Epoch 17/20\n",
      "706/706 [==============================] - 60s 85ms/step - loss: 0.0631 - accuracy: 0.9949 - val_loss: 0.0489 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 18/20\n",
      "706/706 [==============================] - 60s 85ms/step - loss: 0.0610 - accuracy: 0.9944 - val_loss: 0.0522 - val_accuracy: 0.9957 - lr: 3.0000e-04\n",
      "Epoch 19/20\n",
      "706/706 [==============================] - 60s 85ms/step - loss: 0.0554 - accuracy: 0.9958 - val_loss: 0.1047 - val_accuracy: 0.9786 - lr: 3.0000e-04\n",
      "Epoch 20/20\n",
      "706/706 [==============================] - 60s 85ms/step - loss: 0.0522 - accuracy: 0.9965 - val_loss: 0.0690 - val_accuracy: 0.9872 - lr: 3.0000e-04\n",
      "(467, 453) (467, 1)\n",
      "Epoch 1/20\n",
      "706/706 [==============================] - 63s 86ms/step - loss: 0.5295 - accuracy: 0.8221 - val_loss: 0.4114 - val_accuracy: 0.8908 - lr: 3.0000e-04\n",
      "Epoch 2/20\n",
      "706/706 [==============================] - 60s 85ms/step - loss: 0.3513 - accuracy: 0.8926 - val_loss: 0.2728 - val_accuracy: 0.9293 - lr: 3.0000e-04\n",
      "Epoch 3/20\n",
      "706/706 [==============================] - 60s 85ms/step - loss: 0.2404 - accuracy: 0.9412 - val_loss: 0.3317 - val_accuracy: 0.9251 - lr: 3.0000e-04\n",
      "Epoch 4/20\n",
      "706/706 [==============================] - 61s 86ms/step - loss: 0.1767 - accuracy: 0.9663 - val_loss: 0.0929 - val_accuracy: 0.9936 - lr: 3.0000e-04\n",
      "Epoch 5/20\n",
      "706/706 [==============================] - 60s 86ms/step - loss: 0.1598 - accuracy: 0.9714 - val_loss: 0.0917 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 6/20\n",
      "706/706 [==============================] - 60s 85ms/step - loss: 0.1383 - accuracy: 0.9794 - val_loss: 0.2291 - val_accuracy: 0.9443 - lr: 3.0000e-04\n",
      "Epoch 7/20\n",
      "706/706 [==============================] - 60s 85ms/step - loss: 0.1437 - accuracy: 0.9773 - val_loss: 0.0825 - val_accuracy: 0.9979 - lr: 3.0000e-04\n",
      "Epoch 8/20\n",
      "706/706 [==============================] - 60s 85ms/step - loss: 0.1351 - accuracy: 0.9788 - val_loss: 0.0728 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 9/20\n",
      "706/706 [==============================] - 60s 85ms/step - loss: 0.1011 - accuracy: 0.9901 - val_loss: 0.0782 - val_accuracy: 0.9979 - lr: 3.0000e-04\n",
      "Epoch 10/20\n",
      "706/706 [==============================] - 60s 85ms/step - loss: 0.0892 - accuracy: 0.9918 - val_loss: 0.0723 - val_accuracy: 0.9936 - lr: 3.0000e-04\n",
      "Epoch 11/20\n",
      "706/706 [==============================] - 60s 85ms/step - loss: 0.0892 - accuracy: 0.9919 - val_loss: 0.0645 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 12/20\n",
      "706/706 [==============================] - 60s 85ms/step - loss: 0.0812 - accuracy: 0.9927 - val_loss: 0.0573 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 13/20\n",
      "706/706 [==============================] - 61s 86ms/step - loss: 0.0787 - accuracy: 0.9926 - val_loss: 0.0648 - val_accuracy: 0.9979 - lr: 3.0000e-04\n",
      "Epoch 14/20\n",
      "706/706 [==============================] - 60s 85ms/step - loss: 0.0727 - accuracy: 0.9933 - val_loss: 0.0538 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 15/20\n",
      "706/706 [==============================] - 60s 85ms/step - loss: 0.0634 - accuracy: 0.9956 - val_loss: 0.0479 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 16/20\n",
      "706/706 [==============================] - 60s 85ms/step - loss: 0.0595 - accuracy: 0.9961 - val_loss: 0.0450 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 17/20\n",
      "706/706 [==============================] - 60s 85ms/step - loss: 0.0582 - accuracy: 0.9954 - val_loss: 0.0420 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 18/20\n",
      "706/706 [==============================] - 60s 86ms/step - loss: 0.0592 - accuracy: 0.9949 - val_loss: 0.0413 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 19/20\n",
      "706/706 [==============================] - 60s 85ms/step - loss: 0.0489 - accuracy: 0.9973 - val_loss: 0.0358 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 20/20\n",
      "706/706 [==============================] - 60s 85ms/step - loss: 0.0496 - accuracy: 0.9957 - val_loss: 0.0391 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "(290, 453) (290, 1)\n",
      "Epoch 1/20\n",
      "717/717 [==============================] - 65s 86ms/step - loss: 0.5002 - accuracy: 0.8331 - val_loss: 1.9537 - val_accuracy: 0.5379 - lr: 3.0000e-04\n",
      "Epoch 2/20\n",
      "717/717 [==============================] - 61s 85ms/step - loss: 0.3772 - accuracy: 0.8895 - val_loss: 1.4153 - val_accuracy: 0.7241 - lr: 3.0000e-04\n",
      "Epoch 3/20\n",
      "717/717 [==============================] - 61s 85ms/step - loss: 0.2809 - accuracy: 0.9306 - val_loss: 0.4023 - val_accuracy: 0.8552 - lr: 3.0000e-04\n",
      "Epoch 4/20\n",
      "717/717 [==============================] - 61s 85ms/step - loss: 0.2139 - accuracy: 0.9553 - val_loss: 0.1049 - val_accuracy: 0.9966 - lr: 3.0000e-04\n",
      "Epoch 5/20\n",
      "717/717 [==============================] - 61s 85ms/step - loss: 0.1662 - accuracy: 0.9709 - val_loss: 0.1111 - val_accuracy: 0.9966 - lr: 3.0000e-04\n",
      "Epoch 6/20\n",
      "717/717 [==============================] - 61s 85ms/step - loss: 0.1634 - accuracy: 0.9704 - val_loss: 0.1456 - val_accuracy: 0.9724 - lr: 3.0000e-04\n",
      "Epoch 7/20\n",
      "717/717 [==============================] - 62s 87ms/step - loss: 0.1259 - accuracy: 0.9855 - val_loss: 0.0965 - val_accuracy: 0.9931 - lr: 3.0000e-04\n",
      "Epoch 8/20\n",
      "717/717 [==============================] - 61s 86ms/step - loss: 0.1095 - accuracy: 0.9888 - val_loss: 0.0789 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 9/20\n",
      "717/717 [==============================] - 63s 87ms/step - loss: 0.1103 - accuracy: 0.9883 - val_loss: 0.0917 - val_accuracy: 0.9931 - lr: 3.0000e-04\n",
      "Epoch 10/20\n",
      "717/717 [==============================] - 62s 87ms/step - loss: 0.0895 - accuracy: 0.9937 - val_loss: 0.0665 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 11/20\n",
      "717/717 [==============================] - 62s 87ms/step - loss: 0.0870 - accuracy: 0.9935 - val_loss: 0.0641 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 12/20\n",
      "717/717 [==============================] - 60s 83ms/step - loss: 0.0803 - accuracy: 0.9942 - val_loss: 0.0636 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 13/20\n",
      "717/717 [==============================] - 60s 83ms/step - loss: 0.0739 - accuracy: 0.9950 - val_loss: 0.0757 - val_accuracy: 0.9966 - lr: 3.0000e-04\n",
      "Epoch 14/20\n",
      "717/717 [==============================] - 60s 83ms/step - loss: 0.0818 - accuracy: 0.9909 - val_loss: 0.0546 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 15/20\n",
      "717/717 [==============================] - 60s 84ms/step - loss: 0.0844 - accuracy: 0.9908 - val_loss: 0.0543 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 16/20\n",
      "717/717 [==============================] - 60s 84ms/step - loss: 0.0626 - accuracy: 0.9969 - val_loss: 0.0626 - val_accuracy: 0.9966 - lr: 3.0000e-04\n",
      "Epoch 17/20\n",
      "717/717 [==============================] - 60s 84ms/step - loss: 0.0711 - accuracy: 0.9935 - val_loss: 0.0488 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 18/20\n",
      "717/717 [==============================] - 60s 84ms/step - loss: 0.0582 - accuracy: 0.9969 - val_loss: 0.1004 - val_accuracy: 0.9828 - lr: 3.0000e-04\n",
      "Epoch 19/20\n",
      "717/717 [==============================] - 59s 83ms/step - loss: 0.0594 - accuracy: 0.9962 - val_loss: 0.0477 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 20/20\n",
      "717/717 [==============================] - 60s 84ms/step - loss: 0.0507 - accuracy: 0.9977 - val_loss: 0.0894 - val_accuracy: 0.9897 - lr: 3.0000e-04\n",
      "(266, 453) (266, 1)\n",
      "Epoch 1/20\n",
      "719/719 [==============================] - 63s 84ms/step - loss: 0.5172 - accuracy: 0.8247 - val_loss: 0.6285 - val_accuracy: 0.7406 - lr: 3.0000e-04\n",
      "Epoch 2/20\n",
      "719/719 [==============================] - 60s 84ms/step - loss: 0.3781 - accuracy: 0.8854 - val_loss: 0.3325 - val_accuracy: 0.9173 - lr: 3.0000e-04\n",
      "Epoch 3/20\n",
      "719/719 [==============================] - 62s 86ms/step - loss: 0.2851 - accuracy: 0.9237 - val_loss: 0.6031 - val_accuracy: 0.8120 - lr: 3.0000e-04\n",
      "Epoch 4/20\n",
      "719/719 [==============================] - 63s 87ms/step - loss: 0.1875 - accuracy: 0.9601 - val_loss: 0.1264 - val_accuracy: 0.9737 - lr: 3.0000e-04\n",
      "Epoch 5/20\n",
      "719/719 [==============================] - 62s 87ms/step - loss: 0.1282 - accuracy: 0.9813 - val_loss: 0.2187 - val_accuracy: 0.9436 - lr: 3.0000e-04\n",
      "Epoch 6/20\n",
      "719/719 [==============================] - 62s 87ms/step - loss: 0.1086 - accuracy: 0.9893 - val_loss: 0.2017 - val_accuracy: 0.9436 - lr: 3.0000e-04\n",
      "Epoch 7/20\n",
      "719/719 [==============================] - 62s 87ms/step - loss: 0.1062 - accuracy: 0.9893 - val_loss: 0.1198 - val_accuracy: 0.9850 - lr: 3.0000e-04\n",
      "Epoch 8/20\n",
      "719/719 [==============================] - 61s 85ms/step - loss: 0.1015 - accuracy: 0.9904 - val_loss: 0.0780 - val_accuracy: 0.9962 - lr: 3.0000e-04\n",
      "Epoch 9/20\n",
      "719/719 [==============================] - 60s 84ms/step - loss: 0.0843 - accuracy: 0.9941 - val_loss: 0.0879 - val_accuracy: 0.9887 - lr: 3.0000e-04\n",
      "Epoch 10/20\n",
      "719/719 [==============================] - 60s 84ms/step - loss: 0.0817 - accuracy: 0.9929 - val_loss: 0.1583 - val_accuracy: 0.9662 - lr: 3.0000e-04\n",
      "Epoch 11/20\n",
      "719/719 [==============================] - 60s 84ms/step - loss: 0.0704 - accuracy: 0.9952 - val_loss: 0.1180 - val_accuracy: 0.9850 - lr: 3.0000e-04\n",
      "Epoch 12/20\n",
      "719/719 [==============================] - 60s 84ms/step - loss: 0.0715 - accuracy: 0.9939 - val_loss: 0.0518 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 13/20\n",
      "719/719 [==============================] - 61s 84ms/step - loss: 0.0591 - accuracy: 0.9968 - val_loss: 0.1066 - val_accuracy: 0.9737 - lr: 3.0000e-04\n",
      "Epoch 14/20\n",
      "719/719 [==============================] - 60s 84ms/step - loss: 0.0528 - accuracy: 0.9966 - val_loss: 0.0517 - val_accuracy: 0.9962 - lr: 3.0000e-04\n",
      "Epoch 15/20\n",
      "719/719 [==============================] - 60s 84ms/step - loss: 0.0481 - accuracy: 0.9975 - val_loss: 0.0403 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 16/20\n",
      "719/719 [==============================] - 60s 84ms/step - loss: 0.0493 - accuracy: 0.9968 - val_loss: 0.1021 - val_accuracy: 0.9737 - lr: 3.0000e-04\n",
      "Epoch 17/20\n",
      "719/719 [==============================] - 59s 82ms/step - loss: 0.0552 - accuracy: 0.9958 - val_loss: 0.0406 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 18/20\n",
      "719/719 [==============================] - 59s 82ms/step - loss: 0.0435 - accuracy: 0.9977 - val_loss: 0.2202 - val_accuracy: 0.9586 - lr: 3.0000e-04\n",
      "Epoch 19/20\n",
      "719/719 [==============================] - 60s 84ms/step - loss: 0.0465 - accuracy: 0.9970 - val_loss: 0.0389 - val_accuracy: 0.9962 - lr: 3.0000e-04\n",
      "Epoch 20/20\n",
      "719/719 [==============================] - 60s 84ms/step - loss: 0.0381 - accuracy: 0.9977 - val_loss: 0.0320 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "(461, 453) (461, 1)\n",
      "Epoch 1/20\n",
      "706/706 [==============================] - 63s 85ms/step - loss: 0.5227 - accuracy: 0.8227 - val_loss: 0.5861 - val_accuracy: 0.8200 - lr: 3.0000e-04\n",
      "Epoch 2/20\n",
      "706/706 [==============================] - 60s 85ms/step - loss: 0.3795 - accuracy: 0.8817 - val_loss: 0.4485 - val_accuracy: 0.8807 - lr: 3.0000e-04\n",
      "Epoch 3/20\n",
      "706/706 [==============================] - 60s 84ms/step - loss: 0.2292 - accuracy: 0.9431 - val_loss: 0.3168 - val_accuracy: 0.9154 - lr: 3.0000e-04\n",
      "Epoch 4/20\n",
      "706/706 [==============================] - 60s 85ms/step - loss: 0.2021 - accuracy: 0.9556 - val_loss: 0.2103 - val_accuracy: 0.9436 - lr: 3.0000e-04\n",
      "Epoch 5/20\n",
      "706/706 [==============================] - 58s 83ms/step - loss: 0.1441 - accuracy: 0.9801 - val_loss: 0.1012 - val_accuracy: 0.9978 - lr: 3.0000e-04\n",
      "Epoch 6/20\n",
      "706/706 [==============================] - 60s 85ms/step - loss: 0.1133 - accuracy: 0.9899 - val_loss: 0.1438 - val_accuracy: 0.9740 - lr: 3.0000e-04\n",
      "Epoch 7/20\n",
      "706/706 [==============================] - 59s 84ms/step - loss: 0.1028 - accuracy: 0.9915 - val_loss: 0.0898 - val_accuracy: 0.9935 - lr: 3.0000e-04\n",
      "Epoch 8/20\n",
      "706/706 [==============================] - 60s 84ms/step - loss: 0.0915 - accuracy: 0.9938 - val_loss: 0.1089 - val_accuracy: 0.9870 - lr: 3.0000e-04\n",
      "Epoch 9/20\n",
      "706/706 [==============================] - 59s 84ms/step - loss: 0.0857 - accuracy: 0.9946 - val_loss: 0.0912 - val_accuracy: 0.9892 - lr: 3.0000e-04\n",
      "Epoch 10/20\n",
      "706/706 [==============================] - 60s 85ms/step - loss: 0.0856 - accuracy: 0.9928 - val_loss: 0.1039 - val_accuracy: 0.9826 - lr: 3.0000e-04\n",
      "Epoch 11/20\n",
      "706/706 [==============================] - 60s 85ms/step - loss: 0.0787 - accuracy: 0.9934 - val_loss: 0.2873 - val_accuracy: 0.9653 - lr: 3.0000e-04\n",
      "Epoch 12/20\n",
      "706/706 [==============================] - 60s 84ms/step - loss: 0.0729 - accuracy: 0.9950 - val_loss: 0.0890 - val_accuracy: 0.9826 - lr: 3.0000e-04\n",
      "Epoch 13/20\n",
      "706/706 [==============================] - 60s 85ms/step - loss: 0.0554 - accuracy: 0.9988 - val_loss: 0.1175 - val_accuracy: 0.9718 - lr: 3.0000e-04\n",
      "Epoch 14/20\n",
      "706/706 [==============================] - 59s 84ms/step - loss: 0.0672 - accuracy: 0.9950 - val_loss: 0.1741 - val_accuracy: 0.9653 - lr: 3.0000e-04\n",
      "Epoch 15/20\n",
      "706/706 [==============================] - 60s 85ms/step - loss: 0.0566 - accuracy: 0.9958 - val_loss: 0.0961 - val_accuracy: 0.9826 - lr: 3.0000e-04\n",
      "Epoch 16/20\n",
      "706/706 [==============================] - 59s 83ms/step - loss: 0.0526 - accuracy: 0.9970 - val_loss: 0.0516 - val_accuracy: 0.9957 - lr: 3.0000e-04\n",
      "Epoch 17/20\n",
      "706/706 [==============================] - 60s 84ms/step - loss: 0.0427 - accuracy: 0.9988 - val_loss: 0.0416 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 18/20\n",
      "706/706 [==============================] - 60s 85ms/step - loss: 0.0554 - accuracy: 0.9951 - val_loss: 0.0517 - val_accuracy: 0.9957 - lr: 3.0000e-04\n",
      "Epoch 19/20\n",
      "706/706 [==============================] - 59s 83ms/step - loss: 0.0460 - accuracy: 0.9973 - val_loss: 0.0364 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 20/20\n",
      "706/706 [==============================] - 59s 83ms/step - loss: 0.0472 - accuracy: 0.9965 - val_loss: 0.0349 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "(280, 453) (280, 1)\n",
      "Epoch 1/20\n",
      "718/718 [==============================] - 63s 84ms/step - loss: 0.5164 - accuracy: 0.8253 - val_loss: 0.5353 - val_accuracy: 0.7607 - lr: 3.0000e-04\n",
      "Epoch 2/20\n",
      "718/718 [==============================] - 60s 83ms/step - loss: 0.3825 - accuracy: 0.8837 - val_loss: 0.3510 - val_accuracy: 0.9321 - lr: 3.0000e-04\n",
      "Epoch 3/20\n",
      "718/718 [==============================] - 60s 84ms/step - loss: 0.3656 - accuracy: 0.8850 - val_loss: 0.3336 - val_accuracy: 0.8964 - lr: 3.0000e-04\n",
      "Epoch 4/20\n",
      "718/718 [==============================] - 60s 84ms/step - loss: 0.2011 - accuracy: 0.9545 - val_loss: 0.1993 - val_accuracy: 0.9679 - lr: 3.0000e-04\n",
      "Epoch 5/20\n",
      "718/718 [==============================] - 60s 84ms/step - loss: 0.1435 - accuracy: 0.9780 - val_loss: 0.1227 - val_accuracy: 0.9964 - lr: 3.0000e-04\n",
      "Epoch 6/20\n",
      "718/718 [==============================] - 60s 84ms/step - loss: 0.1134 - accuracy: 0.9866 - val_loss: 0.1343 - val_accuracy: 0.9857 - lr: 3.0000e-04\n",
      "Epoch 7/20\n",
      "718/718 [==============================] - 60s 83ms/step - loss: 0.1034 - accuracy: 0.9897 - val_loss: 0.1309 - val_accuracy: 0.9929 - lr: 3.0000e-04\n",
      "Epoch 8/20\n",
      "718/718 [==============================] - 60s 84ms/step - loss: 0.1065 - accuracy: 0.9873 - val_loss: 0.1074 - val_accuracy: 0.9964 - lr: 3.0000e-04\n",
      "Epoch 9/20\n",
      "718/718 [==============================] - 60s 84ms/step - loss: 0.0907 - accuracy: 0.9918 - val_loss: 0.1041 - val_accuracy: 0.9964 - lr: 3.0000e-04\n",
      "Epoch 10/20\n",
      "718/718 [==============================] - 60s 84ms/step - loss: 0.0816 - accuracy: 0.9925 - val_loss: 0.0888 - val_accuracy: 0.9964 - lr: 3.0000e-04\n",
      "Epoch 11/20\n",
      "718/718 [==============================] - 60s 84ms/step - loss: 0.0705 - accuracy: 0.9949 - val_loss: 0.1083 - val_accuracy: 0.9929 - lr: 3.0000e-04\n",
      "Epoch 12/20\n",
      "718/718 [==============================] - 58s 81ms/step - loss: 0.0742 - accuracy: 0.9922 - val_loss: 0.0966 - val_accuracy: 0.9964 - lr: 3.0000e-04\n",
      "Epoch 13/20\n",
      "718/718 [==============================] - 58s 81ms/step - loss: 0.0658 - accuracy: 0.9956 - val_loss: 0.1019 - val_accuracy: 0.9964 - lr: 3.0000e-04\n",
      "Epoch 14/20\n",
      "718/718 [==============================] - 58s 81ms/step - loss: 0.0607 - accuracy: 0.9948 - val_loss: 0.0883 - val_accuracy: 0.9929 - lr: 3.0000e-04\n",
      "Epoch 15/20\n",
      "718/718 [==============================] - 59s 82ms/step - loss: 0.0541 - accuracy: 0.9958 - val_loss: 0.0830 - val_accuracy: 0.9964 - lr: 3.0000e-04\n",
      "Epoch 16/20\n",
      "718/718 [==============================] - 60s 84ms/step - loss: 0.0504 - accuracy: 0.9970 - val_loss: 0.0715 - val_accuracy: 0.9964 - lr: 3.0000e-04\n",
      "Epoch 17/20\n",
      "718/718 [==============================] - 60s 84ms/step - loss: 0.0475 - accuracy: 0.9962 - val_loss: 0.0783 - val_accuracy: 0.9964 - lr: 3.0000e-04\n",
      "Epoch 18/20\n",
      "718/718 [==============================] - 60s 83ms/step - loss: 0.0470 - accuracy: 0.9963 - val_loss: 0.0762 - val_accuracy: 0.9964 - lr: 3.0000e-04\n",
      "Epoch 19/20\n",
      "718/718 [==============================] - 59s 83ms/step - loss: 0.0488 - accuracy: 0.9957 - val_loss: 0.0735 - val_accuracy: 0.9964 - lr: 3.0000e-04\n",
      "Epoch 20/20\n",
      "718/718 [==============================] - 60s 84ms/step - loss: 0.0460 - accuracy: 0.9968 - val_loss: 0.0724 - val_accuracy: 0.9964 - lr: 3.0000e-04\n",
      "(484, 453) (484, 1)\n",
      "Epoch 1/20\n",
      "705/705 [==============================] - 63s 85ms/step - loss: 0.5203 - accuracy: 0.8254 - val_loss: 0.5294 - val_accuracy: 0.8161 - lr: 3.0000e-04\n",
      "Epoch 2/20\n",
      "705/705 [==============================] - 60s 85ms/step - loss: 0.3557 - accuracy: 0.8934 - val_loss: 0.3259 - val_accuracy: 0.9277 - lr: 3.0000e-04\n",
      "Epoch 3/20\n",
      "705/705 [==============================] - 59s 84ms/step - loss: 0.2543 - accuracy: 0.9326 - val_loss: 0.2304 - val_accuracy: 0.9442 - lr: 3.0000e-04\n",
      "Epoch 4/20\n",
      "705/705 [==============================] - 60s 85ms/step - loss: 0.1988 - accuracy: 0.9578 - val_loss: 0.1547 - val_accuracy: 0.9587 - lr: 3.0000e-04\n",
      "Epoch 5/20\n",
      "705/705 [==============================] - 60s 85ms/step - loss: 0.1539 - accuracy: 0.9743 - val_loss: 0.1549 - val_accuracy: 0.9649 - lr: 3.0000e-04\n",
      "Epoch 6/20\n",
      "705/705 [==============================] - 59s 84ms/step - loss: 0.1355 - accuracy: 0.9815 - val_loss: 0.1228 - val_accuracy: 0.9814 - lr: 3.0000e-04\n",
      "Epoch 7/20\n",
      "705/705 [==============================] - 60s 85ms/step - loss: 0.1143 - accuracy: 0.9872 - val_loss: 0.1248 - val_accuracy: 0.9814 - lr: 3.0000e-04\n",
      "Epoch 8/20\n",
      "705/705 [==============================] - 60s 85ms/step - loss: 0.1073 - accuracy: 0.9890 - val_loss: 0.1166 - val_accuracy: 0.9876 - lr: 3.0000e-04\n",
      "Epoch 9/20\n",
      "705/705 [==============================] - 60s 85ms/step - loss: 0.1004 - accuracy: 0.9903 - val_loss: 0.1096 - val_accuracy: 0.9897 - lr: 3.0000e-04\n",
      "Epoch 10/20\n",
      "705/705 [==============================] - 60s 84ms/step - loss: 0.0909 - accuracy: 0.9913 - val_loss: 0.1726 - val_accuracy: 0.9814 - lr: 3.0000e-04\n",
      "Epoch 11/20\n",
      "705/705 [==============================] - 59s 84ms/step - loss: 0.0749 - accuracy: 0.9950 - val_loss: 0.1089 - val_accuracy: 0.9835 - lr: 3.0000e-04\n",
      "Epoch 12/20\n",
      "705/705 [==============================] - 60s 85ms/step - loss: 0.0715 - accuracy: 0.9945 - val_loss: 0.0901 - val_accuracy: 0.9876 - lr: 3.0000e-04\n",
      "Epoch 13/20\n",
      "705/705 [==============================] - 60s 85ms/step - loss: 0.0690 - accuracy: 0.9946 - val_loss: 0.0702 - val_accuracy: 0.9938 - lr: 3.0000e-04\n",
      "Epoch 14/20\n",
      "705/705 [==============================] - 60s 85ms/step - loss: 0.0617 - accuracy: 0.9957 - val_loss: 0.1120 - val_accuracy: 0.9897 - lr: 3.0000e-04\n",
      "Epoch 15/20\n",
      "705/705 [==============================] - 60s 85ms/step - loss: 0.0659 - accuracy: 0.9931 - val_loss: 0.0651 - val_accuracy: 0.9917 - lr: 3.0000e-04\n",
      "Epoch 16/20\n",
      "705/705 [==============================] - 60s 85ms/step - loss: 0.0510 - accuracy: 0.9972 - val_loss: 0.0757 - val_accuracy: 0.9917 - lr: 3.0000e-04\n",
      "Epoch 17/20\n",
      "705/705 [==============================] - 59s 84ms/step - loss: 0.0578 - accuracy: 0.9942 - val_loss: 0.0987 - val_accuracy: 0.9876 - lr: 3.0000e-04\n",
      "Epoch 18/20\n",
      "705/705 [==============================] - 60s 85ms/step - loss: 0.0494 - accuracy: 0.9962 - val_loss: 0.0522 - val_accuracy: 0.9959 - lr: 3.0000e-04\n",
      "Epoch 19/20\n",
      "705/705 [==============================] - 60s 85ms/step - loss: 0.0481 - accuracy: 0.9966 - val_loss: 0.1031 - val_accuracy: 0.9835 - lr: 3.0000e-04\n",
      "Epoch 20/20\n",
      "705/705 [==============================] - 60s 85ms/step - loss: 0.0460 - accuracy: 0.9975 - val_loss: 0.0750 - val_accuracy: 0.9897 - lr: 3.0000e-04\n",
      "(326, 453) (326, 1)\n",
      "Epoch 1/20\n",
      "715/715 [==============================] - 63s 85ms/step - loss: 0.5171 - accuracy: 0.8248 - val_loss: 0.4207 - val_accuracy: 0.8374 - lr: 3.0000e-04\n",
      "Epoch 2/20\n",
      "715/715 [==============================] - 60s 84ms/step - loss: 0.4041 - accuracy: 0.8744 - val_loss: 0.5637 - val_accuracy: 0.7975 - lr: 3.0000e-04\n",
      "Epoch 3/20\n",
      "715/715 [==============================] - 60s 84ms/step - loss: 0.3495 - accuracy: 0.8944 - val_loss: 0.2611 - val_accuracy: 0.8957 - lr: 3.0000e-04\n",
      "Epoch 4/20\n",
      "715/715 [==============================] - 61s 85ms/step - loss: 0.2688 - accuracy: 0.9265 - val_loss: 0.1874 - val_accuracy: 0.9724 - lr: 3.0000e-04\n",
      "Epoch 5/20\n",
      "715/715 [==============================] - 60s 83ms/step - loss: 0.1848 - accuracy: 0.9602 - val_loss: 0.2039 - val_accuracy: 0.9417 - lr: 3.0000e-04\n",
      "Epoch 6/20\n",
      "715/715 [==============================] - 58s 81ms/step - loss: 0.1419 - accuracy: 0.9746 - val_loss: 0.2366 - val_accuracy: 0.9479 - lr: 3.0000e-04\n",
      "Epoch 7/20\n",
      "715/715 [==============================] - 60s 84ms/step - loss: 0.1203 - accuracy: 0.9843 - val_loss: 0.2421 - val_accuracy: 0.9417 - lr: 3.0000e-04\n",
      "Epoch 8/20\n",
      "715/715 [==============================] - 60s 84ms/step - loss: 0.1207 - accuracy: 0.9836 - val_loss: 0.1027 - val_accuracy: 0.9847 - lr: 3.0000e-04\n",
      "Epoch 9/20\n",
      "715/715 [==============================] - 60s 84ms/step - loss: 0.1086 - accuracy: 0.9858 - val_loss: 0.1645 - val_accuracy: 0.9663 - lr: 3.0000e-04\n",
      "Epoch 10/20\n",
      "715/715 [==============================] - 60s 84ms/step - loss: 0.1029 - accuracy: 0.9886 - val_loss: 0.1470 - val_accuracy: 0.9693 - lr: 3.0000e-04\n",
      "Epoch 11/20\n",
      "715/715 [==============================] - 60s 85ms/step - loss: 0.0924 - accuracy: 0.9888 - val_loss: 0.2148 - val_accuracy: 0.9509 - lr: 3.0000e-04\n",
      "Epoch 12/20\n",
      "715/715 [==============================] - 60s 83ms/step - loss: 0.0870 - accuracy: 0.9912 - val_loss: 0.1454 - val_accuracy: 0.9632 - lr: 3.0000e-04\n",
      "Epoch 13/20\n",
      "715/715 [==============================] - 61s 86ms/step - loss: 0.0729 - accuracy: 0.9935 - val_loss: 0.0699 - val_accuracy: 0.9969 - lr: 3.0000e-04\n",
      "Epoch 14/20\n",
      "715/715 [==============================] - 63s 88ms/step - loss: 0.0688 - accuracy: 0.9944 - val_loss: 0.1079 - val_accuracy: 0.9755 - lr: 3.0000e-04\n",
      "Epoch 15/20\n",
      "715/715 [==============================] - 62s 87ms/step - loss: 0.0668 - accuracy: 0.9929 - val_loss: 0.0604 - val_accuracy: 0.9969 - lr: 3.0000e-04\n",
      "Epoch 16/20\n",
      "715/715 [==============================] - 61s 86ms/step - loss: 0.0613 - accuracy: 0.9933 - val_loss: 0.0772 - val_accuracy: 0.9908 - lr: 3.0000e-04\n",
      "Epoch 17/20\n",
      "715/715 [==============================] - 62s 87ms/step - loss: 0.0582 - accuracy: 0.9942 - val_loss: 0.2241 - val_accuracy: 0.9110 - lr: 3.0000e-04\n",
      "Epoch 18/20\n",
      "715/715 [==============================] - 62s 87ms/step - loss: 0.0625 - accuracy: 0.9934 - val_loss: 0.1557 - val_accuracy: 0.9632 - lr: 3.0000e-04\n",
      "Epoch 19/20\n",
      "715/715 [==============================] - 60s 84ms/step - loss: 0.0511 - accuracy: 0.9969 - val_loss: 0.0735 - val_accuracy: 0.9847 - lr: 3.0000e-04\n",
      "Epoch 20/20\n",
      "715/715 [==============================] - 60s 84ms/step - loss: 0.0533 - accuracy: 0.9954 - val_loss: 0.0456 - val_accuracy: 0.9969 - lr: 3.0000e-04\n",
      "(611, 453) (611, 1)\n",
      "Epoch 1/20\n",
      "697/697 [==============================] - 62s 86ms/step - loss: 0.5207 - accuracy: 0.8286 - val_loss: 0.4795 - val_accuracy: 0.8511 - lr: 3.0000e-04\n",
      "Epoch 2/20\n",
      "697/697 [==============================] - 59s 85ms/step - loss: 0.3913 - accuracy: 0.8797 - val_loss: 0.4244 - val_accuracy: 0.8871 - lr: 3.0000e-04\n",
      "Epoch 3/20\n",
      "697/697 [==============================] - 59s 85ms/step - loss: 0.2455 - accuracy: 0.9397 - val_loss: 0.2927 - val_accuracy: 0.9214 - lr: 3.0000e-04\n",
      "Epoch 4/20\n",
      "697/697 [==============================] - 59s 85ms/step - loss: 0.2296 - accuracy: 0.9422 - val_loss: 0.1867 - val_accuracy: 0.9640 - lr: 3.0000e-04\n",
      "Epoch 5/20\n",
      "697/697 [==============================] - 59s 85ms/step - loss: 0.1384 - accuracy: 0.9803 - val_loss: 0.1542 - val_accuracy: 0.9804 - lr: 3.0000e-04\n",
      "Epoch 6/20\n",
      "697/697 [==============================] - 58s 84ms/step - loss: 0.1291 - accuracy: 0.9814 - val_loss: 0.1671 - val_accuracy: 0.9673 - lr: 3.0000e-04\n",
      "Epoch 7/20\n",
      "697/697 [==============================] - 58s 83ms/step - loss: 0.1162 - accuracy: 0.9862 - val_loss: 0.1059 - val_accuracy: 0.9918 - lr: 3.0000e-04\n",
      "Epoch 8/20\n",
      "697/697 [==============================] - 57s 82ms/step - loss: 0.1046 - accuracy: 0.9891 - val_loss: 0.1561 - val_accuracy: 0.9689 - lr: 3.0000e-04\n",
      "Epoch 9/20\n",
      "697/697 [==============================] - 59s 84ms/step - loss: 0.1441 - accuracy: 0.9700 - val_loss: 0.0915 - val_accuracy: 0.9918 - lr: 3.0000e-04\n",
      "Epoch 10/20\n",
      "697/697 [==============================] - 59s 85ms/step - loss: 0.0866 - accuracy: 0.9942 - val_loss: 0.1366 - val_accuracy: 0.9820 - lr: 3.0000e-04\n",
      "Epoch 11/20\n",
      "697/697 [==============================] - 59s 85ms/step - loss: 0.0766 - accuracy: 0.9954 - val_loss: 0.1195 - val_accuracy: 0.9804 - lr: 3.0000e-04\n",
      "Epoch 12/20\n",
      "697/697 [==============================] - 59s 85ms/step - loss: 0.0750 - accuracy: 0.9946 - val_loss: 0.1279 - val_accuracy: 0.9755 - lr: 3.0000e-04\n",
      "Epoch 13/20\n",
      "697/697 [==============================] - 60s 86ms/step - loss: 0.0653 - accuracy: 0.9968 - val_loss: 0.3433 - val_accuracy: 0.9313 - lr: 3.0000e-04\n",
      "Epoch 14/20\n",
      "697/697 [==============================] - 58s 83ms/step - loss: 0.0667 - accuracy: 0.9951 - val_loss: 0.0671 - val_accuracy: 0.9902 - lr: 3.0000e-04\n",
      "Epoch 15/20\n",
      "697/697 [==============================] - 59s 85ms/step - loss: 0.0668 - accuracy: 0.9938 - val_loss: 0.1416 - val_accuracy: 0.9689 - lr: 3.0000e-04\n",
      "Epoch 16/20\n",
      "697/697 [==============================] - 59s 85ms/step - loss: 0.0576 - accuracy: 0.9961 - val_loss: 0.0982 - val_accuracy: 0.9804 - lr: 3.0000e-04\n",
      "Epoch 17/20\n",
      "697/697 [==============================] - 59s 85ms/step - loss: 0.0527 - accuracy: 0.9963 - val_loss: 0.2404 - val_accuracy: 0.9525 - lr: 3.0000e-04\n",
      "Epoch 18/20\n",
      "697/697 [==============================] - 59s 85ms/step - loss: 0.0517 - accuracy: 0.9961 - val_loss: 0.1287 - val_accuracy: 0.9689 - lr: 3.0000e-04\n",
      "Epoch 19/20\n",
      "697/697 [==============================] - 59s 85ms/step - loss: 0.0451 - accuracy: 0.9977 - val_loss: 0.1035 - val_accuracy: 0.9738 - lr: 3.0000e-04\n",
      "Epoch 20/20\n",
      "697/697 [==============================] - 59s 85ms/step - loss: 0.0447 - accuracy: 0.9978 - val_loss: 0.1082 - val_accuracy: 0.9820 - lr: 3.0000e-04\n",
      "(405, 453) (405, 1)\n",
      "Epoch 1/20\n",
      "710/710 [==============================] - 62s 83ms/step - loss: 0.5263 - accuracy: 0.8242 - val_loss: 0.4105 - val_accuracy: 0.8543 - lr: 3.0000e-04\n",
      "Epoch 2/20\n",
      "710/710 [==============================] - 60s 85ms/step - loss: 0.3614 - accuracy: 0.8917 - val_loss: 0.2058 - val_accuracy: 0.9654 - lr: 3.0000e-04\n",
      "Epoch 3/20\n",
      "710/710 [==============================] - 60s 84ms/step - loss: 0.2468 - accuracy: 0.9387 - val_loss: 0.1933 - val_accuracy: 0.9654 - lr: 3.0000e-04\n",
      "Epoch 4/20\n",
      "710/710 [==============================] - 60s 85ms/step - loss: 0.1600 - accuracy: 0.9705 - val_loss: 0.1560 - val_accuracy: 0.9753 - lr: 3.0000e-04\n",
      "Epoch 5/20\n",
      "710/710 [==============================] - 60s 84ms/step - loss: 0.1298 - accuracy: 0.9818 - val_loss: 0.1358 - val_accuracy: 0.9778 - lr: 3.0000e-04\n",
      "Epoch 6/20\n",
      "710/710 [==============================] - 60s 85ms/step - loss: 0.1123 - accuracy: 0.9873 - val_loss: 0.1744 - val_accuracy: 0.9728 - lr: 3.0000e-04\n",
      "Epoch 7/20\n",
      "710/710 [==============================] - 60s 84ms/step - loss: 0.0979 - accuracy: 0.9918 - val_loss: 0.1202 - val_accuracy: 0.9877 - lr: 3.0000e-04\n",
      "Epoch 8/20\n",
      "710/710 [==============================] - 60s 85ms/step - loss: 0.0978 - accuracy: 0.9897 - val_loss: 0.1144 - val_accuracy: 0.9877 - lr: 3.0000e-04\n",
      "Epoch 9/20\n",
      "710/710 [==============================] - 60s 85ms/step - loss: 0.1233 - accuracy: 0.9813 - val_loss: 0.1211 - val_accuracy: 0.9877 - lr: 3.0000e-04\n",
      "Epoch 10/20\n",
      "710/710 [==============================] - 60s 84ms/step - loss: 0.1028 - accuracy: 0.9872 - val_loss: 0.0945 - val_accuracy: 0.9901 - lr: 3.0000e-04\n",
      "Epoch 11/20\n",
      "710/710 [==============================] - 60s 84ms/step - loss: 0.0901 - accuracy: 0.9910 - val_loss: 0.1311 - val_accuracy: 0.9852 - lr: 3.0000e-04\n",
      "Epoch 12/20\n",
      "710/710 [==============================] - 60s 84ms/step - loss: 0.0824 - accuracy: 0.9927 - val_loss: 0.1455 - val_accuracy: 0.9852 - lr: 3.0000e-04\n",
      "Epoch 13/20\n",
      "710/710 [==============================] - 60s 85ms/step - loss: 0.0776 - accuracy: 0.9932 - val_loss: 0.0992 - val_accuracy: 0.9901 - lr: 3.0000e-04\n",
      "Epoch 14/20\n",
      "710/710 [==============================] - 60s 85ms/step - loss: 0.0679 - accuracy: 0.9949 - val_loss: 0.1741 - val_accuracy: 0.9679 - lr: 3.0000e-04\n",
      "Epoch 15/20\n",
      "710/710 [==============================] - 60s 84ms/step - loss: 0.0786 - accuracy: 0.9916 - val_loss: 0.0815 - val_accuracy: 0.9926 - lr: 3.0000e-04\n",
      "Epoch 16/20\n",
      "710/710 [==============================] - 60s 84ms/step - loss: 0.0603 - accuracy: 0.9960 - val_loss: 0.0856 - val_accuracy: 0.9926 - lr: 3.0000e-04\n",
      "Epoch 17/20\n",
      "710/710 [==============================] - 60s 84ms/step - loss: 0.0600 - accuracy: 0.9960 - val_loss: 0.1156 - val_accuracy: 0.9901 - lr: 3.0000e-04\n",
      "Epoch 18/20\n",
      "710/710 [==============================] - 60s 84ms/step - loss: 0.0660 - accuracy: 0.9930 - val_loss: 0.1072 - val_accuracy: 0.9852 - lr: 3.0000e-04\n",
      "Epoch 19/20\n",
      "710/710 [==============================] - 59s 84ms/step - loss: 0.0533 - accuracy: 0.9967 - val_loss: 0.0910 - val_accuracy: 0.9852 - lr: 3.0000e-04\n",
      "Epoch 20/20\n",
      "710/710 [==============================] - 60s 84ms/step - loss: 0.0496 - accuracy: 0.9972 - val_loss: 0.0940 - val_accuracy: 0.9901 - lr: 3.0000e-04\n",
      "(336, 453) (336, 1)\n",
      "Epoch 1/20\n",
      "714/714 [==============================] - 62s 84ms/step - loss: 0.5160 - accuracy: 0.8255 - val_loss: 0.3852 - val_accuracy: 0.8988 - lr: 3.0000e-04\n",
      "Epoch 2/20\n",
      "714/714 [==============================] - 60s 84ms/step - loss: 0.3572 - accuracy: 0.8928 - val_loss: 0.4174 - val_accuracy: 0.8661 - lr: 3.0000e-04\n",
      "Epoch 3/20\n",
      "714/714 [==============================] - 60s 84ms/step - loss: 0.2701 - accuracy: 0.9282 - val_loss: 0.3286 - val_accuracy: 0.8929 - lr: 3.0000e-04\n",
      "Epoch 4/20\n",
      "714/714 [==============================] - 60s 84ms/step - loss: 0.1656 - accuracy: 0.9708 - val_loss: 0.2643 - val_accuracy: 0.9226 - lr: 3.0000e-04\n",
      "Epoch 5/20\n",
      "714/714 [==============================] - 60s 84ms/step - loss: 0.1237 - accuracy: 0.9847 - val_loss: 0.0843 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 6/20\n",
      "714/714 [==============================] - 60s 84ms/step - loss: 0.1110 - accuracy: 0.9889 - val_loss: 0.1291 - val_accuracy: 0.9762 - lr: 3.0000e-04\n",
      "Epoch 7/20\n",
      "714/714 [==============================] - 60s 83ms/step - loss: 0.1010 - accuracy: 0.9907 - val_loss: 0.0793 - val_accuracy: 0.9970 - lr: 3.0000e-04\n",
      "Epoch 8/20\n",
      "714/714 [==============================] - 60s 84ms/step - loss: 0.2109 - accuracy: 0.9511 - val_loss: 0.0948 - val_accuracy: 0.9851 - lr: 3.0000e-04\n",
      "Epoch 9/20\n",
      "714/714 [==============================] - 60s 84ms/step - loss: 0.1044 - accuracy: 0.9877 - val_loss: 0.1365 - val_accuracy: 0.9762 - lr: 3.0000e-04\n",
      "Epoch 10/20\n",
      "714/714 [==============================] - 60s 84ms/step - loss: 0.0888 - accuracy: 0.9923 - val_loss: 0.1475 - val_accuracy: 0.9732 - lr: 3.0000e-04\n",
      "Epoch 11/20\n",
      "714/714 [==============================] - 60s 84ms/step - loss: 0.0795 - accuracy: 0.9941 - val_loss: 0.0769 - val_accuracy: 0.9911 - lr: 3.0000e-04\n",
      "Epoch 12/20\n",
      "714/714 [==============================] - 60s 84ms/step - loss: 0.0812 - accuracy: 0.9918 - val_loss: 0.0764 - val_accuracy: 0.9911 - lr: 3.0000e-04\n",
      "Epoch 13/20\n",
      "714/714 [==============================] - 60s 83ms/step - loss: 0.0687 - accuracy: 0.9960 - val_loss: 0.0609 - val_accuracy: 0.9970 - lr: 3.0000e-04\n",
      "Epoch 14/20\n",
      "714/714 [==============================] - 60s 84ms/step - loss: 0.0659 - accuracy: 0.9952 - val_loss: 0.0987 - val_accuracy: 0.9851 - lr: 3.0000e-04\n",
      "Epoch 15/20\n",
      "714/714 [==============================] - 60s 84ms/step - loss: 0.0690 - accuracy: 0.9927 - val_loss: 0.0534 - val_accuracy: 0.9970 - lr: 3.0000e-04\n",
      "Epoch 16/20\n",
      "714/714 [==============================] - 60s 84ms/step - loss: 0.0598 - accuracy: 0.9955 - val_loss: 0.0667 - val_accuracy: 0.9911 - lr: 3.0000e-04\n",
      "Epoch 17/20\n",
      "714/714 [==============================] - 60s 84ms/step - loss: 0.0566 - accuracy: 0.9949 - val_loss: 0.0869 - val_accuracy: 0.9911 - lr: 3.0000e-04\n",
      "Epoch 18/20\n",
      "714/714 [==============================] - 60s 84ms/step - loss: 0.0563 - accuracy: 0.9953 - val_loss: 0.0718 - val_accuracy: 0.9911 - lr: 3.0000e-04\n",
      "Epoch 19/20\n",
      "714/714 [==============================] - 60s 84ms/step - loss: 0.0491 - accuracy: 0.9972 - val_loss: 0.0709 - val_accuracy: 0.9911 - lr: 3.0000e-04\n",
      "Epoch 20/20\n",
      "714/714 [==============================] - 60s 84ms/step - loss: 0.0462 - accuracy: 0.9973 - val_loss: 0.1137 - val_accuracy: 0.9851 - lr: 3.0000e-04\n",
      "(356, 453) (356, 1)\n",
      "Epoch 1/20\n",
      "713/713 [==============================] - 63s 85ms/step - loss: 0.5177 - accuracy: 0.8259 - val_loss: 0.2134 - val_accuracy: 0.9607 - lr: 3.0000e-04\n",
      "Epoch 2/20\n",
      "713/713 [==============================] - 60s 84ms/step - loss: 0.4029 - accuracy: 0.8705 - val_loss: 0.7245 - val_accuracy: 0.6685 - lr: 3.0000e-04\n",
      "Epoch 3/20\n",
      "713/713 [==============================] - 60s 84ms/step - loss: 0.2473 - accuracy: 0.9385 - val_loss: 0.1111 - val_accuracy: 0.9972 - lr: 3.0000e-04\n",
      "Epoch 4/20\n",
      "713/713 [==============================] - 60s 84ms/step - loss: 0.1809 - accuracy: 0.9632 - val_loss: 0.1033 - val_accuracy: 0.9944 - lr: 3.0000e-04\n",
      "Epoch 5/20\n",
      "713/713 [==============================] - 60s 84ms/step - loss: 0.1241 - accuracy: 0.9839 - val_loss: 0.0873 - val_accuracy: 0.9944 - lr: 3.0000e-04\n",
      "Epoch 6/20\n",
      "713/713 [==============================] - 60s 85ms/step - loss: 0.1109 - accuracy: 0.9883 - val_loss: 0.0841 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 7/20\n",
      "713/713 [==============================] - 60s 85ms/step - loss: 0.1015 - accuracy: 0.9904 - val_loss: 0.0939 - val_accuracy: 0.9972 - lr: 3.0000e-04\n",
      "Epoch 8/20\n",
      "713/713 [==============================] - 60s 84ms/step - loss: 0.0966 - accuracy: 0.9914 - val_loss: 0.0707 - val_accuracy: 0.9972 - lr: 3.0000e-04\n",
      "Epoch 9/20\n",
      "713/713 [==============================] - 60s 85ms/step - loss: 0.0890 - accuracy: 0.9913 - val_loss: 0.0714 - val_accuracy: 0.9972 - lr: 3.0000e-04\n",
      "Epoch 10/20\n",
      "713/713 [==============================] - 60s 84ms/step - loss: 0.0798 - accuracy: 0.9939 - val_loss: 0.0781 - val_accuracy: 0.9944 - lr: 3.0000e-04\n",
      "Epoch 11/20\n",
      "713/713 [==============================] - 60s 84ms/step - loss: 0.0802 - accuracy: 0.9926 - val_loss: 0.1106 - val_accuracy: 0.9691 - lr: 3.0000e-04\n",
      "Epoch 12/20\n",
      "713/713 [==============================] - 58s 82ms/step - loss: 0.0741 - accuracy: 0.9927 - val_loss: 0.0529 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 13/20\n",
      "713/713 [==============================] - 60s 84ms/step - loss: 0.0679 - accuracy: 0.9944 - val_loss: 0.0500 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 14/20\n",
      "713/713 [==============================] - 60s 84ms/step - loss: 0.0641 - accuracy: 0.9950 - val_loss: 0.0564 - val_accuracy: 0.9972 - lr: 3.0000e-04\n",
      "Epoch 15/20\n",
      "713/713 [==============================] - 60s 84ms/step - loss: 0.0653 - accuracy: 0.9939 - val_loss: 0.0646 - val_accuracy: 0.9944 - lr: 3.0000e-04\n",
      "Epoch 16/20\n",
      "713/713 [==============================] - 60s 84ms/step - loss: 0.0516 - accuracy: 0.9978 - val_loss: 0.0526 - val_accuracy: 0.9972 - lr: 3.0000e-04\n",
      "Epoch 17/20\n",
      "713/713 [==============================] - 60s 84ms/step - loss: 0.0587 - accuracy: 0.9951 - val_loss: 0.0907 - val_accuracy: 0.9831 - lr: 3.0000e-04\n",
      "Epoch 18/20\n",
      "713/713 [==============================] - 60s 85ms/step - loss: 0.0463 - accuracy: 0.9985 - val_loss: 0.0383 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 19/20\n",
      "713/713 [==============================] - 60s 85ms/step - loss: 0.0542 - accuracy: 0.9949 - val_loss: 0.0592 - val_accuracy: 0.9944 - lr: 3.0000e-04\n",
      "Epoch 20/20\n",
      "713/713 [==============================] - 60s 84ms/step - loss: 0.0480 - accuracy: 0.9970 - val_loss: 0.0407 - val_accuracy: 0.9972 - lr: 3.0000e-04\n",
      "(316, 453) (316, 1)\n",
      "Epoch 1/20\n",
      "715/715 [==============================] - 62s 84ms/step - loss: 0.5122 - accuracy: 0.8260 - val_loss: 0.3873 - val_accuracy: 0.8797 - lr: 3.0000e-04\n",
      "Epoch 2/20\n",
      "715/715 [==============================] - 60s 84ms/step - loss: 0.3654 - accuracy: 0.8888 - val_loss: 0.3287 - val_accuracy: 0.9272 - lr: 3.0000e-04\n",
      "Epoch 3/20\n",
      "715/715 [==============================] - 60s 84ms/step - loss: 0.2582 - accuracy: 0.9331 - val_loss: 0.2064 - val_accuracy: 0.9494 - lr: 3.0000e-04\n",
      "Epoch 4/20\n",
      "715/715 [==============================] - 60s 83ms/step - loss: 0.1919 - accuracy: 0.9580 - val_loss: 0.1617 - val_accuracy: 0.9715 - lr: 3.0000e-04\n",
      "Epoch 5/20\n",
      "715/715 [==============================] - 60s 84ms/step - loss: 0.1639 - accuracy: 0.9691 - val_loss: 0.1223 - val_accuracy: 0.9810 - lr: 3.0000e-04\n",
      "Epoch 6/20\n",
      "715/715 [==============================] - 60s 84ms/step - loss: 0.1452 - accuracy: 0.9745 - val_loss: 0.1971 - val_accuracy: 0.9715 - lr: 3.0000e-04\n",
      "Epoch 7/20\n",
      "715/715 [==============================] - 60s 85ms/step - loss: 0.1795 - accuracy: 0.9602 - val_loss: 0.1877 - val_accuracy: 0.9462 - lr: 3.0000e-04\n",
      "Epoch 8/20\n",
      "715/715 [==============================] - 60s 84ms/step - loss: 0.1788 - accuracy: 0.9611 - val_loss: 0.0914 - val_accuracy: 0.9937 - lr: 3.0000e-04\n",
      "Epoch 9/20\n",
      "715/715 [==============================] - 60s 84ms/step - loss: 0.1041 - accuracy: 0.9887 - val_loss: 0.0979 - val_accuracy: 0.9842 - lr: 3.0000e-04\n",
      "Epoch 10/20\n",
      "715/715 [==============================] - 60s 84ms/step - loss: 0.0934 - accuracy: 0.9913 - val_loss: 0.1015 - val_accuracy: 0.9842 - lr: 3.0000e-04\n",
      "Epoch 11/20\n",
      "715/715 [==============================] - 60s 84ms/step - loss: 0.0905 - accuracy: 0.9913 - val_loss: 0.0770 - val_accuracy: 0.9937 - lr: 3.0000e-04\n",
      "Epoch 12/20\n",
      "715/715 [==============================] - 60s 85ms/step - loss: 0.0781 - accuracy: 0.9942 - val_loss: 0.0638 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 13/20\n",
      "715/715 [==============================] - 60s 84ms/step - loss: 0.0771 - accuracy: 0.9937 - val_loss: 0.0629 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 14/20\n",
      "715/715 [==============================] - 60s 84ms/step - loss: 0.0676 - accuracy: 0.9957 - val_loss: 0.0543 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 15/20\n",
      "715/715 [==============================] - 59s 82ms/step - loss: 0.0678 - accuracy: 0.9948 - val_loss: 0.0649 - val_accuracy: 0.9968 - lr: 3.0000e-04\n",
      "Epoch 16/20\n",
      "715/715 [==============================] - 60s 84ms/step - loss: 0.0695 - accuracy: 0.9933 - val_loss: 0.0616 - val_accuracy: 0.9905 - lr: 3.0000e-04\n",
      "Epoch 17/20\n",
      "715/715 [==============================] - 60s 84ms/step - loss: 0.0603 - accuracy: 0.9962 - val_loss: 0.0480 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 18/20\n",
      "715/715 [==============================] - 60s 84ms/step - loss: 0.0574 - accuracy: 0.9962 - val_loss: 0.0485 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 19/20\n",
      "715/715 [==============================] - 60s 84ms/step - loss: 0.0555 - accuracy: 0.9958 - val_loss: 0.1806 - val_accuracy: 0.9272 - lr: 3.0000e-04\n",
      "Epoch 20/20\n",
      "715/715 [==============================] - 59s 83ms/step - loss: 0.0514 - accuracy: 0.9962 - val_loss: 0.0398 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "(515, 453) (515, 1)\n",
      "Epoch 1/20\n",
      "703/703 [==============================] - 61s 83ms/step - loss: 0.5395 - accuracy: 0.8222 - val_loss: 0.3149 - val_accuracy: 0.9165 - lr: 3.0000e-04\n",
      "Epoch 2/20\n",
      "703/703 [==============================] - 60s 85ms/step - loss: 0.3693 - accuracy: 0.8883 - val_loss: 0.6087 - val_accuracy: 0.7786 - lr: 3.0000e-04\n",
      "Epoch 3/20\n",
      "703/703 [==============================] - 60s 85ms/step - loss: 0.2394 - accuracy: 0.9422 - val_loss: 0.1658 - val_accuracy: 0.9689 - lr: 3.0000e-04\n",
      "Epoch 4/20\n",
      "703/703 [==============================] - 59s 85ms/step - loss: 0.1866 - accuracy: 0.9629 - val_loss: 0.1072 - val_accuracy: 0.9942 - lr: 3.0000e-04\n",
      "Epoch 5/20\n",
      "703/703 [==============================] - 60s 85ms/step - loss: 0.1795 - accuracy: 0.9657 - val_loss: 0.1027 - val_accuracy: 0.9922 - lr: 3.0000e-04\n",
      "Epoch 6/20\n",
      "703/703 [==============================] - 60s 85ms/step - loss: 0.1364 - accuracy: 0.9810 - val_loss: 0.6320 - val_accuracy: 0.8214 - lr: 3.0000e-04\n",
      "Epoch 7/20\n",
      "703/703 [==============================] - 59s 85ms/step - loss: 0.1231 - accuracy: 0.9827 - val_loss: 0.0897 - val_accuracy: 0.9942 - lr: 3.0000e-04\n",
      "Epoch 8/20\n",
      "703/703 [==============================] - 58s 82ms/step - loss: 0.1021 - accuracy: 0.9893 - val_loss: 0.0772 - val_accuracy: 0.9981 - lr: 3.0000e-04\n",
      "Epoch 9/20\n",
      "703/703 [==============================] - 58s 82ms/step - loss: 0.0901 - accuracy: 0.9931 - val_loss: 0.0758 - val_accuracy: 0.9961 - lr: 3.0000e-04\n",
      "Epoch 10/20\n",
      "703/703 [==============================] - 58s 82ms/step - loss: 0.0845 - accuracy: 0.9940 - val_loss: 0.0674 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 11/20\n",
      "703/703 [==============================] - 60s 85ms/step - loss: 0.0863 - accuracy: 0.9929 - val_loss: 0.0637 - val_accuracy: 0.9981 - lr: 3.0000e-04\n",
      "Epoch 12/20\n",
      "703/703 [==============================] - 60s 85ms/step - loss: 0.0749 - accuracy: 0.9952 - val_loss: 0.0691 - val_accuracy: 0.9961 - lr: 3.0000e-04\n",
      "Epoch 13/20\n",
      "703/703 [==============================] - 59s 85ms/step - loss: 0.0688 - accuracy: 0.9961 - val_loss: 0.0542 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 14/20\n",
      "703/703 [==============================] - 59s 84ms/step - loss: 0.0670 - accuracy: 0.9953 - val_loss: 0.0498 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 15/20\n",
      "703/703 [==============================] - 60s 85ms/step - loss: 0.0540 - accuracy: 0.9985 - val_loss: 0.0470 - val_accuracy: 0.9981 - lr: 3.0000e-04\n",
      "Epoch 16/20\n",
      "703/703 [==============================] - 59s 84ms/step - loss: 0.0582 - accuracy: 0.9964 - val_loss: 0.0432 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 17/20\n",
      "703/703 [==============================] - 59s 84ms/step - loss: 0.0513 - accuracy: 0.9968 - val_loss: 0.0890 - val_accuracy: 0.9845 - lr: 3.0000e-04\n",
      "Epoch 18/20\n",
      "703/703 [==============================] - 60s 85ms/step - loss: 0.0482 - accuracy: 0.9975 - val_loss: 0.0376 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 19/20\n",
      "703/703 [==============================] - 60s 85ms/step - loss: 0.0445 - accuracy: 0.9977 - val_loss: 0.0602 - val_accuracy: 0.9942 - lr: 3.0000e-04\n",
      "Epoch 20/20\n",
      "703/703 [==============================] - 60s 85ms/step - loss: 0.0503 - accuracy: 0.9960 - val_loss: 0.0396 - val_accuracy: 0.9981 - lr: 3.0000e-04\n",
      "(408, 453) (408, 1)\n",
      "Epoch 1/20\n",
      "710/710 [==============================] - 63s 85ms/step - loss: 0.5264 - accuracy: 0.8225 - val_loss: 0.3331 - val_accuracy: 0.8799 - lr: 3.0000e-04\n",
      "Epoch 2/20\n",
      "710/710 [==============================] - 60s 84ms/step - loss: 0.3854 - accuracy: 0.8807 - val_loss: 0.2923 - val_accuracy: 0.9240 - lr: 3.0000e-04\n",
      "Epoch 3/20\n",
      "710/710 [==============================] - 60s 84ms/step - loss: 0.2678 - accuracy: 0.9276 - val_loss: 0.1963 - val_accuracy: 0.9387 - lr: 3.0000e-04\n",
      "Epoch 4/20\n",
      "710/710 [==============================] - 60s 84ms/step - loss: 0.1976 - accuracy: 0.9578 - val_loss: 0.3889 - val_accuracy: 0.9167 - lr: 3.0000e-04\n",
      "Epoch 5/20\n",
      "710/710 [==============================] - 60s 85ms/step - loss: 0.1783 - accuracy: 0.9658 - val_loss: 0.1135 - val_accuracy: 0.9828 - lr: 3.0000e-04\n",
      "Epoch 6/20\n",
      "710/710 [==============================] - 60s 84ms/step - loss: 0.1289 - accuracy: 0.9839 - val_loss: 0.0974 - val_accuracy: 0.9902 - lr: 3.0000e-04\n",
      "Epoch 7/20\n",
      "710/710 [==============================] - 60s 85ms/step - loss: 0.1121 - accuracy: 0.9890 - val_loss: 0.1158 - val_accuracy: 0.9804 - lr: 3.0000e-04\n",
      "Epoch 8/20\n",
      "710/710 [==============================] - 60s 84ms/step - loss: 0.1204 - accuracy: 0.9837 - val_loss: 0.0758 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 9/20\n",
      "710/710 [==============================] - 60s 84ms/step - loss: 0.0970 - accuracy: 0.9907 - val_loss: 0.0745 - val_accuracy: 0.9975 - lr: 3.0000e-04\n",
      "Epoch 10/20\n",
      "710/710 [==============================] - 60s 84ms/step - loss: 0.0969 - accuracy: 0.9896 - val_loss: 0.0857 - val_accuracy: 0.9951 - lr: 3.0000e-04\n",
      "Epoch 11/20\n",
      "710/710 [==============================] - 60s 85ms/step - loss: 0.0832 - accuracy: 0.9927 - val_loss: 0.0620 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 12/20\n",
      "710/710 [==============================] - 60s 84ms/step - loss: 0.0767 - accuracy: 0.9938 - val_loss: 0.0634 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 13/20\n",
      "710/710 [==============================] - 60s 85ms/step - loss: 0.0797 - accuracy: 0.9917 - val_loss: 0.0691 - val_accuracy: 0.9926 - lr: 3.0000e-04\n",
      "Epoch 14/20\n",
      "710/710 [==============================] - 60s 85ms/step - loss: 0.0684 - accuracy: 0.9951 - val_loss: 0.0522 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 15/20\n",
      "710/710 [==============================] - 60s 84ms/step - loss: 0.0681 - accuracy: 0.9933 - val_loss: 0.0560 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 16/20\n",
      "710/710 [==============================] - 60s 84ms/step - loss: 0.0621 - accuracy: 0.9950 - val_loss: 0.0648 - val_accuracy: 0.9926 - lr: 3.0000e-04\n",
      "Epoch 17/20\n",
      "710/710 [==============================] - 60s 84ms/step - loss: 0.0571 - accuracy: 0.9960 - val_loss: 0.0666 - val_accuracy: 0.9902 - lr: 3.0000e-04\n",
      "Epoch 18/20\n",
      "710/710 [==============================] - 60s 84ms/step - loss: 0.0542 - accuracy: 0.9956 - val_loss: 0.0466 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 19/20\n",
      "710/710 [==============================] - 60s 85ms/step - loss: 0.0545 - accuracy: 0.9953 - val_loss: 0.0401 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 20/20\n",
      "710/710 [==============================] - 60s 84ms/step - loss: 0.0503 - accuracy: 0.9964 - val_loss: 0.0395 - val_accuracy: 1.0000 - lr: 3.0000e-04\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "batch_size = 16\n",
    "\n",
    "val_acc_per_fold = []\n",
    "acc_per_fold = []\n",
    "val_loss_per_fold = []\n",
    "loss_per_fold = []\n",
    "\n",
    "for n in range(1, 31):\n",
    "    test_x = pd.DataFrame()\n",
    "    test_y = pd.DataFrame()\n",
    "\n",
    "    idxs_test = df_fall[df_fall['subject'] == n].index.to_list()\n",
    "    idxs_test_2 = df_adl[df_adl['subject'] == n].index.to_list()\n",
    "    test_x = pd.concat([test_x, fall_data.loc[idxs_test], adl_data.loc[idxs_test_2]])\n",
    "    test_y = pd.concat([test_y, df_fall.loc[idxs_test]['label'], df_adl.loc[idxs_test_2]['label']])\n",
    "    print(test_x.shape, test_y.shape)\n",
    "\n",
    "    train_x = pd.DataFrame()\n",
    "    train_y = pd.DataFrame()\n",
    "\n",
    "    fall_data_c = fall_data.copy()\n",
    "    adl_data_c = adl_data.copy()\n",
    "    fall_data_c.drop(idxs_test, axis=0, inplace=True)\n",
    "    adl_data_c.drop(idxs_test_2, axis=0, inplace=True)\n",
    "    train_x = pd.concat([train_x, fall_data_c, adl_data_c])\n",
    "\n",
    "    df_fall_c = df_fall.copy()\n",
    "    df_adl_c = df_adl.copy()\n",
    "    df_fall_c.drop(idxs_test, axis=0, inplace=True)\n",
    "    df_adl_c.drop(idxs_test_2, axis=0, inplace=True)\n",
    "    train_y = pd.concat([train_y, df_fall_c['label'], df_adl_c['label']])\n",
    "    #print(train_x.shape, train_y.shape)\n",
    "\n",
    "    classifier_test_opt_bi_leave = build_model_bidirectional()\n",
    "\n",
    "    #print(np.array(train_y.astype(int)))\n",
    "    \n",
    "    history_test_opt_bi_leave = classifier_test_opt_bi_leave.fit(\n",
    "        np.array(train_x), np.array(train_y),\n",
    "        epochs=epochs,\n",
    "        batch_size = batch_size,\n",
    "        steps_per_epoch=(train_x.shape[0]) // batch_size,\n",
    "        validation_data=(np.array(test_x), np.array(test_y)),\n",
    "        callbacks=[tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=10, min_lr=0.00003)]\n",
    "    )\n",
    "\n",
    "    val_acc_per_fold.append(history_test_opt_bi_leave.history['val_accuracy'])\n",
    "    acc_per_fold.append(history_test_opt_bi_leave.history['accuracy'])\n",
    "    val_loss_per_fold.append(history_test_opt_bi_leave.history['val_loss'])\n",
    "    loss_per_fold.append(history_test_opt_bi_leave.history['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[0.84375,\n",
       "   0.9348958134651184,\n",
       "   0.9635416865348816,\n",
       "   0.9895833134651184,\n",
       "   0.984375,\n",
       "   0.9088541865348816,\n",
       "   0.9973958134651184,\n",
       "   0.953125,\n",
       "   0.9817708134651184,\n",
       "   0.9791666865348816,\n",
       "   0.9505208134651184,\n",
       "   0.9973958134651184,\n",
       "   0.9947916865348816,\n",
       "   1.0,\n",
       "   0.9947916865348816,\n",
       "   0.9947916865348816,\n",
       "   1.0,\n",
       "   0.9583333134651184,\n",
       "   1.0,\n",
       "   1.0],\n",
       "  [0.8233276009559631,\n",
       "   0.8747856020927429,\n",
       "   0.9914236664772034,\n",
       "   0.9862778782844543,\n",
       "   0.9554030895233154,\n",
       "   0.8010291457176208,\n",
       "   0.9931389093399048,\n",
       "   1.0,\n",
       "   0.9073756337165833,\n",
       "   0.9982847571372986,\n",
       "   0.994854211807251,\n",
       "   0.994854211807251,\n",
       "   0.9879931211471558,\n",
       "   1.0,\n",
       "   0.994854211807251,\n",
       "   0.9879931211471558,\n",
       "   0.9794167876243591,\n",
       "   0.994854211807251,\n",
       "   0.994854211807251,\n",
       "   0.9982847571372986],\n",
       "  [0.9177631735801697,\n",
       "   0.9144737124443054,\n",
       "   0.9671052694320679,\n",
       "   0.9703947305679321,\n",
       "   0.9144737124443054,\n",
       "   0.9802631735801697,\n",
       "   0.9934210777282715,\n",
       "   0.9934210777282715,\n",
       "   0.9736841917037964,\n",
       "   0.9769737124443054,\n",
       "   0.9901315569877625,\n",
       "   0.9835526347160339,\n",
       "   0.9605262875556946,\n",
       "   0.9934210777282715,\n",
       "   0.9638158082962036,\n",
       "   0.9802631735801697,\n",
       "   0.9901315569877625,\n",
       "   0.9736841917037964,\n",
       "   0.9901315569877625,\n",
       "   1.0],\n",
       "  [0.6430867910385132,\n",
       "   0.9067524075508118,\n",
       "   0.909967839717865,\n",
       "   0.9356912970542908,\n",
       "   0.8649517893791199,\n",
       "   0.8778135180473328,\n",
       "   0.9324758648872375,\n",
       "   0.9356912970542908,\n",
       "   0.9260450005531311,\n",
       "   0.9260450005531311,\n",
       "   0.9485530257225037,\n",
       "   0.9421221613883972,\n",
       "   0.971061110496521,\n",
       "   0.9163987040519714,\n",
       "   0.9678456783294678,\n",
       "   0.9067524075508118,\n",
       "   0.9035369753837585,\n",
       "   0.9067524075508118,\n",
       "   0.9453375935554504,\n",
       "   0.9228295683860779],\n",
       "  [0.7382550239562988,\n",
       "   0.9127516746520996,\n",
       "   0.963087260723114,\n",
       "   0.9832214713096619,\n",
       "   0.963087260723114,\n",
       "   0.9060402512550354,\n",
       "   0.9530201554298401,\n",
       "   0.8758389353752136,\n",
       "   0.9026845693588257,\n",
       "   0.9597315192222595,\n",
       "   0.9798657894134521,\n",
       "   0.9865771532058716,\n",
       "   0.9597315192222595,\n",
       "   0.9966443181037903,\n",
       "   0.9731543660163879,\n",
       "   1.0,\n",
       "   0.9899328947067261,\n",
       "   1.0,\n",
       "   0.9966443181037903,\n",
       "   1.0],\n",
       "  [0.8907766938209534,\n",
       "   0.9417475461959839,\n",
       "   0.9927184581756592,\n",
       "   0.9805825352668762,\n",
       "   0.9296116232872009,\n",
       "   0.9902912378311157,\n",
       "   0.9975728392601013,\n",
       "   0.9975728392601013,\n",
       "   0.9708737730979919,\n",
       "   1.0,\n",
       "   1.0,\n",
       "   0.9975728392601013,\n",
       "   0.9951456189155579,\n",
       "   1.0,\n",
       "   0.9975728392601013,\n",
       "   0.9854369163513184,\n",
       "   1.0,\n",
       "   0.9805825352668762,\n",
       "   0.9951456189155579,\n",
       "   1.0],\n",
       "  [0.9090909361839294,\n",
       "   0.8757575750350952,\n",
       "   0.9696969985961914,\n",
       "   0.960606038570404,\n",
       "   0.9818181991577148,\n",
       "   0.9939393997192383,\n",
       "   0.9848484992980957,\n",
       "   0.9969696998596191,\n",
       "   0.9939393997192383,\n",
       "   0.9939393997192383,\n",
       "   0.9969696998596191,\n",
       "   0.9969696998596191,\n",
       "   1.0,\n",
       "   0.9939393997192383,\n",
       "   1.0,\n",
       "   1.0,\n",
       "   1.0,\n",
       "   0.9969696998596191,\n",
       "   0.9757575988769531,\n",
       "   1.0],\n",
       "  [0.9017632007598877,\n",
       "   0.8816120624542236,\n",
       "   0.9672544002532959,\n",
       "   0.8261964917182922,\n",
       "   0.9647355079650879,\n",
       "   0.9697732925415039,\n",
       "   0.9622166156768799,\n",
       "   0.9672544002532959,\n",
       "   0.9748110771179199,\n",
       "   0.9697732925415039,\n",
       "   0.9773299694061279,\n",
       "   0.9722921848297119,\n",
       "   0.9521410465240479,\n",
       "   0.9722921848297119,\n",
       "   0.9773299694061279,\n",
       "   0.9697732925415039,\n",
       "   0.9748110771179199,\n",
       "   0.9672544002532959,\n",
       "   0.9773299694061279,\n",
       "   0.9294710159301758],\n",
       "  [0.8115941882133484,\n",
       "   0.9106280207633972,\n",
       "   0.9371980428695679,\n",
       "   0.9589372277259827,\n",
       "   0.7342995405197144,\n",
       "   0.95652174949646,\n",
       "   0.95652174949646,\n",
       "   0.9830917716026306,\n",
       "   0.9855072498321533,\n",
       "   0.95652174949646,\n",
       "   0.97826087474823,\n",
       "   0.990338146686554,\n",
       "   0.9830917716026306,\n",
       "   0.987922728061676,\n",
       "   0.990338146686554,\n",
       "   0.9927536249160767,\n",
       "   0.97826087474823,\n",
       "   0.97826087474823,\n",
       "   0.9951691031455994,\n",
       "   0.987922728061676],\n",
       "  [0.9392971396446228,\n",
       "   0.9808306694030762,\n",
       "   0.9233226776123047,\n",
       "   0.9936102032661438,\n",
       "   1.0,\n",
       "   1.0,\n",
       "   0.9872204661369324,\n",
       "   0.9776358008384705,\n",
       "   0.9936102032661438,\n",
       "   1.0,\n",
       "   0.9904153347015381,\n",
       "   1.0,\n",
       "   0.9936102032661438,\n",
       "   1.0,\n",
       "   1.0,\n",
       "   1.0,\n",
       "   0.9936102032661438,\n",
       "   0.9904153347015381,\n",
       "   0.9968051314353943,\n",
       "   1.0],\n",
       "  [0.9362244606018066,\n",
       "   0.9693877696990967,\n",
       "   0.9668367505073547,\n",
       "   0.9770408272743225,\n",
       "   0.9974489808082581,\n",
       "   1.0,\n",
       "   0.9362244606018066,\n",
       "   0.9821428656578064,\n",
       "   0.9897959232330322,\n",
       "   0.9923469424247742,\n",
       "   0.9948979616165161,\n",
       "   1.0,\n",
       "   1.0,\n",
       "   0.9132652878761292,\n",
       "   0.9974489808082581,\n",
       "   0.9923469424247742,\n",
       "   1.0,\n",
       "   0.9744898080825806,\n",
       "   1.0,\n",
       "   0.9872449040412903],\n",
       "  [0.8059999942779541,\n",
       "   0.9520000219345093,\n",
       "   0.9739999771118164,\n",
       "   0.984000027179718,\n",
       "   0.9779999852180481,\n",
       "   0.9879999756813049,\n",
       "   0.9980000257492065,\n",
       "   0.9980000257492065,\n",
       "   0.9980000257492065,\n",
       "   0.9959999918937683,\n",
       "   0.9980000257492065,\n",
       "   0.9959999918937683,\n",
       "   0.9940000176429749,\n",
       "   1.0,\n",
       "   0.9919999837875366,\n",
       "   0.9959999918937683,\n",
       "   0.9959999918937683,\n",
       "   1.0,\n",
       "   1.0,\n",
       "   1.0],\n",
       "  [0.8938992023468018,\n",
       "   0.8355437517166138,\n",
       "   0.8647214770317078,\n",
       "   0.9151193499565125,\n",
       "   0.9496021270751953,\n",
       "   0.970822274684906,\n",
       "   0.9389920234680176,\n",
       "   0.970822274684906,\n",
       "   0.941644549369812,\n",
       "   0.9469496011734009,\n",
       "   0.9946949481964111,\n",
       "   0.9814323782920837,\n",
       "   0.9734748005867004,\n",
       "   0.9761273264884949,\n",
       "   0.9946949481964111,\n",
       "   0.9946949481964111,\n",
       "   0.9867374300956726,\n",
       "   0.9946949481964111,\n",
       "   0.9814323782920837,\n",
       "   0.9628646969795227],\n",
       "  [0.9139785170555115,\n",
       "   0.9059139490127563,\n",
       "   0.9946236610412598,\n",
       "   0.9946236610412598,\n",
       "   0.9946236610412598,\n",
       "   1.0,\n",
       "   1.0,\n",
       "   1.0,\n",
       "   1.0,\n",
       "   0.9973118305206299,\n",
       "   0.9973118305206299,\n",
       "   0.9919354915618896,\n",
       "   0.9973118305206299,\n",
       "   1.0,\n",
       "   0.9973118305206299,\n",
       "   1.0,\n",
       "   0.9973118305206299,\n",
       "   0.9973118305206299,\n",
       "   1.0,\n",
       "   0.9973118305206299],\n",
       "  [0.8683544397354126,\n",
       "   0.8025316596031189,\n",
       "   0.9392405152320862,\n",
       "   0.9240506291389465,\n",
       "   0.946835458278656,\n",
       "   0.9518987536430359,\n",
       "   0.9721518754959106,\n",
       "   0.9898734092712402,\n",
       "   0.9569620490074158,\n",
       "   0.949367105960846,\n",
       "   0.9645569324493408,\n",
       "   0.9620253443717957,\n",
       "   0.9822784662246704,\n",
       "   0.9822784662246704,\n",
       "   0.9873417615890503,\n",
       "   0.9949367046356201,\n",
       "   0.9620253443717957,\n",
       "   0.9924050569534302,\n",
       "   0.9873417615890503,\n",
       "   0.9974683523178101],\n",
       "  [0.8440170884132385,\n",
       "   0.811965823173523,\n",
       "   0.8547008633613586,\n",
       "   0.9615384340286255,\n",
       "   0.9829059839248657,\n",
       "   0.9871794581413269,\n",
       "   0.9594017267227173,\n",
       "   0.9658119678497314,\n",
       "   0.9722222089767456,\n",
       "   0.9786324501037598,\n",
       "   0.9935897588729858,\n",
       "   0.9764957427978516,\n",
       "   0.9764957427978516,\n",
       "   0.9722222089767456,\n",
       "   0.8269230723381042,\n",
       "   0.9935897588729858,\n",
       "   1.0,\n",
       "   0.995726466178894,\n",
       "   0.9786324501037598,\n",
       "   0.9871794581413269],\n",
       "  [0.8907923102378845,\n",
       "   0.9293361902236938,\n",
       "   0.9250535368919373,\n",
       "   0.9935759902000427,\n",
       "   1.0,\n",
       "   0.9443255066871643,\n",
       "   0.9978586435317993,\n",
       "   1.0,\n",
       "   0.9978586435317993,\n",
       "   0.9935759902000427,\n",
       "   1.0,\n",
       "   1.0,\n",
       "   0.9978586435317993,\n",
       "   1.0,\n",
       "   1.0,\n",
       "   1.0,\n",
       "   1.0,\n",
       "   1.0,\n",
       "   1.0,\n",
       "   1.0],\n",
       "  [0.5379310250282288,\n",
       "   0.7241379022598267,\n",
       "   0.8551723957061768,\n",
       "   0.9965517520904541,\n",
       "   0.9965517520904541,\n",
       "   0.9724137783050537,\n",
       "   0.9931034445762634,\n",
       "   1.0,\n",
       "   0.9931034445762634,\n",
       "   1.0,\n",
       "   1.0,\n",
       "   1.0,\n",
       "   0.9965517520904541,\n",
       "   1.0,\n",
       "   1.0,\n",
       "   0.9965517520904541,\n",
       "   1.0,\n",
       "   0.982758641242981,\n",
       "   1.0,\n",
       "   0.9896551966667175],\n",
       "  [0.7406014800071716,\n",
       "   0.9172932505607605,\n",
       "   0.8120300769805908,\n",
       "   0.9736841917037964,\n",
       "   0.9436089992523193,\n",
       "   0.9436089992523193,\n",
       "   0.9849624037742615,\n",
       "   0.9962406158447266,\n",
       "   0.9887217879295349,\n",
       "   0.9661654233932495,\n",
       "   0.9849624037742615,\n",
       "   1.0,\n",
       "   0.9736841917037964,\n",
       "   0.9962406158447266,\n",
       "   1.0,\n",
       "   0.9736841917037964,\n",
       "   1.0,\n",
       "   0.9586465954780579,\n",
       "   0.9962406158447266,\n",
       "   1.0],\n",
       "  [0.8199566006660461,\n",
       "   0.8806941509246826,\n",
       "   0.9154012799263,\n",
       "   0.9436008930206299,\n",
       "   0.9978308081626892,\n",
       "   0.9739696383476257,\n",
       "   0.9934924244880676,\n",
       "   0.9869847893714905,\n",
       "   0.989154040813446,\n",
       "   0.9826464056968689,\n",
       "   0.9652928709983826,\n",
       "   0.9826464056968689,\n",
       "   0.9718004465103149,\n",
       "   0.9652928709983826,\n",
       "   0.9826464056968689,\n",
       "   0.9956616163253784,\n",
       "   1.0,\n",
       "   0.9956616163253784,\n",
       "   1.0,\n",
       "   1.0],\n",
       "  [0.7607142925262451,\n",
       "   0.9321428537368774,\n",
       "   0.8964285850524902,\n",
       "   0.9678571224212646,\n",
       "   0.9964285492897034,\n",
       "   0.9857142567634583,\n",
       "   0.9928571581840515,\n",
       "   0.9964285492897034,\n",
       "   0.9964285492897034,\n",
       "   0.9964285492897034,\n",
       "   0.9928571581840515,\n",
       "   0.9964285492897034,\n",
       "   0.9964285492897034,\n",
       "   0.9928571581840515,\n",
       "   0.9964285492897034,\n",
       "   0.9964285492897034,\n",
       "   0.9964285492897034,\n",
       "   0.9964285492897034,\n",
       "   0.9964285492897034,\n",
       "   0.9964285492897034],\n",
       "  [0.81611567735672,\n",
       "   0.9276859760284424,\n",
       "   0.9442148804664612,\n",
       "   0.9586777091026306,\n",
       "   0.9648760557174683,\n",
       "   0.9814049601554871,\n",
       "   0.9814049601554871,\n",
       "   0.9876033067703247,\n",
       "   0.9896694421768188,\n",
       "   0.9814049601554871,\n",
       "   0.9834710955619812,\n",
       "   0.9876033067703247,\n",
       "   0.9938016533851624,\n",
       "   0.9896694421768188,\n",
       "   0.9917355179786682,\n",
       "   0.9917355179786682,\n",
       "   0.9876033067703247,\n",
       "   0.9958677887916565,\n",
       "   0.9834710955619812,\n",
       "   0.9896694421768188],\n",
       "  [0.8374233245849609,\n",
       "   0.7975460290908813,\n",
       "   0.89570552110672,\n",
       "   0.9723926186561584,\n",
       "   0.941717803478241,\n",
       "   0.9478527903556824,\n",
       "   0.941717803478241,\n",
       "   0.9846625924110413,\n",
       "   0.9662576913833618,\n",
       "   0.9693251252174377,\n",
       "   0.9509202241897583,\n",
       "   0.9631901979446411,\n",
       "   0.9969325065612793,\n",
       "   0.9754601120948792,\n",
       "   0.9969325065612793,\n",
       "   0.9907975196838379,\n",
       "   0.9110429286956787,\n",
       "   0.9631901979446411,\n",
       "   0.9846625924110413,\n",
       "   0.9969325065612793],\n",
       "  [0.8510638475418091,\n",
       "   0.88707035779953,\n",
       "   0.9214402437210083,\n",
       "   0.9639934301376343,\n",
       "   0.9803600907325745,\n",
       "   0.9672667980194092,\n",
       "   0.9918166995048523,\n",
       "   0.9689034223556519,\n",
       "   0.9918166995048523,\n",
       "   0.9819967150688171,\n",
       "   0.9803600907325745,\n",
       "   0.9754500985145569,\n",
       "   0.9312602281570435,\n",
       "   0.9901800155639648,\n",
       "   0.9689034223556519,\n",
       "   0.9803600907325745,\n",
       "   0.9525368213653564,\n",
       "   0.9689034223556519,\n",
       "   0.9738134145736694,\n",
       "   0.9819967150688171],\n",
       "  [0.8543210029602051,\n",
       "   0.9654321074485779,\n",
       "   0.9654321074485779,\n",
       "   0.9753086566925049,\n",
       "   0.9777777791023254,\n",
       "   0.9728395342826843,\n",
       "   0.9876543283462524,\n",
       "   0.9876543283462524,\n",
       "   0.9876543283462524,\n",
       "   0.990123450756073,\n",
       "   0.9851852059364319,\n",
       "   0.9851852059364319,\n",
       "   0.990123450756073,\n",
       "   0.9679012298583984,\n",
       "   0.9925925731658936,\n",
       "   0.9925925731658936,\n",
       "   0.990123450756073,\n",
       "   0.9851852059364319,\n",
       "   0.9851852059364319,\n",
       "   0.990123450756073],\n",
       "  [0.898809552192688,\n",
       "   0.8660714030265808,\n",
       "   0.8928571343421936,\n",
       "   0.9226190447807312,\n",
       "   1.0,\n",
       "   0.976190447807312,\n",
       "   0.9970238208770752,\n",
       "   0.9851190447807312,\n",
       "   0.976190447807312,\n",
       "   0.9732142686843872,\n",
       "   0.9910714030265808,\n",
       "   0.9910714030265808,\n",
       "   0.9970238208770752,\n",
       "   0.9851190447807312,\n",
       "   0.9970238208770752,\n",
       "   0.9910714030265808,\n",
       "   0.9910714030265808,\n",
       "   0.9910714030265808,\n",
       "   0.9910714030265808,\n",
       "   0.9851190447807312],\n",
       "  [0.9606741666793823,\n",
       "   0.6685393452644348,\n",
       "   0.9971910119056702,\n",
       "   0.9943820238113403,\n",
       "   0.9943820238113403,\n",
       "   1.0,\n",
       "   0.9971910119056702,\n",
       "   0.9971910119056702,\n",
       "   0.9971910119056702,\n",
       "   0.9943820238113403,\n",
       "   0.9691011309623718,\n",
       "   1.0,\n",
       "   1.0,\n",
       "   0.9971910119056702,\n",
       "   0.9943820238113403,\n",
       "   0.9971910119056702,\n",
       "   0.983146071434021,\n",
       "   1.0,\n",
       "   0.9943820238113403,\n",
       "   0.9971910119056702],\n",
       "  [0.8797468543052673,\n",
       "   0.9272152185440063,\n",
       "   0.949367105960846,\n",
       "   0.9715189933776855,\n",
       "   0.9810126423835754,\n",
       "   0.9715189933776855,\n",
       "   0.9462025165557861,\n",
       "   0.9936708807945251,\n",
       "   0.9841772317886353,\n",
       "   0.9841772317886353,\n",
       "   0.9936708807945251,\n",
       "   1.0,\n",
       "   1.0,\n",
       "   1.0,\n",
       "   0.996835470199585,\n",
       "   0.9905063509941101,\n",
       "   1.0,\n",
       "   1.0,\n",
       "   0.9272152185440063,\n",
       "   1.0],\n",
       "  [0.9165048599243164,\n",
       "   0.7786407470703125,\n",
       "   0.968932032585144,\n",
       "   0.9941747784614563,\n",
       "   0.9922330379486084,\n",
       "   0.8213592171669006,\n",
       "   0.9941747784614563,\n",
       "   0.9980582594871521,\n",
       "   0.9961165189743042,\n",
       "   1.0,\n",
       "   0.9980582594871521,\n",
       "   0.9961165189743042,\n",
       "   1.0,\n",
       "   1.0,\n",
       "   0.9980582594871521,\n",
       "   1.0,\n",
       "   0.984466016292572,\n",
       "   1.0,\n",
       "   0.9941747784614563,\n",
       "   0.9980582594871521],\n",
       "  [0.8799019455909729,\n",
       "   0.9240196347236633,\n",
       "   0.938725471496582,\n",
       "   0.9166666865348816,\n",
       "   0.9828431606292725,\n",
       "   0.9901960492134094,\n",
       "   0.9803921580314636,\n",
       "   1.0,\n",
       "   0.9975489974021912,\n",
       "   0.9950980544090271,\n",
       "   1.0,\n",
       "   1.0,\n",
       "   0.9926470518112183,\n",
       "   1.0,\n",
       "   1.0,\n",
       "   0.9926470518112183,\n",
       "   0.9901960492134094,\n",
       "   1.0,\n",
       "   1.0,\n",
       "   1.0]],\n",
       " [[0.822257399559021,\n",
       "   0.8858499526977539,\n",
       "   0.9363292455673218,\n",
       "   0.9438923597335815,\n",
       "   0.9810922741889954,\n",
       "   0.9850496649742126,\n",
       "   0.9845220446586609,\n",
       "   0.9876880049705505,\n",
       "   0.9903262853622437,\n",
       "   0.9925248622894287,\n",
       "   0.9919092655181885,\n",
       "   0.9939319491386414,\n",
       "   0.9962184429168701,\n",
       "   0.9934922456741333,\n",
       "   0.9949872493743896,\n",
       "   0.9975376129150391,\n",
       "   0.9927886724472046,\n",
       "   0.9965702295303345,\n",
       "   0.9962184429168701,\n",
       "   0.9968340396881104],\n",
       "  [0.8191165924072266,\n",
       "   0.875134289264679,\n",
       "   0.9222162365913391,\n",
       "   0.9706408977508545,\n",
       "   0.9657178521156311,\n",
       "   0.9808449745178223,\n",
       "   0.9824561476707458,\n",
       "   0.9823666214942932,\n",
       "   0.9914070963859558,\n",
       "   0.9914070963859558,\n",
       "   0.9927497506141663,\n",
       "   0.9948084354400635,\n",
       "   0.9944503903388977,\n",
       "   0.9930182695388794,\n",
       "   0.9956140518188477,\n",
       "   0.9957035183906555,\n",
       "   0.9941818714141846,\n",
       "   0.9977622628211975,\n",
       "   0.9973146915435791,\n",
       "   0.997046172618866],\n",
       "  [0.8317912220954895,\n",
       "   0.8924111723899841,\n",
       "   0.9322329759597778,\n",
       "   0.9711815714836121,\n",
       "   0.9517073035240173,\n",
       "   0.9452449679374695,\n",
       "   0.9761592745780945,\n",
       "   0.9714435338973999,\n",
       "   0.9732774496078491,\n",
       "   0.981835663318634,\n",
       "   0.9852414727210999,\n",
       "   0.9887346029281616,\n",
       "   0.9870753884315491,\n",
       "   0.9912671446800232,\n",
       "   0.9910051226615906,\n",
       "   0.9931883811950684,\n",
       "   0.9944109916687012,\n",
       "   0.9930137395858765,\n",
       "   0.9937996864318848,\n",
       "   0.9954589009284973],\n",
       "  [0.8290851712226868,\n",
       "   0.9015204310417175,\n",
       "   0.9512408375740051,\n",
       "   0.9698532223701477,\n",
       "   0.9788535237312317,\n",
       "   0.9824362397193909,\n",
       "   0.9861936569213867,\n",
       "   0.9887276887893677,\n",
       "   0.9936211109161377,\n",
       "   0.9917860627174377,\n",
       "   0.9929220676422119,\n",
       "   0.9958056807518005,\n",
       "   0.9940580129623413,\n",
       "   0.9964173436164856,\n",
       "   0.9958056807518005,\n",
       "   0.9966794848442078,\n",
       "   0.9958056807518005,\n",
       "   0.9947570562362671,\n",
       "   0.9981649518013,\n",
       "   0.9983397126197815],\n",
       "  [0.8170327544212341,\n",
       "   0.8922056555747986,\n",
       "   0.9344505667686462,\n",
       "   0.9562712907791138,\n",
       "   0.9705856442451477,\n",
       "   0.9426551461219788,\n",
       "   0.9458845853805542,\n",
       "   0.9588024616241455,\n",
       "   0.9345378279685974,\n",
       "   0.9374181628227234,\n",
       "   0.946757435798645,\n",
       "   0.9640394449234009,\n",
       "   0.9728550314903259,\n",
       "   0.9736405611038208,\n",
       "   0.9780920147895813,\n",
       "   0.9803613424301147,\n",
       "   0.9819324612617493,\n",
       "   0.984463632106781,\n",
       "   0.9837653636932373,\n",
       "   0.985423743724823],\n",
       "  [0.8275740742683411,\n",
       "   0.884157657623291,\n",
       "   0.9368773698806763,\n",
       "   0.9675570726394653,\n",
       "   0.9664109945297241,\n",
       "   0.9805166125297546,\n",
       "   0.9881865382194519,\n",
       "   0.9910076856613159,\n",
       "   0.9914484620094299,\n",
       "   0.9936524629592896,\n",
       "   0.992682695388794,\n",
       "   0.996297299861908,\n",
       "   0.994710385799408,\n",
       "   0.9933879971504211,\n",
       "   0.996738076210022,\n",
       "   0.9962091445922852,\n",
       "   0.9963854551315308,\n",
       "   0.9970025420188904,\n",
       "   0.9955919981002808,\n",
       "   0.9970906972885132],\n",
       "  [0.8153846263885498,\n",
       "   0.8851641416549683,\n",
       "   0.9470459222793579,\n",
       "   0.9775054454803467,\n",
       "   0.9654266834259033,\n",
       "   0.983982503414154,\n",
       "   0.987483561038971,\n",
       "   0.990196943283081,\n",
       "   0.9900218844413757,\n",
       "   0.9924726486206055,\n",
       "   0.993698000907898,\n",
       "   0.9939606189727783,\n",
       "   0.9957111477851868,\n",
       "   0.9971991181373596,\n",
       "   0.9947483539581299,\n",
       "   0.9966739416122437,\n",
       "   0.9971991181373596,\n",
       "   0.9941356778144836,\n",
       "   0.9971991181373596,\n",
       "   0.9981619119644165],\n",
       "  [0.8215668797492981,\n",
       "   0.8785877823829651,\n",
       "   0.9476140141487122,\n",
       "   0.9301813840866089,\n",
       "   0.9572988152503967,\n",
       "   0.9542173147201538,\n",
       "   0.9777249693870544,\n",
       "   0.9848564863204956,\n",
       "   0.9867054224014282,\n",
       "   0.9904912710189819,\n",
       "   0.9928684830665588,\n",
       "   0.9916358590126038,\n",
       "   0.9923402070999146,\n",
       "   0.9933086633682251,\n",
       "   0.9933086633682251,\n",
       "   0.9919880032539368,\n",
       "   0.994893491268158,\n",
       "   0.9970945715904236,\n",
       "   0.9941891431808472,\n",
       "   0.994893491268158],\n",
       "  [0.8241360783576965,\n",
       "   0.903447687625885,\n",
       "   0.9567939043045044,\n",
       "   0.9621726274490356,\n",
       "   0.9619081020355225,\n",
       "   0.9237280488014221,\n",
       "   0.9747817516326904,\n",
       "   0.9857155680656433,\n",
       "   0.9888898730278015,\n",
       "   0.9876554012298584,\n",
       "   0.9923287034034729,\n",
       "   0.9925932288169861,\n",
       "   0.9923287034034729,\n",
       "   0.9953266978263855,\n",
       "   0.9946212768554688,\n",
       "   0.9942685961723328,\n",
       "   0.9948858022689819,\n",
       "   0.9960321187973022,\n",
       "   0.9973547458648682,\n",
       "   0.9971783757209778],\n",
       "  [0.8241969347000122,\n",
       "   0.8988813161849976,\n",
       "   0.8900541663169861,\n",
       "   0.9070966839790344,\n",
       "   0.9647789001464844,\n",
       "   0.9739556312561035,\n",
       "   0.9836567044258118,\n",
       "   0.9862785935401917,\n",
       "   0.9856668710708618,\n",
       "   0.988201379776001,\n",
       "   0.989250123500824,\n",
       "   0.9893375039100647,\n",
       "   0.9921342134475708,\n",
       "   0.9865407943725586,\n",
       "   0.9927459955215454,\n",
       "   0.9938821792602539,\n",
       "   0.9950183629989624,\n",
       "   0.9965041279792786,\n",
       "   0.99667888879776,\n",
       "   0.9944939613342285],\n",
       "  [0.8161919713020325,\n",
       "   0.8745929598808289,\n",
       "   0.8948341012001038,\n",
       "   0.9429727792739868,\n",
       "   0.9645339846611023,\n",
       "   0.9772067070007324,\n",
       "   0.9863592386245728,\n",
       "   0.9846871495246887,\n",
       "   0.9888233542442322,\n",
       "   0.9831030368804932,\n",
       "   0.984511137008667,\n",
       "   0.9932236075401306,\n",
       "   0.9948956966400146,\n",
       "   0.9933116436004639,\n",
       "   0.9943676590919495,\n",
       "   0.9951597452163696,\n",
       "   0.9951597452163696,\n",
       "   0.9953357577323914,\n",
       "   0.9956877827644348,\n",
       "   0.9952477216720581],\n",
       "  [0.8263494372367859,\n",
       "   0.8806752562522888,\n",
       "   0.954420268535614,\n",
       "   0.9715681672096252,\n",
       "   0.9772545695304871,\n",
       "   0.9811639189720154,\n",
       "   0.9845402240753174,\n",
       "   0.9860506653785706,\n",
       "   0.9910262227058411,\n",
       "   0.9892492294311523,\n",
       "   0.9901376962661743,\n",
       "   0.9906708002090454,\n",
       "   0.9922701120376587,\n",
       "   0.9922701120376587,\n",
       "   0.9915593266487122,\n",
       "   0.9945802092552185,\n",
       "   0.9941359162330627,\n",
       "   0.9944913387298584,\n",
       "   0.995735228061676,\n",
       "   0.995735228061676],\n",
       "  [0.8211902976036072,\n",
       "   0.8662331104278564,\n",
       "   0.9263491034507751,\n",
       "   0.9445421099662781,\n",
       "   0.9631745219230652,\n",
       "   0.9744243025779724,\n",
       "   0.984531581401825,\n",
       "   0.9872561097145081,\n",
       "   0.9883986711502075,\n",
       "   0.9904201030731201,\n",
       "   0.9944630265235901,\n",
       "   0.9924415349960327,\n",
       "   0.9951661229133606,\n",
       "   0.9963086843490601,\n",
       "   0.993144690990448,\n",
       "   0.994990348815918,\n",
       "   0.9958692193031311,\n",
       "   0.9959571361541748,\n",
       "   0.998242199420929,\n",
       "   0.9968360066413879],\n",
       "  [0.8201369643211365,\n",
       "   0.8851796388626099,\n",
       "   0.9657383561134338,\n",
       "   0.9831327199935913,\n",
       "   0.9831327199935913,\n",
       "   0.9913028478622437,\n",
       "   0.9904243350028992,\n",
       "   0.9912149906158447,\n",
       "   0.992620587348938,\n",
       "   0.9935869574546814,\n",
       "   0.9956074953079224,\n",
       "   0.9957832098007202,\n",
       "   0.9943775534629822,\n",
       "   0.9955196380615234,\n",
       "   0.9990336298942566,\n",
       "   0.9973644614219666,\n",
       "   0.9968373775482178,\n",
       "   0.9978037476539612,\n",
       "   0.9950804114341736,\n",
       "   0.9970130920410156],\n",
       "  [0.8281469941139221,\n",
       "   0.8884493708610535,\n",
       "   0.9186884760856628,\n",
       "   0.9211497902870178,\n",
       "   0.9633438587188721,\n",
       "   0.984968364238739,\n",
       "   0.9884845018386841,\n",
       "   0.9866385459899902,\n",
       "   0.9921765327453613,\n",
       "   0.9908579587936401,\n",
       "   0.99472576379776,\n",
       "   0.9960442781448364,\n",
       "   0.9949015378952026,\n",
       "   0.9970991611480713,\n",
       "   0.9956926703453064,\n",
       "   0.9969233274459839,\n",
       "   0.9981539845466614,\n",
       "   0.9961321949958801,\n",
       "   0.9970112442970276,\n",
       "   0.9977144598960876],\n",
       "  [0.8262216448783875,\n",
       "   0.8819881081581116,\n",
       "   0.9106051325798035,\n",
       "   0.9591565728187561,\n",
       "   0.972623348236084,\n",
       "   0.9759900569915771,\n",
       "   0.9826349020004272,\n",
       "   0.9836980700492859,\n",
       "   0.9863559603691101,\n",
       "   0.9912288188934326,\n",
       "   0.9886595010757446,\n",
       "   0.9921148419380188,\n",
       "   0.9905200600624084,\n",
       "   0.9907858371734619,\n",
       "   0.9930893778800964,\n",
       "   0.9941525459289551,\n",
       "   0.994861364364624,\n",
       "   0.9944183826446533,\n",
       "   0.9958359003067017,\n",
       "   0.9964560866355896],\n",
       "  [0.8220608830451965,\n",
       "   0.8926293253898621,\n",
       "   0.9411764740943909,\n",
       "   0.9663359522819519,\n",
       "   0.9713855385780334,\n",
       "   0.9793586134910583,\n",
       "   0.9773210287094116,\n",
       "   0.9788270592689514,\n",
       "   0.9900779724121094,\n",
       "   0.9918497800827026,\n",
       "   0.9919383525848389,\n",
       "   0.9927356243133545,\n",
       "   0.9926470518112183,\n",
       "   0.9932671785354614,\n",
       "   0.9955705404281616,\n",
       "   0.9961020350456238,\n",
       "   0.9953933358192444,\n",
       "   0.9948617815971375,\n",
       "   0.9972537159919739,\n",
       "   0.9956591129302979],\n",
       "  [0.8330718278884888,\n",
       "   0.8894897699356079,\n",
       "   0.9305713176727295,\n",
       "   0.9553423523902893,\n",
       "   0.9708678722381592,\n",
       "   0.9704317450523376,\n",
       "   0.985521137714386,\n",
       "   0.9888355731964111,\n",
       "   0.9883122444152832,\n",
       "   0.9937199950218201,\n",
       "   0.9935455918312073,\n",
       "   0.994243323802948,\n",
       "   0.9950283765792847,\n",
       "   0.9909288883209229,\n",
       "   0.9907544851303101,\n",
       "   0.9968600273132324,\n",
       "   0.9935455918312073,\n",
       "   0.9969472289085388,\n",
       "   0.9962494373321533,\n",
       "   0.9977322220802307],\n",
       "  [0.8246696591377258,\n",
       "   0.8853685855865479,\n",
       "   0.9236661195755005,\n",
       "   0.9601357579231262,\n",
       "   0.9812864661216736,\n",
       "   0.9892941117286682,\n",
       "   0.9892941117286682,\n",
       "   0.9904256463050842,\n",
       "   0.9940813183784485,\n",
       "   0.9928627610206604,\n",
       "   0.9952127933502197,\n",
       "   0.9939072132110596,\n",
       "   0.9967795014381409,\n",
       "   0.9966054558753967,\n",
       "   0.9974758625030518,\n",
       "   0.9967795014381409,\n",
       "   0.9958220720291138,\n",
       "   0.9977369904518127,\n",
       "   0.9969536066055298,\n",
       "   0.9977369904518127],\n",
       "  [0.8226805925369263,\n",
       "   0.8817070722579956,\n",
       "   0.9430671334266663,\n",
       "   0.9556401371955872,\n",
       "   0.9800779223442078,\n",
       "   0.9899061322212219,\n",
       "   0.9914999008178711,\n",
       "   0.993802011013031,\n",
       "   0.994598925113678,\n",
       "   0.9928280711174011,\n",
       "   0.9934478402137756,\n",
       "   0.9949530959129333,\n",
       "   0.9988489747047424,\n",
       "   0.9949530959129333,\n",
       "   0.9958385229110718,\n",
       "   0.9969895482063293,\n",
       "   0.9988489747047424,\n",
       "   0.995130181312561,\n",
       "   0.9973437190055847,\n",
       "   0.9964582920074463],\n",
       "  [0.8252959847450256,\n",
       "   0.8836601376533508,\n",
       "   0.8849673271179199,\n",
       "   0.9545097947120667,\n",
       "   0.9780392050743103,\n",
       "   0.9865795373916626,\n",
       "   0.9897167682647705,\n",
       "   0.9872766733169556,\n",
       "   0.991808295249939,\n",
       "   0.9925054311752319,\n",
       "   0.9948583841323853,\n",
       "   0.9922440052032471,\n",
       "   0.9956427216529846,\n",
       "   0.9947712421417236,\n",
       "   0.9958170056343079,\n",
       "   0.9970370531082153,\n",
       "   0.9961655735969543,\n",
       "   0.9963398575782776,\n",
       "   0.9957298636436462,\n",
       "   0.9967756271362305],\n",
       "  [0.8254432678222656,\n",
       "   0.8933546543121338,\n",
       "   0.9325703382492065,\n",
       "   0.9577677249908447,\n",
       "   0.974270224571228,\n",
       "   0.9815455675125122,\n",
       "   0.9872238636016846,\n",
       "   0.988998293876648,\n",
       "   0.9903291463851929,\n",
       "   0.9913051128387451,\n",
       "   0.995031476020813,\n",
       "   0.994499146938324,\n",
       "   0.9945878982543945,\n",
       "   0.9957413077354431,\n",
       "   0.9930796027183533,\n",
       "   0.9971608519554138,\n",
       "   0.9942330121994019,\n",
       "   0.9961848855018616,\n",
       "   0.9966285228729248,\n",
       "   0.9975157380104065],\n",
       "  [0.8248251676559448,\n",
       "   0.8743547201156616,\n",
       "   0.8943914771080017,\n",
       "   0.9265027642250061,\n",
       "   0.9601889848709106,\n",
       "   0.9746259450912476,\n",
       "   0.9842506051063538,\n",
       "   0.9835506081581116,\n",
       "   0.9858255386352539,\n",
       "   0.9886254072189331,\n",
       "   0.9888004064559937,\n",
       "   0.9911628365516663,\n",
       "   0.9935252666473389,\n",
       "   0.9944002032279968,\n",
       "   0.992912769317627,\n",
       "   0.993262767791748,\n",
       "   0.9942252039909363,\n",
       "   0.9934377670288086,\n",
       "   0.9968501329421997,\n",
       "   0.9953626990318298],\n",
       "  [0.8285509347915649,\n",
       "   0.8796662092208862,\n",
       "   0.9396985173225403,\n",
       "   0.9422110319137573,\n",
       "   0.9803481698036194,\n",
       "   0.9814249873161316,\n",
       "   0.9861809015274048,\n",
       "   0.989142119884491,\n",
       "   0.9700286984443665,\n",
       "   0.9941672682762146,\n",
       "   0.9954235553741455,\n",
       "   0.994615912437439,\n",
       "   0.9967695474624634,\n",
       "   0.9950646162033081,\n",
       "   0.9938083291053772,\n",
       "   0.9961414337158203,\n",
       "   0.996320903301239,\n",
       "   0.9961414337158203,\n",
       "   0.9976668953895569,\n",
       "   0.9977566599845886],\n",
       "  [0.8242077231407166,\n",
       "   0.8917180895805359,\n",
       "   0.9386784434318542,\n",
       "   0.9704845547676086,\n",
       "   0.9817621111869812,\n",
       "   0.9873127937316895,\n",
       "   0.991806149482727,\n",
       "   0.9896916151046753,\n",
       "   0.9813215732574463,\n",
       "   0.9872246980667114,\n",
       "   0.99101322889328,\n",
       "   0.9926872253417969,\n",
       "   0.9932158589363098,\n",
       "   0.9948898553848267,\n",
       "   0.991629958152771,\n",
       "   0.9960352182388306,\n",
       "   0.9960352182388306,\n",
       "   0.992951512336731,\n",
       "   0.9966520071029663,\n",
       "   0.9971806406974792],\n",
       "  [0.8254551887512207,\n",
       "   0.8928102254867554,\n",
       "   0.9281898736953735,\n",
       "   0.9708380699157715,\n",
       "   0.9846746921539307,\n",
       "   0.9888781905174255,\n",
       "   0.9907172322273254,\n",
       "   0.9511340856552124,\n",
       "   0.9876521825790405,\n",
       "   0.9922935366630554,\n",
       "   0.9941325783729553,\n",
       "   0.9917681217193604,\n",
       "   0.9959716200828552,\n",
       "   0.9951834678649902,\n",
       "   0.9927313923835754,\n",
       "   0.9955337643623352,\n",
       "   0.9949207305908203,\n",
       "   0.9952710270881653,\n",
       "   0.9971976280212402,\n",
       "   0.9972852468490601],\n",
       "  [0.8259116411209106,\n",
       "   0.8705149292945862,\n",
       "   0.9385033845901489,\n",
       "   0.963154673576355,\n",
       "   0.9839459657669067,\n",
       "   0.9883323311805725,\n",
       "   0.9904377460479736,\n",
       "   0.9914027452468872,\n",
       "   0.9913150072097778,\n",
       "   0.9938591122627258,\n",
       "   0.9926309585571289,\n",
       "   0.9927186369895935,\n",
       "   0.9943854808807373,\n",
       "   0.9949995875358582,\n",
       "   0.9939468502998352,\n",
       "   0.9978068470954895,\n",
       "   0.9950872659683228,\n",
       "   0.998508632183075,\n",
       "   0.9949118494987488,\n",
       "   0.9970172643661499],\n",
       "  [0.8259615302085876,\n",
       "   0.8888014554977417,\n",
       "   0.9331235289573669,\n",
       "   0.9579508900642395,\n",
       "   0.9690532684326172,\n",
       "   0.9744732975959778,\n",
       "   0.9602237939834595,\n",
       "   0.9610980153083801,\n",
       "   0.9887228012084961,\n",
       "   0.9913454055786133,\n",
       "   0.9912579655647278,\n",
       "   0.9942302703857422,\n",
       "   0.9937057495117188,\n",
       "   0.995716392993927,\n",
       "   0.9948422312736511,\n",
       "   0.9932686686515808,\n",
       "   0.9961535334587097,\n",
       "   0.9961535334587097,\n",
       "   0.9958038330078125,\n",
       "   0.9961535334587097],\n",
       "  [0.8221905827522278,\n",
       "   0.8883451819419861,\n",
       "   0.9421707987785339,\n",
       "   0.9629003405570984,\n",
       "   0.9657473564147949,\n",
       "   0.9809608459472656,\n",
       "   0.9826512336730957,\n",
       "   0.9893238544464111,\n",
       "   0.99314945936203,\n",
       "   0.9940391182899475,\n",
       "   0.9928825497627258,\n",
       "   0.995195746421814,\n",
       "   0.9960854053497314,\n",
       "   0.9952846765518188,\n",
       "   0.9984875321388245,\n",
       "   0.9964413046836853,\n",
       "   0.9967971444129944,\n",
       "   0.9975088834762573,\n",
       "   0.9976868033409119,\n",
       "   0.9959964156150818],\n",
       "  [0.8225352168083191,\n",
       "   0.8806732892990112,\n",
       "   0.9276461005210876,\n",
       "   0.9577862024307251,\n",
       "   0.9658059477806091,\n",
       "   0.9838724136352539,\n",
       "   0.9889838695526123,\n",
       "   0.9836961030960083,\n",
       "   0.9907464385032654,\n",
       "   0.9896007776260376,\n",
       "   0.9926853179931641,\n",
       "   0.9938309788703918,\n",
       "   0.9917158484458923,\n",
       "   0.9950647950172424,\n",
       "   0.9933021664619446,\n",
       "   0.9949766397476196,\n",
       "   0.9960342049598694,\n",
       "   0.9955935478210449,\n",
       "   0.9953291416168213,\n",
       "   0.996386706829071]],\n",
       " [[0.45123180747032166,\n",
       "   0.2559323012828827,\n",
       "   0.21231985092163086,\n",
       "   0.12776745855808258,\n",
       "   0.1288435012102127,\n",
       "   0.4076753556728363,\n",
       "   0.08299452066421509,\n",
       "   0.15353326499462128,\n",
       "   0.10811012238264084,\n",
       "   0.10045880824327469,\n",
       "   0.17523981630802155,\n",
       "   0.06719016283750534,\n",
       "   0.06902032345533371,\n",
       "   0.050915494561195374,\n",
       "   0.06456317007541656,\n",
       "   0.048200178891420364,\n",
       "   0.04319436848163605,\n",
       "   0.16738469898700714,\n",
       "   0.03838789835572243,\n",
       "   0.0380060151219368],\n",
       "  [0.4477689862251282,\n",
       "   0.37271377444267273,\n",
       "   0.10419868677854538,\n",
       "   0.12366016954183578,\n",
       "   0.1905948668718338,\n",
       "   0.5910208225250244,\n",
       "   0.10601344704627991,\n",
       "   0.07271093875169754,\n",
       "   0.3297687768936157,\n",
       "   0.0659468024969101,\n",
       "   0.07666902989149094,\n",
       "   0.06113750487565994,\n",
       "   0.0719573050737381,\n",
       "   0.05136478692293167,\n",
       "   0.05680285766720772,\n",
       "   0.07661453634500504,\n",
       "   0.08677313476800919,\n",
       "   0.04598980396986008,\n",
       "   0.054260022938251495,\n",
       "   0.04251750186085701],\n",
       "  [0.328244149684906,\n",
       "   0.31275758147239685,\n",
       "   0.16915015876293182,\n",
       "   0.139853835105896,\n",
       "   0.30573388934135437,\n",
       "   0.11136115342378616,\n",
       "   0.0977523997426033,\n",
       "   0.11545883864164352,\n",
       "   0.1288711577653885,\n",
       "   0.1749439537525177,\n",
       "   0.09282829612493515,\n",
       "   0.1279325932264328,\n",
       "   0.19552282989025116,\n",
       "   0.08193990588188171,\n",
       "   0.16917553544044495,\n",
       "   0.09200260043144226,\n",
       "   0.07985949516296387,\n",
       "   0.13662827014923096,\n",
       "   0.09318841248750687,\n",
       "   0.05149421468377113],\n",
       "  [1.2780518531799316,\n",
       "   0.3180205821990967,\n",
       "   0.30991384387016296,\n",
       "   0.24671143293380737,\n",
       "   0.44654160737991333,\n",
       "   0.7251163125038147,\n",
       "   0.21367572247982025,\n",
       "   0.32629668712615967,\n",
       "   0.28130581974983215,\n",
       "   0.2951350212097168,\n",
       "   0.16001978516578674,\n",
       "   0.2408737689256668,\n",
       "   0.14719215035438538,\n",
       "   0.36332806944847107,\n",
       "   0.16986361145973206,\n",
       "   0.3875897526741028,\n",
       "   0.45707932114601135,\n",
       "   0.43517717719078064,\n",
       "   0.19362691044807434,\n",
       "   0.3428596556186676],\n",
       "  [0.5245310068130493,\n",
       "   0.273475706577301,\n",
       "   0.17921718955039978,\n",
       "   0.16565269231796265,\n",
       "   0.18516626954078674,\n",
       "   0.27094802260398865,\n",
       "   0.2566245496273041,\n",
       "   0.3904758393764496,\n",
       "   0.28666168451309204,\n",
       "   0.1885998547077179,\n",
       "   0.11509253084659576,\n",
       "   0.10333061963319778,\n",
       "   0.15941494703292847,\n",
       "   0.08646263182163239,\n",
       "   0.11690628528594971,\n",
       "   0.07085800915956497,\n",
       "   0.0988789051771164,\n",
       "   0.07051331549882889,\n",
       "   0.07646837085485458,\n",
       "   0.06372711062431335],\n",
       "  [0.43964114785194397,\n",
       "   0.2354135662317276,\n",
       "   0.10063046216964722,\n",
       "   0.11729861050844193,\n",
       "   0.23314256966114044,\n",
       "   0.10166008770465851,\n",
       "   0.0837799459695816,\n",
       "   0.07750819623470306,\n",
       "   0.16987793147563934,\n",
       "   0.06310715526342392,\n",
       "   0.061242301017045975,\n",
       "   0.061943333595991135,\n",
       "   0.06068878248333931,\n",
       "   0.04981321841478348,\n",
       "   0.04755628854036331,\n",
       "   0.07721709460020065,\n",
       "   0.04446797072887421,\n",
       "   0.08130461722612381,\n",
       "   0.05136046186089516,\n",
       "   0.03915112838149071],\n",
       "  [0.30101585388183594,\n",
       "   0.42830756306648254,\n",
       "   0.17142823338508606,\n",
       "   0.17413581907749176,\n",
       "   0.14342913031578064,\n",
       "   0.09631945937871933,\n",
       "   0.10390161722898483,\n",
       "   0.07964686304330826,\n",
       "   0.08590907603502274,\n",
       "   0.09154713153839111,\n",
       "   0.0829564779996872,\n",
       "   0.060615602880716324,\n",
       "   0.05471770092844963,\n",
       "   0.058732591569423676,\n",
       "   0.046884045004844666,\n",
       "   0.04326595366001129,\n",
       "   0.040560122579336166,\n",
       "   0.04173055291175842,\n",
       "   0.10733166337013245,\n",
       "   0.0347958467900753],\n",
       "  [0.3700293302536011,\n",
       "   0.36218515038490295,\n",
       "   0.22457154095172882,\n",
       "   0.8320321440696716,\n",
       "   0.20456582307815552,\n",
       "   0.1835140585899353,\n",
       "   0.26420772075653076,\n",
       "   0.23579180240631104,\n",
       "   0.19601982831954956,\n",
       "   0.19438286125659943,\n",
       "   0.17432624101638794,\n",
       "   0.24853143095970154,\n",
       "   0.3580520451068878,\n",
       "   0.21298468112945557,\n",
       "   0.18363913893699646,\n",
       "   0.17988020181655884,\n",
       "   0.15462438762187958,\n",
       "   0.24270039796829224,\n",
       "   0.1448790729045868,\n",
       "   0.2959446310997009],\n",
       "  [0.6806465983390808,\n",
       "   0.3965139389038086,\n",
       "   0.22577688097953796,\n",
       "   0.21106986701488495,\n",
       "   0.6003347039222717,\n",
       "   0.18636812269687653,\n",
       "   0.1683821976184845,\n",
       "   0.10310584306716919,\n",
       "   0.09357095509767532,\n",
       "   0.21904881298542023,\n",
       "   0.12911508977413177,\n",
       "   0.07686474174261093,\n",
       "   0.10705544054508209,\n",
       "   0.0908319428563118,\n",
       "   0.06395165622234344,\n",
       "   0.06576628237962723,\n",
       "   0.11409923434257507,\n",
       "   0.12509730458259583,\n",
       "   0.04825913533568382,\n",
       "   0.0647934228181839],\n",
       "  [0.2780594527721405,\n",
       "   0.1408068686723709,\n",
       "   0.23981541395187378,\n",
       "   0.10520900040864944,\n",
       "   0.08317364752292633,\n",
       "   0.07947002351284027,\n",
       "   0.11147037893533707,\n",
       "   0.12249228358268738,\n",
       "   0.07764630019664764,\n",
       "   0.06403765082359314,\n",
       "   0.07508209347724915,\n",
       "   0.05967484042048454,\n",
       "   0.07210512459278107,\n",
       "   0.05386431887745857,\n",
       "   0.05056018754839897,\n",
       "   0.05175791680812836,\n",
       "   0.05695806071162224,\n",
       "   0.07283515483140945,\n",
       "   0.05748182162642479,\n",
       "   0.042038291692733765],\n",
       "  [0.274421751499176,\n",
       "   0.18908917903900146,\n",
       "   0.1868092268705368,\n",
       "   0.13582880795001984,\n",
       "   0.09245277941226959,\n",
       "   0.08298393338918686,\n",
       "   0.20643176138401031,\n",
       "   0.10746555030345917,\n",
       "   0.09995085000991821,\n",
       "   0.08856465667486191,\n",
       "   0.07396328449249268,\n",
       "   0.06585054844617844,\n",
       "   0.0643598884344101,\n",
       "   0.4404321014881134,\n",
       "   0.060805708169937134,\n",
       "   0.061263736337423325,\n",
       "   0.04825693368911743,\n",
       "   0.1024845540523529,\n",
       "   0.053600914776325226,\n",
       "   0.06216421350836754],\n",
       "  [0.5234061479568481,\n",
       "   0.22909152507781982,\n",
       "   0.162372887134552,\n",
       "   0.11184199154376984,\n",
       "   0.15321293473243713,\n",
       "   0.11534443497657776,\n",
       "   0.08011732250452042,\n",
       "   0.0727725401520729,\n",
       "   0.06764295697212219,\n",
       "   0.07052107155323029,\n",
       "   0.06199861317873001,\n",
       "   0.06715703010559082,\n",
       "   0.06199920177459717,\n",
       "   0.05195039510726929,\n",
       "   0.05693020299077034,\n",
       "   0.06409721076488495,\n",
       "   0.050682179629802704,\n",
       "   0.04535325616598129,\n",
       "   0.04449430853128433,\n",
       "   0.0402592197060585],\n",
       "  [0.38767415285110474,\n",
       "   0.547325849533081,\n",
       "   0.31060463190078735,\n",
       "   0.3036372661590576,\n",
       "   0.23451566696166992,\n",
       "   0.19965039193630219,\n",
       "   0.2882905900478363,\n",
       "   0.16829799115657806,\n",
       "   0.21120601892471313,\n",
       "   0.2836003005504608,\n",
       "   0.12010622769594193,\n",
       "   0.16607998311519623,\n",
       "   0.1353464275598526,\n",
       "   0.15252050757408142,\n",
       "   0.09632647782564163,\n",
       "   0.1006607711315155,\n",
       "   0.13234344124794006,\n",
       "   0.07922157645225525,\n",
       "   0.12396243959665298,\n",
       "   0.28240805864334106],\n",
       "  [0.3640885353088379,\n",
       "   0.31090423464775085,\n",
       "   0.10296003520488739,\n",
       "   0.10112480819225311,\n",
       "   0.0837164968252182,\n",
       "   0.0798167735338211,\n",
       "   0.06971969455480576,\n",
       "   0.06427895277738571,\n",
       "   0.06113202124834061,\n",
       "   0.0655442550778389,\n",
       "   0.05906908959150314,\n",
       "   0.06623219698667526,\n",
       "   0.06741484999656677,\n",
       "   0.044173937290906906,\n",
       "   0.043501824140548706,\n",
       "   0.04078513756394386,\n",
       "   0.047070734202861786,\n",
       "   0.039682649075984955,\n",
       "   0.03469187021255493,\n",
       "   0.04707343131303787],\n",
       "  [0.5057570934295654,\n",
       "   1.010650634765625,\n",
       "   0.2784731388092041,\n",
       "   0.26492172479629517,\n",
       "   0.24381811916828156,\n",
       "   0.24813127517700195,\n",
       "   0.12427683174610138,\n",
       "   0.11027052998542786,\n",
       "   0.31249895691871643,\n",
       "   0.32205769419670105,\n",
       "   0.19296051561832428,\n",
       "   0.17311127483844757,\n",
       "   0.11166932433843613,\n",
       "   0.09246454387903214,\n",
       "   0.09222151339054108,\n",
       "   0.059775181114673615,\n",
       "   0.18830876052379608,\n",
       "   0.05216626450419426,\n",
       "   0.09169545024633408,\n",
       "   0.0469830296933651],\n",
       "  [0.42780303955078125,\n",
       "   0.5583546161651611,\n",
       "   0.41958698630332947,\n",
       "   0.1667085587978363,\n",
       "   0.11916519701480865,\n",
       "   0.1338254064321518,\n",
       "   0.16490662097930908,\n",
       "   0.19172437489032745,\n",
       "   0.14072178304195404,\n",
       "   0.12077778577804565,\n",
       "   0.0896589457988739,\n",
       "   0.11193270236253738,\n",
       "   0.13098107278347015,\n",
       "   0.15178333222866058,\n",
       "   0.5843123197555542,\n",
       "   0.06170022115111351,\n",
       "   0.04887114092707634,\n",
       "   0.05222281068563461,\n",
       "   0.10473328083753586,\n",
       "   0.06903015077114105],\n",
       "  [0.411409854888916,\n",
       "   0.27278804779052734,\n",
       "   0.3317328095436096,\n",
       "   0.09290965646505356,\n",
       "   0.0917205810546875,\n",
       "   0.22910310328006744,\n",
       "   0.08250242471694946,\n",
       "   0.07276618480682373,\n",
       "   0.07824234664440155,\n",
       "   0.07226159423589706,\n",
       "   0.06447400897741318,\n",
       "   0.05727813020348549,\n",
       "   0.06475909054279327,\n",
       "   0.05382856726646423,\n",
       "   0.047858819365501404,\n",
       "   0.045042458921670914,\n",
       "   0.04203392565250397,\n",
       "   0.04128279164433479,\n",
       "   0.035825613886117935,\n",
       "   0.03913619741797447],\n",
       "  [1.9536852836608887,\n",
       "   1.4152668714523315,\n",
       "   0.40226981043815613,\n",
       "   0.1048647090792656,\n",
       "   0.11105747520923615,\n",
       "   0.1456063687801361,\n",
       "   0.09647423774003983,\n",
       "   0.0788651630282402,\n",
       "   0.09165597707033157,\n",
       "   0.06652183830738068,\n",
       "   0.06410897523164749,\n",
       "   0.06355205923318863,\n",
       "   0.07566846162080765,\n",
       "   0.05464523285627365,\n",
       "   0.05425095558166504,\n",
       "   0.06256100535392761,\n",
       "   0.048798203468322754,\n",
       "   0.10035481303930283,\n",
       "   0.047711003571748734,\n",
       "   0.0893726646900177],\n",
       "  [0.6285485625267029,\n",
       "   0.33254143595695496,\n",
       "   0.6031203269958496,\n",
       "   0.12639307975769043,\n",
       "   0.21867002546787262,\n",
       "   0.20166577398777008,\n",
       "   0.11984190344810486,\n",
       "   0.07799860835075378,\n",
       "   0.0879485085606575,\n",
       "   0.15828265249729156,\n",
       "   0.11795077472925186,\n",
       "   0.05179478973150253,\n",
       "   0.1065557673573494,\n",
       "   0.051738154143095016,\n",
       "   0.0402563102543354,\n",
       "   0.10213854908943176,\n",
       "   0.040639735758304596,\n",
       "   0.22019056975841522,\n",
       "   0.03887921944260597,\n",
       "   0.032047804445028305],\n",
       "  [0.586081862449646,\n",
       "   0.44847777485847473,\n",
       "   0.31677624583244324,\n",
       "   0.21027633547782898,\n",
       "   0.10117066651582718,\n",
       "   0.14378808438777924,\n",
       "   0.08975283801555634,\n",
       "   0.10887489467859268,\n",
       "   0.09115771949291229,\n",
       "   0.1039285808801651,\n",
       "   0.28732120990753174,\n",
       "   0.08900303393602371,\n",
       "   0.11752081662416458,\n",
       "   0.17408835887908936,\n",
       "   0.0960911214351654,\n",
       "   0.051584452390670776,\n",
       "   0.04160996153950691,\n",
       "   0.051705874502658844,\n",
       "   0.036404818296432495,\n",
       "   0.03491256758570671],\n",
       "  [0.5352563261985779,\n",
       "   0.35104724764823914,\n",
       "   0.33358508348464966,\n",
       "   0.19926650822162628,\n",
       "   0.12269672751426697,\n",
       "   0.13433071970939636,\n",
       "   0.13085338473320007,\n",
       "   0.10739077627658844,\n",
       "   0.10406385362148285,\n",
       "   0.08882635086774826,\n",
       "   0.10825163125991821,\n",
       "   0.09658800065517426,\n",
       "   0.10186298191547394,\n",
       "   0.08829105645418167,\n",
       "   0.08297793567180634,\n",
       "   0.07149410992860794,\n",
       "   0.07826109230518341,\n",
       "   0.07620435208082199,\n",
       "   0.07350301742553711,\n",
       "   0.0723748654127121],\n",
       "  [0.529363751411438,\n",
       "   0.3258753716945648,\n",
       "   0.23038063943386078,\n",
       "   0.15473207831382751,\n",
       "   0.15485402941703796,\n",
       "   0.1227869912981987,\n",
       "   0.12484918534755707,\n",
       "   0.11664126068353653,\n",
       "   0.10955069959163666,\n",
       "   0.17260053753852844,\n",
       "   0.10889328271150589,\n",
       "   0.09014829248189926,\n",
       "   0.07017076760530472,\n",
       "   0.11202891916036606,\n",
       "   0.0651276707649231,\n",
       "   0.07570480555295944,\n",
       "   0.09872008860111237,\n",
       "   0.052216071635484695,\n",
       "   0.10305999964475632,\n",
       "   0.07503599673509598],\n",
       "  [0.4206724166870117,\n",
       "   0.5636643171310425,\n",
       "   0.2611433267593384,\n",
       "   0.18738315999507904,\n",
       "   0.20385053753852844,\n",
       "   0.2365931123495102,\n",
       "   0.24209702014923096,\n",
       "   0.10267186164855957,\n",
       "   0.16454213857650757,\n",
       "   0.1470322161912918,\n",
       "   0.21484674513339996,\n",
       "   0.14537742733955383,\n",
       "   0.06993971765041351,\n",
       "   0.1079331636428833,\n",
       "   0.06037808582186699,\n",
       "   0.07720165699720383,\n",
       "   0.2241416871547699,\n",
       "   0.15565559267997742,\n",
       "   0.07354451715946198,\n",
       "   0.04555874690413475],\n",
       "  [0.47954294085502625,\n",
       "   0.4243514835834503,\n",
       "   0.2926813066005707,\n",
       "   0.1866724193096161,\n",
       "   0.15422503650188446,\n",
       "   0.16713373363018036,\n",
       "   0.10591454058885574,\n",
       "   0.15611545741558075,\n",
       "   0.09145121276378632,\n",
       "   0.13664813339710236,\n",
       "   0.11953017115592957,\n",
       "   0.12787753343582153,\n",
       "   0.3433067798614502,\n",
       "   0.06713476032018661,\n",
       "   0.14162012934684753,\n",
       "   0.09815114736557007,\n",
       "   0.24036668241024017,\n",
       "   0.12866660952568054,\n",
       "   0.10348154604434967,\n",
       "   0.1082293689250946],\n",
       "  [0.4104940593242645,\n",
       "   0.2058364301919937,\n",
       "   0.1933317631483078,\n",
       "   0.15603196620941162,\n",
       "   0.13582460582256317,\n",
       "   0.1743531972169876,\n",
       "   0.12018387764692307,\n",
       "   0.11437232047319412,\n",
       "   0.12106181681156158,\n",
       "   0.09452212601900101,\n",
       "   0.1310950219631195,\n",
       "   0.1454511135816574,\n",
       "   0.09922368824481964,\n",
       "   0.17409171164035797,\n",
       "   0.08149848133325577,\n",
       "   0.08560869097709656,\n",
       "   0.11564067006111145,\n",
       "   0.10723026096820831,\n",
       "   0.09095600247383118,\n",
       "   0.0940437912940979],\n",
       "  [0.3852408826351166,\n",
       "   0.4173724353313446,\n",
       "   0.3286435008049011,\n",
       "   0.2642901837825775,\n",
       "   0.08428313583135605,\n",
       "   0.12912869453430176,\n",
       "   0.07928481698036194,\n",
       "   0.09484640508890152,\n",
       "   0.13654763996601105,\n",
       "   0.1474892497062683,\n",
       "   0.07686638832092285,\n",
       "   0.0763804093003273,\n",
       "   0.060875944793224335,\n",
       "   0.0987134799361229,\n",
       "   0.05343543738126755,\n",
       "   0.06665225327014923,\n",
       "   0.08687277883291245,\n",
       "   0.07177148014307022,\n",
       "   0.07090431451797485,\n",
       "   0.11370639503002167],\n",
       "  [0.21344077587127686,\n",
       "   0.7245103716850281,\n",
       "   0.11113705486059189,\n",
       "   0.10327677428722382,\n",
       "   0.08728359639644623,\n",
       "   0.08409799635410309,\n",
       "   0.09391060471534729,\n",
       "   0.07072144746780396,\n",
       "   0.07142750918865204,\n",
       "   0.0780947208404541,\n",
       "   0.11056268960237503,\n",
       "   0.052858106791973114,\n",
       "   0.050032030791044235,\n",
       "   0.05642551928758621,\n",
       "   0.06458529084920883,\n",
       "   0.05258667469024658,\n",
       "   0.09072355926036835,\n",
       "   0.03831261023879051,\n",
       "   0.059153150767087936,\n",
       "   0.04066399484872818],\n",
       "  [0.38726839423179626,\n",
       "   0.328685462474823,\n",
       "   0.20642615854740143,\n",
       "   0.16166041791439056,\n",
       "   0.12233716249465942,\n",
       "   0.19709032773971558,\n",
       "   0.18765905499458313,\n",
       "   0.0913618877530098,\n",
       "   0.09786956757307053,\n",
       "   0.10150656849145889,\n",
       "   0.07700824737548828,\n",
       "   0.06375731527805328,\n",
       "   0.06292909383773804,\n",
       "   0.05429239943623543,\n",
       "   0.06491333246231079,\n",
       "   0.061639368534088135,\n",
       "   0.048007335513830185,\n",
       "   0.04852817580103874,\n",
       "   0.18061552941799164,\n",
       "   0.03984011709690094],\n",
       "  [0.31494590640068054,\n",
       "   0.6087268590927124,\n",
       "   0.165750190615654,\n",
       "   0.10715347528457642,\n",
       "   0.10267741233110428,\n",
       "   0.6319550275802612,\n",
       "   0.08967778086662292,\n",
       "   0.07716614007949829,\n",
       "   0.0758373811841011,\n",
       "   0.06739701330661774,\n",
       "   0.06372920423746109,\n",
       "   0.06910332292318344,\n",
       "   0.05417563021183014,\n",
       "   0.04977446421980858,\n",
       "   0.04698260501027107,\n",
       "   0.04319620877504349,\n",
       "   0.08900720626115799,\n",
       "   0.03760688006877899,\n",
       "   0.06024360656738281,\n",
       "   0.039552826434373856],\n",
       "  [0.33308038115501404,\n",
       "   0.29226431250572205,\n",
       "   0.19625309109687805,\n",
       "   0.3888707756996155,\n",
       "   0.11349973827600479,\n",
       "   0.09735530614852905,\n",
       "   0.1157967746257782,\n",
       "   0.07583639770746231,\n",
       "   0.07451295852661133,\n",
       "   0.0857386663556099,\n",
       "   0.06197747215628624,\n",
       "   0.06342083215713501,\n",
       "   0.0690658837556839,\n",
       "   0.05220741033554077,\n",
       "   0.05597851425409317,\n",
       "   0.06482178717851639,\n",
       "   0.06655796617269516,\n",
       "   0.046590764075517654,\n",
       "   0.04007112234830856,\n",
       "   0.03945131227374077]],\n",
       " [[0.519737720489502,\n",
       "   0.3699319362640381,\n",
       "   0.25691238045692444,\n",
       "   0.2384207397699356,\n",
       "   0.14138175547122955,\n",
       "   0.12633676826953888,\n",
       "   0.11867620050907135,\n",
       "   0.10881313681602478,\n",
       "   0.09628481417894363,\n",
       "   0.0870497077703476,\n",
       "   0.08548998832702637,\n",
       "   0.07953688502311707,\n",
       "   0.06944731622934341,\n",
       "   0.0745168924331665,\n",
       "   0.06434450298547745,\n",
       "   0.05384725704789162,\n",
       "   0.06576428562402725,\n",
       "   0.050261545926332474,\n",
       "   0.04954294487833977,\n",
       "   0.046666327863931656],\n",
       "  [0.5278257727622986,\n",
       "   0.4001193344593048,\n",
       "   0.28072425723075867,\n",
       "   0.1675209254026413,\n",
       "   0.17132630944252014,\n",
       "   0.1339898705482483,\n",
       "   0.12635032832622528,\n",
       "   0.1191779375076294,\n",
       "   0.0932467058300972,\n",
       "   0.09005662798881531,\n",
       "   0.0826847031712532,\n",
       "   0.07684920728206635,\n",
       "   0.070462666451931,\n",
       "   0.07250320166349411,\n",
       "   0.06156562268733978,\n",
       "   0.05632263794541359,\n",
       "   0.05920349434018135,\n",
       "   0.04873047024011612,\n",
       "   0.04667822644114494,\n",
       "   0.04699940234422684],\n",
       "  [0.5200623869895935,\n",
       "   0.3554050922393799,\n",
       "   0.2619187831878662,\n",
       "   0.16158609092235565,\n",
       "   0.20672327280044556,\n",
       "   0.22459448873996735,\n",
       "   0.14701539278030396,\n",
       "   0.1553376168012619,\n",
       "   0.14577291905879974,\n",
       "   0.1288519650697708,\n",
       "   0.1119609922170639,\n",
       "   0.10049459338188171,\n",
       "   0.10405413806438446,\n",
       "   0.09085221588611603,\n",
       "   0.08686614036560059,\n",
       "   0.07713037729263306,\n",
       "   0.07404083758592606,\n",
       "   0.07373879104852676,\n",
       "   0.07277700304985046,\n",
       "   0.06289294362068176],\n",
       "  [0.5101887583732605,\n",
       "   0.3363555669784546,\n",
       "   0.2184062898159027,\n",
       "   0.1658114641904831,\n",
       "   0.13764908909797668,\n",
       "   0.1288096308708191,\n",
       "   0.11171765625476837,\n",
       "   0.10232685506343842,\n",
       "   0.08982213586568832,\n",
       "   0.09197302907705307,\n",
       "   0.0827583521604538,\n",
       "   0.07005467265844345,\n",
       "   0.07226681709289551,\n",
       "   0.06306656450033188,\n",
       "   0.057659298181533813,\n",
       "   0.0561746209859848,\n",
       "   0.05548865348100662,\n",
       "   0.05614437535405159,\n",
       "   0.043376222252845764,\n",
       "   0.04005695506930351],\n",
       "  [0.5355143547058105,\n",
       "   0.3677389919757843,\n",
       "   0.2517453730106354,\n",
       "   0.20002298057079315,\n",
       "   0.16032564640045166,\n",
       "   0.23300394415855408,\n",
       "   0.22619496285915375,\n",
       "   0.18149837851524353,\n",
       "   0.245152086019516,\n",
       "   0.2431003898382187,\n",
       "   0.2142026573419571,\n",
       "   0.1691141277551651,\n",
       "   0.14630238711833954,\n",
       "   0.14602865278720856,\n",
       "   0.1330479085445404,\n",
       "   0.12348996102809906,\n",
       "   0.1213262677192688,\n",
       "   0.11119074374437332,\n",
       "   0.11115939915180206,\n",
       "   0.10335355997085571],\n",
       "  [0.5142552256584167,\n",
       "   0.3759395480155945,\n",
       "   0.24530598521232605,\n",
       "   0.1686662882566452,\n",
       "   0.17112241685390472,\n",
       "   0.1340043991804123,\n",
       "   0.11372596770524979,\n",
       "   0.10194811224937439,\n",
       "   0.09697844088077545,\n",
       "   0.08451700955629349,\n",
       "   0.0832865908741951,\n",
       "   0.07100921124219894,\n",
       "   0.06846190989017487,\n",
       "   0.0722666084766388,\n",
       "   0.05825381353497505,\n",
       "   0.055985741317272186,\n",
       "   0.056388262659311295,\n",
       "   0.05230526626110077,\n",
       "   0.055414970964193344,\n",
       "   0.04864206165075302],\n",
       "  [0.5365051031112671,\n",
       "   0.36943188309669495,\n",
       "   0.22644482553005219,\n",
       "   0.15328845381736755,\n",
       "   0.1795220822095871,\n",
       "   0.13216879963874817,\n",
       "   0.1159936711192131,\n",
       "   0.10319846868515015,\n",
       "   0.10230126976966858,\n",
       "   0.09069838374853134,\n",
       "   0.08591598272323608,\n",
       "   0.0778447836637497,\n",
       "   0.07198551297187805,\n",
       "   0.06263155490159988,\n",
       "   0.06534059345722198,\n",
       "   0.05740634351968765,\n",
       "   0.051359135657548904,\n",
       "   0.05898992717266083,\n",
       "   0.04942230507731438,\n",
       "   0.040943752974271774],\n",
       "  [0.5224383473396301,\n",
       "   0.3858327567577362,\n",
       "   0.2190401405096054,\n",
       "   0.26632919907569885,\n",
       "   0.1907511055469513,\n",
       "   0.19934454560279846,\n",
       "   0.13702721893787384,\n",
       "   0.11739416420459747,\n",
       "   0.1076822429895401,\n",
       "   0.09540765732526779,\n",
       "   0.08425994962453842,\n",
       "   0.08323097974061966,\n",
       "   0.07831241190433502,\n",
       "   0.07141102850437164,\n",
       "   0.06817782670259476,\n",
       "   0.07260585576295853,\n",
       "   0.0621265172958374,\n",
       "   0.05312136188149452,\n",
       "   0.05686676874756813,\n",
       "   0.05558973178267479],\n",
       "  [0.5136853456497192,\n",
       "   0.3327098488807678,\n",
       "   0.2025567889213562,\n",
       "   0.18583542108535767,\n",
       "   0.18023131787776947,\n",
       "   0.2693336606025696,\n",
       "   0.14244778454303741,\n",
       "   0.12090203166007996,\n",
       "   0.10207565873861313,\n",
       "   0.10375580191612244,\n",
       "   0.08749105781316757,\n",
       "   0.08085969090461731,\n",
       "   0.07851263135671616,\n",
       "   0.0685330256819725,\n",
       "   0.06625965237617493,\n",
       "   0.06439056992530823,\n",
       "   0.0627017617225647,\n",
       "   0.05664512887597084,\n",
       "   0.05055992305278778,\n",
       "   0.04798698052763939],\n",
       "  [0.5162907838821411,\n",
       "   0.34513360261917114,\n",
       "   0.3649042546749115,\n",
       "   0.317664235830307,\n",
       "   0.179576113820076,\n",
       "   0.15477056801319122,\n",
       "   0.12178584188222885,\n",
       "   0.1107812225818634,\n",
       "   0.10617449879646301,\n",
       "   0.09828647971153259,\n",
       "   0.09272435307502747,\n",
       "   0.0883856862783432,\n",
       "   0.07987203449010849,\n",
       "   0.09372708201408386,\n",
       "   0.07602165639400482,\n",
       "   0.06918830424547195,\n",
       "   0.06654059886932373,\n",
       "   0.059811677783727646,\n",
       "   0.05771980807185173,\n",
       "   0.06287254393100739],\n",
       "  [0.5370162129402161,\n",
       "   0.394754558801651,\n",
       "   0.36449480056762695,\n",
       "   0.23856520652770996,\n",
       "   0.18188130855560303,\n",
       "   0.14549532532691956,\n",
       "   0.11863571405410767,\n",
       "   0.11867249757051468,\n",
       "   0.11004173010587692,\n",
       "   0.11983781307935715,\n",
       "   0.11177393794059753,\n",
       "   0.08941522985696793,\n",
       "   0.07777706533670425,\n",
       "   0.08022814989089966,\n",
       "   0.0747535452246666,\n",
       "   0.06877820938825607,\n",
       "   0.06473033875226974,\n",
       "   0.060748789459466934,\n",
       "   0.0572371631860733,\n",
       "   0.056826598942279816],\n",
       "  [0.5167346596717834,\n",
       "   0.3905966281890869,\n",
       "   0.20782408118247986,\n",
       "   0.16206926107406616,\n",
       "   0.14215713739395142,\n",
       "   0.12972131371498108,\n",
       "   0.11430720239877701,\n",
       "   0.11005925387144089,\n",
       "   0.09439563006162643,\n",
       "   0.09503869712352753,\n",
       "   0.08640193939208984,\n",
       "   0.08066566288471222,\n",
       "   0.07982911169528961,\n",
       "   0.07306452095508575,\n",
       "   0.0724191889166832,\n",
       "   0.060614846646785736,\n",
       "   0.06275656819343567,\n",
       "   0.060417842119932175,\n",
       "   0.058792006224393845,\n",
       "   0.05580449849367142],\n",
       "  [0.5207725167274475,\n",
       "   0.41837751865386963,\n",
       "   0.2743384540081024,\n",
       "   0.23474636673927307,\n",
       "   0.18319769203662872,\n",
       "   0.1505453884601593,\n",
       "   0.12264613062143326,\n",
       "   0.11300313472747803,\n",
       "   0.10862655192613602,\n",
       "   0.09661996364593506,\n",
       "   0.08113133907318115,\n",
       "   0.08407548069953918,\n",
       "   0.07716453820466995,\n",
       "   0.06474408507347107,\n",
       "   0.0733390599489212,\n",
       "   0.06671049445867538,\n",
       "   0.06043988838791847,\n",
       "   0.05850379168987274,\n",
       "   0.04681878164410591,\n",
       "   0.04946506395936012],\n",
       "  [0.5281081795692444,\n",
       "   0.37293341755867004,\n",
       "   0.1780453771352768,\n",
       "   0.12975630164146423,\n",
       "   0.12671872973442078,\n",
       "   0.10072880238294601,\n",
       "   0.0979393720626831,\n",
       "   0.0923430472612381,\n",
       "   0.08298275619745255,\n",
       "   0.07883068919181824,\n",
       "   0.06706895679235458,\n",
       "   0.06432311981916428,\n",
       "   0.06711604446172714,\n",
       "   0.06020290032029152,\n",
       "   0.04849116504192352,\n",
       "   0.051975496113300323,\n",
       "   0.05011684074997902,\n",
       "   0.046446457505226135,\n",
       "   0.05070112645626068,\n",
       "   0.043286897242069244],\n",
       "  [0.5207381844520569,\n",
       "   0.36410510540008545,\n",
       "   0.29571533203125,\n",
       "   0.2894026041030884,\n",
       "   0.17710837721824646,\n",
       "   0.12007548660039902,\n",
       "   0.10544117540121078,\n",
       "   0.11119808256626129,\n",
       "   0.09237329661846161,\n",
       "   0.09300359338521957,\n",
       "   0.07882127165794373,\n",
       "   0.0722728818655014,\n",
       "   0.07315551489591599,\n",
       "   0.06352467834949493,\n",
       "   0.06525187194347382,\n",
       "   0.056959670037031174,\n",
       "   0.049636490643024445,\n",
       "   0.05012834072113037,\n",
       "   0.047261811792850494,\n",
       "   0.042967889457941055],\n",
       "  [0.5255176424980164,\n",
       "   0.3949316740036011,\n",
       "   0.3152998089790344,\n",
       "   0.20184700191020966,\n",
       "   0.1585111767053604,\n",
       "   0.15065984427928925,\n",
       "   0.12728939950466156,\n",
       "   0.12290415912866592,\n",
       "   0.11101392656564713,\n",
       "   0.09701938182115555,\n",
       "   0.10120737552642822,\n",
       "   0.0860358253121376,\n",
       "   0.0834473967552185,\n",
       "   0.08198282867670059,\n",
       "   0.06961292028427124,\n",
       "   0.0680539682507515,\n",
       "   0.06308737397193909,\n",
       "   0.06101126968860626,\n",
       "   0.055420201271772385,\n",
       "   0.05217437818646431],\n",
       "  [0.5294790267944336,\n",
       "   0.3512602746486664,\n",
       "   0.24035002291202545,\n",
       "   0.1766790896654129,\n",
       "   0.15983770787715912,\n",
       "   0.13829731941223145,\n",
       "   0.14369209110736847,\n",
       "   0.1350889503955841,\n",
       "   0.1010943353176117,\n",
       "   0.08920586854219437,\n",
       "   0.08922678977251053,\n",
       "   0.08119892328977585,\n",
       "   0.0786987766623497,\n",
       "   0.07266736775636673,\n",
       "   0.06340587884187698,\n",
       "   0.059524502605199814,\n",
       "   0.058248355984687805,\n",
       "   0.059225574135780334,\n",
       "   0.04885971173644066,\n",
       "   0.04962877184152603],\n",
       "  [0.5002086162567139,\n",
       "   0.377233624458313,\n",
       "   0.2808973491191864,\n",
       "   0.2139166295528412,\n",
       "   0.1661791056394577,\n",
       "   0.16337130963802338,\n",
       "   0.1258803904056549,\n",
       "   0.10951082408428192,\n",
       "   0.11025036126375198,\n",
       "   0.08949746191501617,\n",
       "   0.0870121568441391,\n",
       "   0.08032501488924026,\n",
       "   0.07390356808900833,\n",
       "   0.08175541460514069,\n",
       "   0.08436588943004608,\n",
       "   0.06264032423496246,\n",
       "   0.0711260661482811,\n",
       "   0.05817734822630882,\n",
       "   0.059372060000896454,\n",
       "   0.05073107033967972],\n",
       "  [0.5172046422958374,\n",
       "   0.37813276052474976,\n",
       "   0.28508058190345764,\n",
       "   0.18749326467514038,\n",
       "   0.12824416160583496,\n",
       "   0.10864280164241791,\n",
       "   0.10621967166662216,\n",
       "   0.10145258158445358,\n",
       "   0.08425334841012955,\n",
       "   0.08167130500078201,\n",
       "   0.07037152349948883,\n",
       "   0.07154703885316849,\n",
       "   0.05906997248530388,\n",
       "   0.05282805114984512,\n",
       "   0.04807943478226662,\n",
       "   0.049332067370414734,\n",
       "   0.05522618070244789,\n",
       "   0.04346146807074547,\n",
       "   0.04645466059446335,\n",
       "   0.03807554394006729],\n",
       "  [0.5226743221282959,\n",
       "   0.379515141248703,\n",
       "   0.22918100655078888,\n",
       "   0.2021489292383194,\n",
       "   0.14411349594593048,\n",
       "   0.11332286149263382,\n",
       "   0.10279708355665207,\n",
       "   0.09147793054580688,\n",
       "   0.08565644919872284,\n",
       "   0.08563722670078278,\n",
       "   0.0787179097533226,\n",
       "   0.07287516444921494,\n",
       "   0.05544424057006836,\n",
       "   0.06717419624328613,\n",
       "   0.056596726179122925,\n",
       "   0.05256754532456398,\n",
       "   0.04266541823744774,\n",
       "   0.05542906001210213,\n",
       "   0.045979201793670654,\n",
       "   0.047187551856040955],\n",
       "  [0.5164440870285034,\n",
       "   0.3824678361415863,\n",
       "   0.36559513211250305,\n",
       "   0.20114609599113464,\n",
       "   0.14347992837429047,\n",
       "   0.11342291533946991,\n",
       "   0.10340046137571335,\n",
       "   0.10653682798147202,\n",
       "   0.09067093580961227,\n",
       "   0.0815829336643219,\n",
       "   0.070540651679039,\n",
       "   0.07416480779647827,\n",
       "   0.06576605886220932,\n",
       "   0.06067658215761185,\n",
       "   0.054107338190078735,\n",
       "   0.05040929466485977,\n",
       "   0.04748712107539177,\n",
       "   0.04697317257523537,\n",
       "   0.048833612352609634,\n",
       "   0.04595831409096718],\n",
       "  [0.520265519618988,\n",
       "   0.35571911931037903,\n",
       "   0.25427597761154175,\n",
       "   0.1988263875246048,\n",
       "   0.15388177335262299,\n",
       "   0.135543093085289,\n",
       "   0.11426511406898499,\n",
       "   0.10731616616249084,\n",
       "   0.10039344429969788,\n",
       "   0.09086839854717255,\n",
       "   0.07490266859531403,\n",
       "   0.07154498249292374,\n",
       "   0.06899867951869965,\n",
       "   0.061670564115047455,\n",
       "   0.06592579931020737,\n",
       "   0.0509587898850441,\n",
       "   0.05783528462052345,\n",
       "   0.04938960820436478,\n",
       "   0.04813382029533386,\n",
       "   0.046008042991161346],\n",
       "  [0.5170705318450928,\n",
       "   0.40413883328437805,\n",
       "   0.3494904637336731,\n",
       "   0.26881855726242065,\n",
       "   0.18478666245937347,\n",
       "   0.14190775156021118,\n",
       "   0.12031454592943192,\n",
       "   0.12067694216966629,\n",
       "   0.10856670141220093,\n",
       "   0.10292832553386688,\n",
       "   0.09244619309902191,\n",
       "   0.08701326698064804,\n",
       "   0.07285017520189285,\n",
       "   0.068756103515625,\n",
       "   0.06684048473834991,\n",
       "   0.0613284707069397,\n",
       "   0.05822000280022621,\n",
       "   0.06254444271326065,\n",
       "   0.05114077031612396,\n",
       "   0.053278107196092606],\n",
       "  [0.5207337737083435,\n",
       "   0.3912827670574188,\n",
       "   0.2454909384250641,\n",
       "   0.22958263754844666,\n",
       "   0.13835126161575317,\n",
       "   0.12906703352928162,\n",
       "   0.1162170022726059,\n",
       "   0.10464470833539963,\n",
       "   0.14414604008197784,\n",
       "   0.08661404252052307,\n",
       "   0.07662826776504517,\n",
       "   0.0749654471874237,\n",
       "   0.06531227380037308,\n",
       "   0.06668040156364441,\n",
       "   0.06675026565790176,\n",
       "   0.05761951953172684,\n",
       "   0.052672870457172394,\n",
       "   0.05172392725944519,\n",
       "   0.045070234686136246,\n",
       "   0.044747818261384964],\n",
       "  [0.5263450741767883,\n",
       "   0.3614472448825836,\n",
       "   0.24681216478347778,\n",
       "   0.16001228988170624,\n",
       "   0.1297626793384552,\n",
       "   0.1123000979423523,\n",
       "   0.09786026179790497,\n",
       "   0.09779009968042374,\n",
       "   0.12330412864685059,\n",
       "   0.10280416160821915,\n",
       "   0.09012434631586075,\n",
       "   0.08244591951370239,\n",
       "   0.07759973406791687,\n",
       "   0.06786279380321503,\n",
       "   0.0786452367901802,\n",
       "   0.06028217449784279,\n",
       "   0.06004960834980011,\n",
       "   0.06603351980447769,\n",
       "   0.05327807739377022,\n",
       "   0.0495830774307251],\n",
       "  [0.5160433053970337,\n",
       "   0.35724252462387085,\n",
       "   0.27011239528656006,\n",
       "   0.16564199328422546,\n",
       "   0.12373120337724686,\n",
       "   0.11104848980903625,\n",
       "   0.10097075253725052,\n",
       "   0.2109285593032837,\n",
       "   0.10443281382322311,\n",
       "   0.08882730454206467,\n",
       "   0.07951197773218155,\n",
       "   0.08122662454843521,\n",
       "   0.06869374960660934,\n",
       "   0.06590023636817932,\n",
       "   0.06901008635759354,\n",
       "   0.059803932905197144,\n",
       "   0.056586913764476776,\n",
       "   0.05634548142552376,\n",
       "   0.049123894423246384,\n",
       "   0.046173810958862305],\n",
       "  [0.5177440643310547,\n",
       "   0.40289151668548584,\n",
       "   0.24727404117584229,\n",
       "   0.18090085685253143,\n",
       "   0.12407334893941879,\n",
       "   0.11090013384819031,\n",
       "   0.10153813660144806,\n",
       "   0.09658349305391312,\n",
       "   0.08904600143432617,\n",
       "   0.07981280982494354,\n",
       "   0.0801565945148468,\n",
       "   0.0740734115242958,\n",
       "   0.06794014573097229,\n",
       "   0.06413161754608154,\n",
       "   0.06529714167118073,\n",
       "   0.05164137855172157,\n",
       "   0.05873354524374008,\n",
       "   0.04632459208369255,\n",
       "   0.05424463376402855,\n",
       "   0.04804304242134094],\n",
       "  [0.5121866464614868,\n",
       "   0.3653506636619568,\n",
       "   0.2581695318222046,\n",
       "   0.19188183546066284,\n",
       "   0.16385550796985626,\n",
       "   0.14518795907497406,\n",
       "   0.17953993380069733,\n",
       "   0.1788228452205658,\n",
       "   0.10405288636684418,\n",
       "   0.09343760460615158,\n",
       "   0.09050402045249939,\n",
       "   0.07813762128353119,\n",
       "   0.07709581404924393,\n",
       "   0.06757314503192902,\n",
       "   0.06778858602046967,\n",
       "   0.069534070789814,\n",
       "   0.06028499826788902,\n",
       "   0.05735849216580391,\n",
       "   0.05552813038229942,\n",
       "   0.051388218998909],\n",
       "  [0.5395104885101318,\n",
       "   0.36932164430618286,\n",
       "   0.23940028250217438,\n",
       "   0.18661557137966156,\n",
       "   0.1795320063829422,\n",
       "   0.1364089697599411,\n",
       "   0.1231393963098526,\n",
       "   0.10207506269216537,\n",
       "   0.09009049832820892,\n",
       "   0.08445890992879868,\n",
       "   0.08628960698843002,\n",
       "   0.07492566853761673,\n",
       "   0.06884710490703583,\n",
       "   0.06701593101024628,\n",
       "   0.05403479188680649,\n",
       "   0.0581660196185112,\n",
       "   0.05125690624117851,\n",
       "   0.04824065789580345,\n",
       "   0.044484809041023254,\n",
       "   0.050262466073036194],\n",
       "  [0.5264260768890381,\n",
       "   0.38538485765457153,\n",
       "   0.26783615350723267,\n",
       "   0.19760458171367645,\n",
       "   0.178330197930336,\n",
       "   0.12893836200237274,\n",
       "   0.1121266558766365,\n",
       "   0.1203615739941597,\n",
       "   0.09704986959695816,\n",
       "   0.09690189361572266,\n",
       "   0.0831647515296936,\n",
       "   0.07674100250005722,\n",
       "   0.07971419394016266,\n",
       "   0.06839007884263992,\n",
       "   0.06809840351343155,\n",
       "   0.062134742736816406,\n",
       "   0.05710417404770851,\n",
       "   0.054161421954631805,\n",
       "   0.054463405162096024,\n",
       "   0.050348635762929916]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_acc_per_fold, acc_per_fold, val_loss_per_fold, loss_per_fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "results = {\"val_acc_per_fold\": val_acc_per_fold, \"acc_per_fold\": acc_per_fold, \"val_loss_per_fold\": val_loss_per_fold, \"loss_per_fold\": loss_per_fold}\n",
    "\n",
    "with open('results_model_bi_leave_one_out.pickle', 'wb') as file:\n",
    " pickle.dump(results, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.9982847571372986\n",
      "1.0\n",
      "0.9228295683860779\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.9294710159301758\n",
      "0.987922728061676\n",
      "1.0\n",
      "0.9872449040412903\n",
      "1.0\n",
      "0.9628646969795227\n",
      "0.9973118305206299\n",
      "0.9974683523178101\n",
      "0.9871794581413269\n",
      "1.0\n",
      "0.9896551966667175\n",
      "1.0\n",
      "1.0\n",
      "0.9964285492897034\n",
      "0.9896694421768188\n",
      "0.9969325065612793\n",
      "0.9819967150688171\n",
      "0.990123450756073\n",
      "0.9851190447807312\n",
      "0.9971910119056702\n",
      "1.0\n",
      "0.9980582594871521\n",
      "1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9898583829402924"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 0\n",
    "for fold in val_acc_per_fold:\n",
    "    print(fold[-1])\n",
    "    s += fold[-1]\n",
    "s/30"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#poprzednio: 0.98176"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as gru_cell_88_layer_call_fn, gru_cell_88_layer_call_and_return_conditional_losses, gru_cell_89_layer_call_fn, gru_cell_89_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: classifier_test_opt_bi_leave_one_out\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: classifier_test_opt_bi_leave_one_out\\assets\n"
     ]
    }
   ],
   "source": [
    "# Calling `save('my_model')` creates a SavedModel folder `my_model`.\n",
    "classifier_test_opt_bi_leave.save(\"classifier_test_opt_bi_leave_one_out\")\n",
    "\n",
    "# It can be used to reconstruct the model identically.\n",
    "#reconstructed_model = keras.models.load_model(\"classifier_RNN_leave_one_out\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simplified RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_RNN():\n",
    "    hidden_dim = 512\n",
    "    seq_len = fold_1.shape[1]\n",
    "    n_features = 1\n",
    "    learning_rate = 0.0003\n",
    "\n",
    "    in1 = tf.keras.layers.Input(shape=(seq_len, n_features))\n",
    "    gru1 = tf.keras.layers.GRU(hidden_dim, name='latent_layer')(in1)\n",
    "    bn = tf.keras.layers.BatchNormalization()(gru1)\n",
    "    dense = tf.keras.layers.Dense(128, activation=\"relu\")(bn)\n",
    "    drop = tf.keras.layers.Dropout(0.3)(dense)\n",
    "    d1   = tf.keras.layers.Dense(1, activation='sigmoid')(drop)\n",
    "\n",
    "    classifier_RNN = tf.keras.Model(\n",
    "        inputs=[in1], \n",
    "        outputs=[d1]\n",
    "    )\n",
    "\n",
    "    #print(classifier_RNN.summary())\n",
    "\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    classifier_RNN.compile(loss='binary_crossentropy', optimizer=opt, \n",
    "        metrics=['accuracy'])\n",
    "    \n",
    "    return classifier_RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "711/711 [==============================] - 30s 40ms/step - loss: 0.4293 - accuracy: 0.8254 - val_loss: 0.4676 - val_accuracy: 0.8203 - lr: 3.0000e-04\n",
      "Epoch 2/20\n",
      "711/711 [==============================] - 28s 40ms/step - loss: 0.2818 - accuracy: 0.8953 - val_loss: 0.1386 - val_accuracy: 0.9505 - lr: 3.0000e-04\n",
      "Epoch 3/20\n",
      "711/711 [==============================] - 29s 40ms/step - loss: 0.1595 - accuracy: 0.9425 - val_loss: 0.1230 - val_accuracy: 0.9583 - lr: 3.0000e-04\n",
      "Epoch 4/20\n",
      "711/711 [==============================] - 29s 40ms/step - loss: 0.1014 - accuracy: 0.9654 - val_loss: 0.1688 - val_accuracy: 0.9505 - lr: 3.0000e-04\n",
      "Epoch 5/20\n",
      "711/711 [==============================] - 28s 40ms/step - loss: 0.0676 - accuracy: 0.9762 - val_loss: 0.0176 - val_accuracy: 0.9974 - lr: 3.0000e-04\n",
      "Epoch 6/20\n",
      "711/711 [==============================] - 28s 40ms/step - loss: 0.0621 - accuracy: 0.9795 - val_loss: 0.0554 - val_accuracy: 0.9792 - lr: 3.0000e-04\n",
      "Epoch 7/20\n",
      "711/711 [==============================] - 29s 40ms/step - loss: 0.0416 - accuracy: 0.9867 - val_loss: 0.0693 - val_accuracy: 0.9818 - lr: 3.0000e-04\n",
      "Epoch 8/20\n",
      "711/711 [==============================] - 31s 44ms/step - loss: 0.1096 - accuracy: 0.9597 - val_loss: 0.0613 - val_accuracy: 0.9792 - lr: 3.0000e-04\n",
      "Epoch 9/20\n",
      "711/711 [==============================] - 32s 45ms/step - loss: 0.1895 - accuracy: 0.9267 - val_loss: 0.4053 - val_accuracy: 0.8047 - lr: 3.0000e-04\n",
      "Epoch 10/20\n",
      "711/711 [==============================] - 30s 43ms/step - loss: 0.2303 - accuracy: 0.9093 - val_loss: 0.2133 - val_accuracy: 0.8932 - lr: 3.0000e-04\n",
      "Epoch 11/20\n",
      "711/711 [==============================] - 30s 42ms/step - loss: 0.1562 - accuracy: 0.9412 - val_loss: 0.1684 - val_accuracy: 0.9245 - lr: 3.0000e-04\n",
      "Epoch 12/20\n",
      "711/711 [==============================] - 31s 44ms/step - loss: 0.1232 - accuracy: 0.9555 - val_loss: 0.1152 - val_accuracy: 0.9427 - lr: 3.0000e-04\n",
      "Epoch 13/20\n",
      "711/711 [==============================] - 31s 43ms/step - loss: 0.1064 - accuracy: 0.9595 - val_loss: 0.1540 - val_accuracy: 0.9453 - lr: 3.0000e-04\n",
      "Epoch 14/20\n",
      "711/711 [==============================] - 31s 43ms/step - loss: 0.1058 - accuracy: 0.9632 - val_loss: 0.0745 - val_accuracy: 0.9766 - lr: 3.0000e-04\n",
      "Epoch 15/20\n",
      "711/711 [==============================] - 31s 43ms/step - loss: 0.0873 - accuracy: 0.9697 - val_loss: 0.0561 - val_accuracy: 0.9792 - lr: 3.0000e-04\n",
      "Epoch 16/20\n",
      "711/711 [==============================] - 33s 46ms/step - loss: 0.0558 - accuracy: 0.9814 - val_loss: 0.0342 - val_accuracy: 0.9896 - lr: 6.0000e-05\n",
      "Epoch 17/20\n",
      "711/711 [==============================] - 30s 42ms/step - loss: 0.0501 - accuracy: 0.9842 - val_loss: 0.0561 - val_accuracy: 0.9844 - lr: 6.0000e-05\n",
      "Epoch 18/20\n",
      "711/711 [==============================] - 30s 42ms/step - loss: 0.0480 - accuracy: 0.9836 - val_loss: 0.0307 - val_accuracy: 0.9922 - lr: 6.0000e-05\n",
      "Epoch 19/20\n",
      "711/711 [==============================] - 30s 42ms/step - loss: 0.0426 - accuracy: 0.9867 - val_loss: 0.0392 - val_accuracy: 0.9896 - lr: 6.0000e-05\n",
      "Epoch 20/20\n",
      "711/711 [==============================] - 30s 43ms/step - loss: 0.0418 - accuracy: 0.9858 - val_loss: 0.0291 - val_accuracy: 0.9896 - lr: 6.0000e-05\n",
      "Epoch 1/20\n",
      "699/699 [==============================] - 31s 42ms/step - loss: 0.4346 - accuracy: 0.8233 - val_loss: 0.3773 - val_accuracy: 0.8165 - lr: 3.0000e-04\n",
      "Epoch 2/20\n",
      "699/699 [==============================] - 29s 42ms/step - loss: 0.3272 - accuracy: 0.8716 - val_loss: 0.2341 - val_accuracy: 0.8714 - lr: 3.0000e-04\n",
      "Epoch 3/20\n",
      "699/699 [==============================] - 29s 42ms/step - loss: 0.1816 - accuracy: 0.9346 - val_loss: 0.1329 - val_accuracy: 0.9417 - lr: 3.0000e-04\n",
      "Epoch 4/20\n",
      "699/699 [==============================] - 29s 41ms/step - loss: 0.4063 - accuracy: 0.8289 - val_loss: 0.3981 - val_accuracy: 0.7873 - lr: 3.0000e-04\n",
      "Epoch 5/20\n",
      "699/699 [==============================] - 29s 42ms/step - loss: 0.2496 - accuracy: 0.9016 - val_loss: 0.0849 - val_accuracy: 0.9811 - lr: 3.0000e-04\n",
      "Epoch 6/20\n",
      "699/699 [==============================] - 29s 41ms/step - loss: 0.1657 - accuracy: 0.9390 - val_loss: 0.0975 - val_accuracy: 0.9674 - lr: 3.0000e-04\n",
      "Epoch 7/20\n",
      "699/699 [==============================] - 29s 41ms/step - loss: 0.1269 - accuracy: 0.9539 - val_loss: 0.1175 - val_accuracy: 0.9503 - lr: 3.0000e-04\n",
      "Epoch 8/20\n",
      "699/699 [==============================] - 29s 42ms/step - loss: 0.1098 - accuracy: 0.9623 - val_loss: 0.0532 - val_accuracy: 0.9880 - lr: 3.0000e-04\n",
      "Epoch 9/20\n",
      "699/699 [==============================] - 29s 41ms/step - loss: 0.0845 - accuracy: 0.9702 - val_loss: 0.0494 - val_accuracy: 0.9777 - lr: 3.0000e-04\n",
      "Epoch 10/20\n",
      "699/699 [==============================] - 28s 41ms/step - loss: 0.0697 - accuracy: 0.9767 - val_loss: 0.0703 - val_accuracy: 0.9691 - lr: 3.0000e-04\n",
      "Epoch 11/20\n",
      "699/699 [==============================] - 29s 41ms/step - loss: 0.0576 - accuracy: 0.9797 - val_loss: 2.5466 - val_accuracy: 0.5592 - lr: 3.0000e-04\n",
      "Epoch 12/20\n",
      "699/699 [==============================] - 29s 41ms/step - loss: 0.0537 - accuracy: 0.9815 - val_loss: 0.4613 - val_accuracy: 0.7959 - lr: 3.0000e-04\n",
      "Epoch 13/20\n",
      "699/699 [==============================] - 29s 41ms/step - loss: 0.0463 - accuracy: 0.9834 - val_loss: 0.0933 - val_accuracy: 0.9588 - lr: 3.0000e-04\n",
      "Epoch 14/20\n",
      "699/699 [==============================] - 28s 41ms/step - loss: 0.0491 - accuracy: 0.9839 - val_loss: 0.0529 - val_accuracy: 0.9828 - lr: 3.0000e-04\n",
      "Epoch 15/20\n",
      "699/699 [==============================] - 29s 41ms/step - loss: 0.0523 - accuracy: 0.9812 - val_loss: 0.0672 - val_accuracy: 0.9691 - lr: 3.0000e-04\n",
      "Epoch 16/20\n",
      "699/699 [==============================] - 29s 41ms/step - loss: 0.0598 - accuracy: 0.9801 - val_loss: 0.1039 - val_accuracy: 0.9657 - lr: 3.0000e-04\n",
      "Epoch 17/20\n",
      "699/699 [==============================] - 29s 41ms/step - loss: 0.0393 - accuracy: 0.9884 - val_loss: 0.0425 - val_accuracy: 0.9777 - lr: 3.0000e-04\n",
      "Epoch 18/20\n",
      "699/699 [==============================] - 29s 41ms/step - loss: 0.0304 - accuracy: 0.9902 - val_loss: 0.0139 - val_accuracy: 0.9966 - lr: 3.0000e-04\n",
      "Epoch 19/20\n",
      "699/699 [==============================] - 29s 41ms/step - loss: 0.0265 - accuracy: 0.9923 - val_loss: 0.0882 - val_accuracy: 0.9691 - lr: 3.0000e-04\n",
      "Epoch 20/20\n",
      "699/699 [==============================] - 29s 41ms/step - loss: 0.0317 - accuracy: 0.9885 - val_loss: 0.0169 - val_accuracy: 0.9949 - lr: 3.0000e-04\n",
      "Epoch 1/20\n",
      "716/716 [==============================] - 31s 41ms/step - loss: 0.4210 - accuracy: 0.8297 - val_loss: 0.5164 - val_accuracy: 0.7664 - lr: 3.0000e-04\n",
      "Epoch 2/20\n",
      "716/716 [==============================] - 29s 40ms/step - loss: 0.3100 - accuracy: 0.8807 - val_loss: 0.3277 - val_accuracy: 0.8586 - lr: 3.0000e-04\n",
      "Epoch 3/20\n",
      "716/716 [==============================] - 29s 41ms/step - loss: 0.1300 - accuracy: 0.9542 - val_loss: 0.0726 - val_accuracy: 0.9770 - lr: 3.0000e-04\n",
      "Epoch 4/20\n",
      "716/716 [==============================] - 29s 41ms/step - loss: 0.0580 - accuracy: 0.9813 - val_loss: 0.0955 - val_accuracy: 0.9836 - lr: 3.0000e-04\n",
      "Epoch 5/20\n",
      "716/716 [==============================] - 29s 41ms/step - loss: 0.0429 - accuracy: 0.9857 - val_loss: 0.0218 - val_accuracy: 0.9901 - lr: 3.0000e-04\n",
      "Epoch 6/20\n",
      "716/716 [==============================] - 30s 41ms/step - loss: 0.0367 - accuracy: 0.9892 - val_loss: 0.0780 - val_accuracy: 0.9770 - lr: 3.0000e-04\n",
      "Epoch 7/20\n",
      "716/716 [==============================] - 29s 40ms/step - loss: 0.0238 - accuracy: 0.9930 - val_loss: 0.1769 - val_accuracy: 0.9441 - lr: 3.0000e-04\n",
      "Epoch 8/20\n",
      "716/716 [==============================] - 29s 40ms/step - loss: 0.0227 - accuracy: 0.9928 - val_loss: 0.0950 - val_accuracy: 0.9737 - lr: 3.0000e-04\n",
      "Epoch 9/20\n",
      "716/716 [==============================] - 29s 41ms/step - loss: 0.0719 - accuracy: 0.9786 - val_loss: 0.2740 - val_accuracy: 0.9243 - lr: 3.0000e-04\n",
      "Epoch 10/20\n",
      "716/716 [==============================] - 29s 41ms/step - loss: 0.0499 - accuracy: 0.9837 - val_loss: 0.1552 - val_accuracy: 0.9539 - lr: 3.0000e-04\n",
      "Epoch 11/20\n",
      "716/716 [==============================] - 29s 40ms/step - loss: 0.0313 - accuracy: 0.9891 - val_loss: 0.1596 - val_accuracy: 0.9572 - lr: 3.0000e-04\n",
      "Epoch 12/20\n",
      "716/716 [==============================] - 29s 40ms/step - loss: 0.0340 - accuracy: 0.9896 - val_loss: 0.0781 - val_accuracy: 0.9671 - lr: 3.0000e-04\n",
      "Epoch 13/20\n",
      "716/716 [==============================] - 29s 41ms/step - loss: 0.0437 - accuracy: 0.9868 - val_loss: 0.0557 - val_accuracy: 0.9836 - lr: 3.0000e-04\n",
      "Epoch 14/20\n",
      "716/716 [==============================] - 30s 41ms/step - loss: 0.0176 - accuracy: 0.9938 - val_loss: 0.1271 - val_accuracy: 0.9704 - lr: 3.0000e-04\n",
      "Epoch 15/20\n",
      "716/716 [==============================] - 31s 43ms/step - loss: 0.0236 - accuracy: 0.9921 - val_loss: 0.2464 - val_accuracy: 0.9474 - lr: 3.0000e-04\n",
      "Epoch 16/20\n",
      "716/716 [==============================] - 29s 41ms/step - loss: 0.0083 - accuracy: 0.9977 - val_loss: 0.1331 - val_accuracy: 0.9704 - lr: 6.0000e-05\n",
      "Epoch 17/20\n",
      "716/716 [==============================] - 29s 41ms/step - loss: 0.0054 - accuracy: 0.9985 - val_loss: 0.0628 - val_accuracy: 0.9836 - lr: 6.0000e-05\n",
      "Epoch 18/20\n",
      "716/716 [==============================] - 29s 41ms/step - loss: 0.0053 - accuracy: 0.9982 - val_loss: 0.1008 - val_accuracy: 0.9671 - lr: 6.0000e-05\n",
      "Epoch 19/20\n",
      "716/716 [==============================] - 29s 41ms/step - loss: 0.0048 - accuracy: 0.9982 - val_loss: 0.0719 - val_accuracy: 0.9770 - lr: 6.0000e-05\n",
      "Epoch 20/20\n",
      "716/716 [==============================] - 29s 41ms/step - loss: 0.0056 - accuracy: 0.9983 - val_loss: 0.1030 - val_accuracy: 0.9770 - lr: 6.0000e-05\n",
      "Epoch 1/20\n",
      "716/716 [==============================] - 31s 41ms/step - loss: 0.4292 - accuracy: 0.8254 - val_loss: 0.3692 - val_accuracy: 0.8457 - lr: 3.0000e-04\n",
      "Epoch 2/20\n",
      "716/716 [==============================] - 29s 41ms/step - loss: 0.3078 - accuracy: 0.8811 - val_loss: 0.2332 - val_accuracy: 0.9003 - lr: 3.0000e-04\n",
      "Epoch 3/20\n",
      "716/716 [==============================] - 29s 41ms/step - loss: 0.1639 - accuracy: 0.9401 - val_loss: 0.1083 - val_accuracy: 0.9518 - lr: 3.0000e-04\n",
      "Epoch 4/20\n",
      "716/716 [==============================] - 29s 41ms/step - loss: 0.0983 - accuracy: 0.9663 - val_loss: 0.6491 - val_accuracy: 0.7814 - lr: 3.0000e-04\n",
      "Epoch 5/20\n",
      "716/716 [==============================] - 29s 41ms/step - loss: 0.1220 - accuracy: 0.9553 - val_loss: 0.1789 - val_accuracy: 0.9389 - lr: 3.0000e-04\n",
      "Epoch 6/20\n",
      "716/716 [==============================] - 29s 41ms/step - loss: 0.0890 - accuracy: 0.9690 - val_loss: 0.1045 - val_accuracy: 0.9518 - lr: 3.0000e-04\n",
      "Epoch 7/20\n",
      "716/716 [==============================] - 29s 41ms/step - loss: 0.0501 - accuracy: 0.9844 - val_loss: 0.1998 - val_accuracy: 0.9357 - lr: 3.0000e-04\n",
      "Epoch 8/20\n",
      "716/716 [==============================] - 29s 41ms/step - loss: 0.0713 - accuracy: 0.9761 - val_loss: 0.2112 - val_accuracy: 0.9486 - lr: 3.0000e-04\n",
      "Epoch 9/20\n",
      "716/716 [==============================] - 29s 41ms/step - loss: 0.0478 - accuracy: 0.9855 - val_loss: 0.2125 - val_accuracy: 0.9003 - lr: 3.0000e-04\n",
      "Epoch 10/20\n",
      "716/716 [==============================] - 29s 41ms/step - loss: 0.0285 - accuracy: 0.9915 - val_loss: 0.0857 - val_accuracy: 0.9711 - lr: 3.0000e-04\n",
      "Epoch 11/20\n",
      "716/716 [==============================] - 29s 41ms/step - loss: 0.0277 - accuracy: 0.9915 - val_loss: 0.0997 - val_accuracy: 0.9646 - lr: 3.0000e-04\n",
      "Epoch 12/20\n",
      "716/716 [==============================] - 29s 41ms/step - loss: 0.0268 - accuracy: 0.9913 - val_loss: 0.1094 - val_accuracy: 0.9453 - lr: 3.0000e-04\n",
      "Epoch 13/20\n",
      "716/716 [==============================] - 29s 41ms/step - loss: 0.0236 - accuracy: 0.9927 - val_loss: 0.1944 - val_accuracy: 0.9293 - lr: 3.0000e-04\n",
      "Epoch 14/20\n",
      "716/716 [==============================] - 29s 41ms/step - loss: 0.0148 - accuracy: 0.9959 - val_loss: 0.6083 - val_accuracy: 0.9164 - lr: 3.0000e-04\n",
      "Epoch 15/20\n",
      "716/716 [==============================] - 29s 41ms/step - loss: 0.0228 - accuracy: 0.9933 - val_loss: 0.3050 - val_accuracy: 0.9132 - lr: 3.0000e-04\n",
      "Epoch 16/20\n",
      "716/716 [==============================] - 29s 41ms/step - loss: 0.0152 - accuracy: 0.9958 - val_loss: 0.4655 - val_accuracy: 0.9228 - lr: 3.0000e-04\n",
      "Epoch 17/20\n",
      "716/716 [==============================] - 29s 41ms/step - loss: 0.0204 - accuracy: 0.9944 - val_loss: 0.1735 - val_accuracy: 0.9614 - lr: 3.0000e-04\n",
      "Epoch 18/20\n",
      "716/716 [==============================] - 29s 41ms/step - loss: 0.0189 - accuracy: 0.9951 - val_loss: 0.0558 - val_accuracy: 0.9775 - lr: 3.0000e-04\n",
      "Epoch 19/20\n",
      "716/716 [==============================] - 29s 41ms/step - loss: 0.0077 - accuracy: 0.9977 - val_loss: 0.0200 - val_accuracy: 0.9936 - lr: 3.0000e-04\n",
      "Epoch 20/20\n",
      "716/716 [==============================] - 29s 41ms/step - loss: 0.0093 - accuracy: 0.9979 - val_loss: 0.1573 - val_accuracy: 0.9421 - lr: 3.0000e-04\n",
      "Epoch 1/20\n",
      "717/717 [==============================] - 31s 41ms/step - loss: 0.4247 - accuracy: 0.8302 - val_loss: 0.4236 - val_accuracy: 0.8557 - lr: 3.0000e-04\n",
      "Epoch 2/20\n",
      "717/717 [==============================] - 29s 41ms/step - loss: 0.2874 - accuracy: 0.8929 - val_loss: 0.4078 - val_accuracy: 0.7987 - lr: 3.0000e-04\n",
      "Epoch 3/20\n",
      "717/717 [==============================] - 29s 41ms/step - loss: 0.1788 - accuracy: 0.9304 - val_loss: 0.1368 - val_accuracy: 0.9564 - lr: 3.0000e-04\n",
      "Epoch 4/20\n",
      "717/717 [==============================] - 29s 41ms/step - loss: 0.1407 - accuracy: 0.9504 - val_loss: 0.1571 - val_accuracy: 0.9530 - lr: 3.0000e-04\n",
      "Epoch 5/20\n",
      "717/717 [==============================] - 29s 41ms/step - loss: 0.0733 - accuracy: 0.9752 - val_loss: 0.1024 - val_accuracy: 0.9698 - lr: 3.0000e-04\n",
      "Epoch 6/20\n",
      "717/717 [==============================] - 29s 41ms/step - loss: 0.0536 - accuracy: 0.9831 - val_loss: 0.1133 - val_accuracy: 0.9765 - lr: 3.0000e-04\n",
      "Epoch 7/20\n",
      "717/717 [==============================] - 29s 41ms/step - loss: 0.0590 - accuracy: 0.9813 - val_loss: 0.0306 - val_accuracy: 0.9832 - lr: 3.0000e-04\n",
      "Epoch 8/20\n",
      "717/717 [==============================] - 29s 41ms/step - loss: 0.0675 - accuracy: 0.9795 - val_loss: 0.0285 - val_accuracy: 0.9899 - lr: 3.0000e-04\n",
      "Epoch 9/20\n",
      "717/717 [==============================] - 29s 41ms/step - loss: 0.0270 - accuracy: 0.9926 - val_loss: 0.0283 - val_accuracy: 0.9866 - lr: 3.0000e-04\n",
      "Epoch 10/20\n",
      "717/717 [==============================] - 30s 42ms/step - loss: 0.0429 - accuracy: 0.9859 - val_loss: 0.0320 - val_accuracy: 0.9899 - lr: 3.0000e-04\n",
      "Epoch 11/20\n",
      "717/717 [==============================] - 29s 41ms/step - loss: 0.0175 - accuracy: 0.9949 - val_loss: 0.0154 - val_accuracy: 0.9966 - lr: 3.0000e-04\n",
      "Epoch 12/20\n",
      "717/717 [==============================] - 29s 41ms/step - loss: 0.0203 - accuracy: 0.9936 - val_loss: 0.0355 - val_accuracy: 0.9933 - lr: 3.0000e-04\n",
      "Epoch 13/20\n",
      "717/717 [==============================] - 29s 41ms/step - loss: 0.0152 - accuracy: 0.9950 - val_loss: 0.0569 - val_accuracy: 0.9899 - lr: 3.0000e-04\n",
      "Epoch 14/20\n",
      "717/717 [==============================] - 29s 40ms/step - loss: 0.0250 - accuracy: 0.9928 - val_loss: 0.0527 - val_accuracy: 0.9899 - lr: 3.0000e-04\n",
      "Epoch 15/20\n",
      "717/717 [==============================] - 29s 41ms/step - loss: 0.0095 - accuracy: 0.9976 - val_loss: 0.0248 - val_accuracy: 0.9832 - lr: 3.0000e-04\n",
      "Epoch 16/20\n",
      "717/717 [==============================] - 30s 41ms/step - loss: 0.0144 - accuracy: 0.9959 - val_loss: 0.0530 - val_accuracy: 0.9899 - lr: 3.0000e-04\n",
      "Epoch 17/20\n",
      "717/717 [==============================] - 29s 41ms/step - loss: 0.0115 - accuracy: 0.9970 - val_loss: 0.0351 - val_accuracy: 0.9866 - lr: 3.0000e-04\n",
      "Epoch 18/20\n",
      "717/717 [==============================] - 29s 41ms/step - loss: 0.0115 - accuracy: 0.9966 - val_loss: 0.0337 - val_accuracy: 0.9899 - lr: 3.0000e-04\n",
      "Epoch 19/20\n",
      "717/717 [==============================] - 30s 41ms/step - loss: 0.0122 - accuracy: 0.9965 - val_loss: 0.0030 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 20/20\n",
      "717/717 [==============================] - 29s 41ms/step - loss: 0.0126 - accuracy: 0.9961 - val_loss: 0.0179 - val_accuracy: 0.9966 - lr: 3.0000e-04\n",
      "Epoch 1/20\n",
      "709/709 [==============================] - 31s 41ms/step - loss: 0.4299 - accuracy: 0.8282 - val_loss: 0.2601 - val_accuracy: 0.9150 - lr: 3.0000e-04\n",
      "Epoch 2/20\n",
      "709/709 [==============================] - 29s 41ms/step - loss: 0.3271 - accuracy: 0.8717 - val_loss: 0.2979 - val_accuracy: 0.9102 - lr: 3.0000e-04\n",
      "Epoch 3/20\n",
      "709/709 [==============================] - 29s 41ms/step - loss: 0.1669 - accuracy: 0.9368 - val_loss: 0.0469 - val_accuracy: 0.9854 - lr: 3.0000e-04\n",
      "Epoch 4/20\n",
      "709/709 [==============================] - 29s 41ms/step - loss: 0.0581 - accuracy: 0.9803 - val_loss: 0.0150 - val_accuracy: 0.9976 - lr: 3.0000e-04\n",
      "Epoch 5/20\n",
      "709/709 [==============================] - 29s 41ms/step - loss: 0.0388 - accuracy: 0.9868 - val_loss: 0.0041 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 6/20\n",
      "709/709 [==============================] - 29s 41ms/step - loss: 0.0410 - accuracy: 0.9855 - val_loss: 0.0049 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 7/20\n",
      "709/709 [==============================] - 29s 41ms/step - loss: 0.0277 - accuracy: 0.9922 - val_loss: 0.0280 - val_accuracy: 0.9830 - lr: 3.0000e-04\n",
      "Epoch 8/20\n",
      "709/709 [==============================] - 29s 41ms/step - loss: 0.0256 - accuracy: 0.9916 - val_loss: 2.9628e-04 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 9/20\n",
      "709/709 [==============================] - 29s 41ms/step - loss: 0.0171 - accuracy: 0.9944 - val_loss: 0.0079 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 10/20\n",
      "709/709 [==============================] - 29s 41ms/step - loss: 0.0154 - accuracy: 0.9950 - val_loss: 0.0083 - val_accuracy: 0.9976 - lr: 3.0000e-04\n",
      "Epoch 11/20\n",
      "709/709 [==============================] - 29s 41ms/step - loss: 0.0143 - accuracy: 0.9959 - val_loss: 0.0145 - val_accuracy: 0.9951 - lr: 3.0000e-04\n",
      "Epoch 12/20\n",
      "709/709 [==============================] - 29s 41ms/step - loss: 0.0117 - accuracy: 0.9967 - val_loss: 7.0042e-04 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 13/20\n",
      "709/709 [==============================] - 29s 41ms/step - loss: 0.0168 - accuracy: 0.9953 - val_loss: 0.0179 - val_accuracy: 0.9927 - lr: 3.0000e-04\n",
      "Epoch 14/20\n",
      "709/709 [==============================] - 29s 41ms/step - loss: 0.0089 - accuracy: 0.9972 - val_loss: 5.6419e-04 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 15/20\n",
      "709/709 [==============================] - 29s 41ms/step - loss: 0.0110 - accuracy: 0.9956 - val_loss: 0.0028 - val_accuracy: 0.9976 - lr: 3.0000e-04\n",
      "Epoch 16/20\n",
      "709/709 [==============================] - 29s 41ms/step - loss: 0.0104 - accuracy: 0.9965 - val_loss: 0.0154 - val_accuracy: 0.9927 - lr: 3.0000e-04\n",
      "Epoch 17/20\n",
      "709/709 [==============================] - 29s 41ms/step - loss: 0.0106 - accuracy: 0.9959 - val_loss: 8.2993e-04 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 18/20\n",
      "709/709 [==============================] - 29s 41ms/step - loss: 0.0076 - accuracy: 0.9981 - val_loss: 0.0012 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 19/20\n",
      "709/709 [==============================] - 29s 41ms/step - loss: 0.0026 - accuracy: 0.9994 - val_loss: 0.0030 - val_accuracy: 0.9976 - lr: 6.0000e-05\n",
      "Epoch 20/20\n",
      "709/709 [==============================] - 29s 41ms/step - loss: 0.0012 - accuracy: 0.9996 - val_loss: 1.7294e-04 - val_accuracy: 1.0000 - lr: 6.0000e-05\n",
      "Epoch 1/20\n",
      "715/715 [==============================] - 31s 42ms/step - loss: 0.4313 - accuracy: 0.8230 - val_loss: 0.2324 - val_accuracy: 0.9121 - lr: 3.0000e-04\n",
      "Epoch 2/20\n",
      "715/715 [==============================] - 29s 41ms/step - loss: 0.2815 - accuracy: 0.8945 - val_loss: 0.1011 - val_accuracy: 0.9758 - lr: 3.0000e-04\n",
      "Epoch 3/20\n",
      "715/715 [==============================] - 29s 41ms/step - loss: 0.1327 - accuracy: 0.9519 - val_loss: 0.3193 - val_accuracy: 0.8939 - lr: 3.0000e-04\n",
      "Epoch 4/20\n",
      "715/715 [==============================] - 29s 41ms/step - loss: 0.0810 - accuracy: 0.9720 - val_loss: 0.0308 - val_accuracy: 0.9848 - lr: 3.0000e-04\n",
      "Epoch 5/20\n",
      "715/715 [==============================] - 29s 41ms/step - loss: 0.0581 - accuracy: 0.9817 - val_loss: 0.0359 - val_accuracy: 0.9909 - lr: 3.0000e-04\n",
      "Epoch 6/20\n",
      "715/715 [==============================] - 29s 41ms/step - loss: 0.0567 - accuracy: 0.9815 - val_loss: 0.0286 - val_accuracy: 0.9909 - lr: 3.0000e-04\n",
      "Epoch 7/20\n",
      "715/715 [==============================] - 29s 41ms/step - loss: 0.0389 - accuracy: 0.9877 - val_loss: 0.0876 - val_accuracy: 0.9636 - lr: 3.0000e-04\n",
      "Epoch 8/20\n",
      "715/715 [==============================] - 29s 41ms/step - loss: 0.0308 - accuracy: 0.9891 - val_loss: 0.0491 - val_accuracy: 0.9727 - lr: 3.0000e-04\n",
      "Epoch 9/20\n",
      "715/715 [==============================] - 29s 40ms/step - loss: 0.0232 - accuracy: 0.9929 - val_loss: 0.0105 - val_accuracy: 0.9939 - lr: 3.0000e-04\n",
      "Epoch 10/20\n",
      "715/715 [==============================] - 29s 41ms/step - loss: 0.0231 - accuracy: 0.9931 - val_loss: 0.0101 - val_accuracy: 0.9939 - lr: 3.0000e-04\n",
      "Epoch 11/20\n",
      "715/715 [==============================] - 29s 41ms/step - loss: 0.0171 - accuracy: 0.9961 - val_loss: 0.0083 - val_accuracy: 0.9939 - lr: 3.0000e-04\n",
      "Epoch 12/20\n",
      "715/715 [==============================] - 29s 40ms/step - loss: 0.0190 - accuracy: 0.9948 - val_loss: 0.0109 - val_accuracy: 0.9909 - lr: 3.0000e-04\n",
      "Epoch 13/20\n",
      "715/715 [==============================] - 29s 40ms/step - loss: 0.0184 - accuracy: 0.9942 - val_loss: 9.9979e-04 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 14/20\n",
      "715/715 [==============================] - 29s 41ms/step - loss: 0.0182 - accuracy: 0.9939 - val_loss: 0.0288 - val_accuracy: 0.9939 - lr: 3.0000e-04\n",
      "Epoch 15/20\n",
      "715/715 [==============================] - 29s 41ms/step - loss: 0.0139 - accuracy: 0.9962 - val_loss: 0.1159 - val_accuracy: 0.9697 - lr: 3.0000e-04\n",
      "Epoch 16/20\n",
      "715/715 [==============================] - 29s 41ms/step - loss: 0.0173 - accuracy: 0.9954 - val_loss: 0.0512 - val_accuracy: 0.9788 - lr: 3.0000e-04\n",
      "Epoch 17/20\n",
      "715/715 [==============================] - 29s 41ms/step - loss: 0.0060 - accuracy: 0.9982 - val_loss: 2.1799e-04 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 18/20\n",
      "715/715 [==============================] - 29s 41ms/step - loss: 0.0088 - accuracy: 0.9975 - val_loss: 7.6201e-04 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 19/20\n",
      "715/715 [==============================] - 29s 41ms/step - loss: 0.0206 - accuracy: 0.9934 - val_loss: 0.0013 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 20/20\n",
      "715/715 [==============================] - 29s 40ms/step - loss: 0.0083 - accuracy: 0.9978 - val_loss: 0.0220 - val_accuracy: 0.9909 - lr: 3.0000e-04\n",
      "Epoch 1/20\n",
      "710/710 [==============================] - 30s 41ms/step - loss: 0.4327 - accuracy: 0.8290 - val_loss: 0.3552 - val_accuracy: 0.8791 - lr: 3.0000e-04\n",
      "Epoch 2/20\n",
      "710/710 [==============================] - 29s 41ms/step - loss: 0.2975 - accuracy: 0.8852 - val_loss: 0.1902 - val_accuracy: 0.9295 - lr: 3.0000e-04\n",
      "Epoch 3/20\n",
      "710/710 [==============================] - 29s 40ms/step - loss: 0.1643 - accuracy: 0.9373 - val_loss: 0.1863 - val_accuracy: 0.9673 - lr: 3.0000e-04\n",
      "Epoch 4/20\n",
      "710/710 [==============================] - 29s 40ms/step - loss: 0.0909 - accuracy: 0.9684 - val_loss: 0.0839 - val_accuracy: 0.9773 - lr: 3.0000e-04\n",
      "Epoch 5/20\n",
      "710/710 [==============================] - 29s 42ms/step - loss: 0.0786 - accuracy: 0.9730 - val_loss: 0.1256 - val_accuracy: 0.9521 - lr: 3.0000e-04\n",
      "Epoch 6/20\n",
      "710/710 [==============================] - 29s 41ms/step - loss: 0.0595 - accuracy: 0.9809 - val_loss: 0.0733 - val_accuracy: 0.9773 - lr: 3.0000e-04\n",
      "Epoch 7/20\n",
      "710/710 [==============================] - 29s 41ms/step - loss: 0.0442 - accuracy: 0.9855 - val_loss: 0.1172 - val_accuracy: 0.9698 - lr: 3.0000e-04\n",
      "Epoch 8/20\n",
      "710/710 [==============================] - 29s 41ms/step - loss: 0.0404 - accuracy: 0.9878 - val_loss: 0.1170 - val_accuracy: 0.9798 - lr: 3.0000e-04\n",
      "Epoch 9/20\n",
      "710/710 [==============================] - 29s 41ms/step - loss: 0.0286 - accuracy: 0.9908 - val_loss: 0.1458 - val_accuracy: 0.9647 - lr: 3.0000e-04\n",
      "Epoch 10/20\n",
      "710/710 [==============================] - 29s 41ms/step - loss: 0.0489 - accuracy: 0.9854 - val_loss: 0.0858 - val_accuracy: 0.9723 - lr: 3.0000e-04\n",
      "Epoch 11/20\n",
      "710/710 [==============================] - 29s 41ms/step - loss: 0.0328 - accuracy: 0.9908 - val_loss: 0.0570 - val_accuracy: 0.9874 - lr: 3.0000e-04\n",
      "Epoch 12/20\n",
      "710/710 [==============================] - 29s 41ms/step - loss: 0.0195 - accuracy: 0.9945 - val_loss: 0.1260 - val_accuracy: 0.9698 - lr: 3.0000e-04\n",
      "Epoch 13/20\n",
      "710/710 [==============================] - 29s 41ms/step - loss: 0.0128 - accuracy: 0.9963 - val_loss: 0.2464 - val_accuracy: 0.9698 - lr: 3.0000e-04\n",
      "Epoch 14/20\n",
      "710/710 [==============================] - 29s 41ms/step - loss: 0.0225 - accuracy: 0.9939 - val_loss: 0.1018 - val_accuracy: 0.9824 - lr: 3.0000e-04\n",
      "Epoch 15/20\n",
      "710/710 [==============================] - 29s 41ms/step - loss: 0.0172 - accuracy: 0.9950 - val_loss: 0.0754 - val_accuracy: 0.9773 - lr: 3.0000e-04\n",
      "Epoch 16/20\n",
      "710/710 [==============================] - 29s 41ms/step - loss: 0.0140 - accuracy: 0.9957 - val_loss: 0.1037 - val_accuracy: 0.9723 - lr: 3.0000e-04\n",
      "Epoch 17/20\n",
      "710/710 [==============================] - 29s 41ms/step - loss: 0.0114 - accuracy: 0.9974 - val_loss: 0.1373 - val_accuracy: 0.9798 - lr: 3.0000e-04\n",
      "Epoch 18/20\n",
      "710/710 [==============================] - 29s 41ms/step - loss: 0.0136 - accuracy: 0.9958 - val_loss: 0.0880 - val_accuracy: 0.9748 - lr: 3.0000e-04\n",
      "Epoch 19/20\n",
      "710/710 [==============================] - 29s 41ms/step - loss: 0.0147 - accuracy: 0.9955 - val_loss: 0.0738 - val_accuracy: 0.9874 - lr: 3.0000e-04\n",
      "Epoch 20/20\n",
      "710/710 [==============================] - 29s 41ms/step - loss: 0.0079 - accuracy: 0.9974 - val_loss: 0.1845 - val_accuracy: 0.9723 - lr: 3.0000e-04\n",
      "Epoch 1/20\n",
      "709/709 [==============================] - 31s 42ms/step - loss: 0.4186 - accuracy: 0.8372 - val_loss: 0.5780 - val_accuracy: 0.7560 - lr: 3.0000e-04\n",
      "Epoch 2/20\n",
      "709/709 [==============================] - 29s 41ms/step - loss: 0.2867 - accuracy: 0.8949 - val_loss: 0.2122 - val_accuracy: 0.9179 - lr: 3.0000e-04\n",
      "Epoch 3/20\n",
      "709/709 [==============================] - 29s 41ms/step - loss: 0.1453 - accuracy: 0.9458 - val_loss: 0.1336 - val_accuracy: 0.9324 - lr: 3.0000e-04\n",
      "Epoch 4/20\n",
      "709/709 [==============================] - 29s 40ms/step - loss: 0.0902 - accuracy: 0.9668 - val_loss: 0.2150 - val_accuracy: 0.9469 - lr: 3.0000e-04\n",
      "Epoch 5/20\n",
      "709/709 [==============================] - 29s 40ms/step - loss: 0.2469 - accuracy: 0.9005 - val_loss: 0.1312 - val_accuracy: 0.9541 - lr: 3.0000e-04\n",
      "Epoch 6/20\n",
      "709/709 [==============================] - 29s 41ms/step - loss: 0.1903 - accuracy: 0.9272 - val_loss: 1.0366 - val_accuracy: 0.7077 - lr: 3.0000e-04\n",
      "Epoch 7/20\n",
      "709/709 [==============================] - 29s 41ms/step - loss: 0.1158 - accuracy: 0.9586 - val_loss: 0.3399 - val_accuracy: 0.8889 - lr: 3.0000e-04\n",
      "Epoch 8/20\n",
      "709/709 [==============================] - 29s 41ms/step - loss: 0.0869 - accuracy: 0.9716 - val_loss: 0.2636 - val_accuracy: 0.9324 - lr: 3.0000e-04\n",
      "Epoch 9/20\n",
      "709/709 [==============================] - 29s 41ms/step - loss: 0.0617 - accuracy: 0.9790 - val_loss: 0.0405 - val_accuracy: 0.9831 - lr: 3.0000e-04\n",
      "Epoch 10/20\n",
      "709/709 [==============================] - 29s 41ms/step - loss: 0.0523 - accuracy: 0.9817 - val_loss: 0.0431 - val_accuracy: 0.9758 - lr: 3.0000e-04\n",
      "Epoch 11/20\n",
      "709/709 [==============================] - 29s 42ms/step - loss: 0.0414 - accuracy: 0.9862 - val_loss: 0.0649 - val_accuracy: 0.9855 - lr: 3.0000e-04\n",
      "Epoch 12/20\n",
      "709/709 [==============================] - 29s 41ms/step - loss: 0.0402 - accuracy: 0.9874 - val_loss: 0.0530 - val_accuracy: 0.9879 - lr: 3.0000e-04\n",
      "Epoch 13/20\n",
      "709/709 [==============================] - 29s 41ms/step - loss: 0.0371 - accuracy: 0.9878 - val_loss: 0.0472 - val_accuracy: 0.9879 - lr: 3.0000e-04\n",
      "Epoch 14/20\n",
      "709/709 [==============================] - 29s 41ms/step - loss: 0.0303 - accuracy: 0.9901 - val_loss: 0.0564 - val_accuracy: 0.9831 - lr: 3.0000e-04\n",
      "Epoch 15/20\n",
      "709/709 [==============================] - 29s 41ms/step - loss: 0.0289 - accuracy: 0.9895 - val_loss: 0.0578 - val_accuracy: 0.9758 - lr: 3.0000e-04\n",
      "Epoch 16/20\n",
      "709/709 [==============================] - 29s 41ms/step - loss: 0.0259 - accuracy: 0.9914 - val_loss: 0.0421 - val_accuracy: 0.9855 - lr: 3.0000e-04\n",
      "Epoch 17/20\n",
      "709/709 [==============================] - 29s 40ms/step - loss: 0.0245 - accuracy: 0.9927 - val_loss: 0.0527 - val_accuracy: 0.9758 - lr: 3.0000e-04\n",
      "Epoch 18/20\n",
      "709/709 [==============================] - 29s 41ms/step - loss: 0.0265 - accuracy: 0.9909 - val_loss: 0.0281 - val_accuracy: 0.9903 - lr: 3.0000e-04\n",
      "Epoch 19/20\n",
      "709/709 [==============================] - 29s 41ms/step - loss: 0.0235 - accuracy: 0.9925 - val_loss: 0.0395 - val_accuracy: 0.9831 - lr: 3.0000e-04\n",
      "Epoch 20/20\n",
      "709/709 [==============================] - 29s 41ms/step - loss: 0.0181 - accuracy: 0.9942 - val_loss: 0.0304 - val_accuracy: 0.9903 - lr: 3.0000e-04\n",
      "Epoch 1/20\n",
      "716/716 [==============================] - 31s 41ms/step - loss: 0.4163 - accuracy: 0.8361 - val_loss: 0.1612 - val_accuracy: 0.9649 - lr: 3.0000e-04\n",
      "Epoch 2/20\n",
      "716/716 [==============================] - 29s 40ms/step - loss: 0.2699 - accuracy: 0.8956 - val_loss: 0.0399 - val_accuracy: 0.9872 - lr: 3.0000e-04\n",
      "Epoch 3/20\n",
      "716/716 [==============================] - 30s 42ms/step - loss: 0.1501 - accuracy: 0.9438 - val_loss: 0.0367 - val_accuracy: 0.9840 - lr: 3.0000e-04\n",
      "Epoch 4/20\n",
      "716/716 [==============================] - 29s 41ms/step - loss: 0.1082 - accuracy: 0.9622 - val_loss: 0.0492 - val_accuracy: 0.9872 - lr: 3.0000e-04\n",
      "Epoch 5/20\n",
      "716/716 [==============================] - 29s 40ms/step - loss: 0.0650 - accuracy: 0.9785 - val_loss: 0.0191 - val_accuracy: 0.9968 - lr: 3.0000e-04\n",
      "Epoch 6/20\n",
      "716/716 [==============================] - 29s 41ms/step - loss: 0.0493 - accuracy: 0.9845 - val_loss: 0.0333 - val_accuracy: 0.9872 - lr: 3.0000e-04\n",
      "Epoch 7/20\n",
      "716/716 [==============================] - 29s 41ms/step - loss: 0.0364 - accuracy: 0.9887 - val_loss: 0.0560 - val_accuracy: 0.9776 - lr: 3.0000e-04\n",
      "Epoch 8/20\n",
      "716/716 [==============================] - 29s 41ms/step - loss: 0.0345 - accuracy: 0.9884 - val_loss: 0.0267 - val_accuracy: 0.9872 - lr: 3.0000e-04\n",
      "Epoch 9/20\n",
      "716/716 [==============================] - 29s 41ms/step - loss: 0.0248 - accuracy: 0.9924 - val_loss: 0.0026 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 10/20\n",
      "716/716 [==============================] - 30s 41ms/step - loss: 0.0216 - accuracy: 0.9932 - val_loss: 0.0119 - val_accuracy: 0.9904 - lr: 3.0000e-04\n",
      "Epoch 11/20\n",
      "716/716 [==============================] - 30s 41ms/step - loss: 0.0215 - accuracy: 0.9934 - val_loss: 0.0018 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 12/20\n",
      "716/716 [==============================] - 29s 40ms/step - loss: 0.0126 - accuracy: 0.9967 - val_loss: 0.0084 - val_accuracy: 0.9968 - lr: 3.0000e-04\n",
      "Epoch 13/20\n",
      "716/716 [==============================] - 29s 41ms/step - loss: 0.0139 - accuracy: 0.9952 - val_loss: 0.0113 - val_accuracy: 0.9968 - lr: 3.0000e-04\n",
      "Epoch 14/20\n",
      "716/716 [==============================] - 29s 41ms/step - loss: 0.0135 - accuracy: 0.9961 - val_loss: 1.0233e-04 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 15/20\n",
      "716/716 [==============================] - 30s 41ms/step - loss: 0.0105 - accuracy: 0.9969 - val_loss: 5.4499e-04 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 16/20\n",
      "716/716 [==============================] - 29s 41ms/step - loss: 0.0078 - accuracy: 0.9982 - val_loss: 1.1534e-04 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 17/20\n",
      "716/716 [==============================] - 29s 41ms/step - loss: 0.0171 - accuracy: 0.9944 - val_loss: 4.8419e-04 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 18/20\n",
      "716/716 [==============================] - 29s 41ms/step - loss: 0.0164 - accuracy: 0.9946 - val_loss: 1.4547e-04 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 19/20\n",
      "716/716 [==============================] - 29s 41ms/step - loss: 0.0047 - accuracy: 0.9990 - val_loss: 2.6407e-04 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 20/20\n",
      "716/716 [==============================] - 29s 40ms/step - loss: 0.0051 - accuracy: 0.9988 - val_loss: 7.4258e-05 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 1/20\n",
      "711/711 [==============================] - 31s 41ms/step - loss: 0.4355 - accuracy: 0.8216 - val_loss: 0.2688 - val_accuracy: 0.8852 - lr: 3.0000e-04\n",
      "Epoch 2/20\n",
      "711/711 [==============================] - 29s 41ms/step - loss: 0.2968 - accuracy: 0.8858 - val_loss: 0.1299 - val_accuracy: 0.9541 - lr: 3.0000e-04\n",
      "Epoch 3/20\n",
      "711/711 [==============================] - 29s 41ms/step - loss: 0.1684 - accuracy: 0.9402 - val_loss: 0.1174 - val_accuracy: 0.9515 - lr: 3.0000e-04\n",
      "Epoch 4/20\n",
      "711/711 [==============================] - 29s 41ms/step - loss: 0.0956 - accuracy: 0.9670 - val_loss: 0.0617 - val_accuracy: 0.9745 - lr: 3.0000e-04\n",
      "Epoch 5/20\n",
      "711/711 [==============================] - 29s 41ms/step - loss: 0.0750 - accuracy: 0.9743 - val_loss: 0.0586 - val_accuracy: 0.9847 - lr: 3.0000e-04\n",
      "Epoch 6/20\n",
      "711/711 [==============================] - 29s 41ms/step - loss: 0.0604 - accuracy: 0.9807 - val_loss: 0.0623 - val_accuracy: 0.9694 - lr: 3.0000e-04\n",
      "Epoch 7/20\n",
      "711/711 [==============================] - 29s 41ms/step - loss: 0.0520 - accuracy: 0.9835 - val_loss: 0.0223 - val_accuracy: 0.9923 - lr: 3.0000e-04\n",
      "Epoch 8/20\n",
      "711/711 [==============================] - 29s 41ms/step - loss: 0.0437 - accuracy: 0.9868 - val_loss: 0.1333 - val_accuracy: 0.9515 - lr: 3.0000e-04\n",
      "Epoch 9/20\n",
      "711/711 [==============================] - 29s 41ms/step - loss: 0.0346 - accuracy: 0.9898 - val_loss: 0.0095 - val_accuracy: 0.9974 - lr: 3.0000e-04\n",
      "Epoch 10/20\n",
      "711/711 [==============================] - 29s 41ms/step - loss: 0.0279 - accuracy: 0.9914 - val_loss: 0.0045 - val_accuracy: 0.9974 - lr: 3.0000e-04\n",
      "Epoch 11/20\n",
      "711/711 [==============================] - 29s 41ms/step - loss: 0.0255 - accuracy: 0.9918 - val_loss: 0.0010 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 12/20\n",
      "711/711 [==============================] - 29s 41ms/step - loss: 0.0252 - accuracy: 0.9920 - val_loss: 0.0054 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 13/20\n",
      "711/711 [==============================] - 29s 40ms/step - loss: 0.0162 - accuracy: 0.9953 - val_loss: 0.0662 - val_accuracy: 0.9796 - lr: 3.0000e-04\n",
      "Epoch 14/20\n",
      "711/711 [==============================] - 29s 41ms/step - loss: 0.0239 - accuracy: 0.9927 - val_loss: 0.0982 - val_accuracy: 0.9643 - lr: 3.0000e-04\n",
      "Epoch 15/20\n",
      "711/711 [==============================] - 30s 42ms/step - loss: 0.0171 - accuracy: 0.9952 - val_loss: 0.0021 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 16/20\n",
      "711/711 [==============================] - 29s 41ms/step - loss: 0.0106 - accuracy: 0.9967 - val_loss: 0.0214 - val_accuracy: 0.9923 - lr: 3.0000e-04\n",
      "Epoch 17/20\n",
      "711/711 [==============================] - 29s 41ms/step - loss: 0.0213 - accuracy: 0.9943 - val_loss: 7.8638e-04 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 18/20\n",
      "711/711 [==============================] - 29s 41ms/step - loss: 0.0058 - accuracy: 0.9983 - val_loss: 0.1441 - val_accuracy: 0.9847 - lr: 3.0000e-04\n",
      "Epoch 19/20\n",
      "711/711 [==============================] - 29s 41ms/step - loss: 0.0161 - accuracy: 0.9952 - val_loss: 0.0014 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 20/20\n",
      "711/711 [==============================] - 29s 40ms/step - loss: 0.0044 - accuracy: 0.9987 - val_loss: 0.0183 - val_accuracy: 0.9949 - lr: 3.0000e-04\n",
      "Epoch 1/20\n",
      "704/704 [==============================] - 31s 42ms/step - loss: 0.4258 - accuracy: 0.8298 - val_loss: 0.3811 - val_accuracy: 0.8360 - lr: 3.0000e-04\n",
      "Epoch 2/20\n",
      "704/704 [==============================] - 29s 41ms/step - loss: 0.2993 - accuracy: 0.8889 - val_loss: 0.1411 - val_accuracy: 0.9520 - lr: 3.0000e-04\n",
      "Epoch 3/20\n",
      "704/704 [==============================] - 28s 40ms/step - loss: 0.1675 - accuracy: 0.9398 - val_loss: 0.0824 - val_accuracy: 0.9680 - lr: 3.0000e-04\n",
      "Epoch 4/20\n",
      "704/704 [==============================] - 29s 41ms/step - loss: 0.1423 - accuracy: 0.9481 - val_loss: 0.1602 - val_accuracy: 0.9480 - lr: 3.0000e-04\n",
      "Epoch 5/20\n",
      "704/704 [==============================] - 29s 41ms/step - loss: 0.1005 - accuracy: 0.9646 - val_loss: 0.0317 - val_accuracy: 0.9880 - lr: 3.0000e-04\n",
      "Epoch 6/20\n",
      "704/704 [==============================] - 29s 41ms/step - loss: 0.0692 - accuracy: 0.9796 - val_loss: 0.0191 - val_accuracy: 0.9960 - lr: 3.0000e-04\n",
      "Epoch 7/20\n",
      "704/704 [==============================] - 29s 41ms/step - loss: 0.0513 - accuracy: 0.9828 - val_loss: 0.0379 - val_accuracy: 0.9880 - lr: 3.0000e-04\n",
      "Epoch 8/20\n",
      "704/704 [==============================] - 28s 40ms/step - loss: 0.0460 - accuracy: 0.9849 - val_loss: 0.0165 - val_accuracy: 0.9920 - lr: 3.0000e-04\n",
      "Epoch 9/20\n",
      "704/704 [==============================] - 29s 41ms/step - loss: 0.0382 - accuracy: 0.9881 - val_loss: 0.0255 - val_accuracy: 0.9940 - lr: 3.0000e-04\n",
      "Epoch 10/20\n",
      "704/704 [==============================] - 29s 42ms/step - loss: 0.0387 - accuracy: 0.9887 - val_loss: 0.0059 - val_accuracy: 0.9980 - lr: 3.0000e-04\n",
      "Epoch 11/20\n",
      "704/704 [==============================] - 29s 41ms/step - loss: 0.0263 - accuracy: 0.9923 - val_loss: 6.9411e-04 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 12/20\n",
      "704/704 [==============================] - 29s 41ms/step - loss: 0.0182 - accuracy: 0.9943 - val_loss: 0.0043 - val_accuracy: 0.9980 - lr: 3.0000e-04\n",
      "Epoch 13/20\n",
      "704/704 [==============================] - 28s 40ms/step - loss: 0.0231 - accuracy: 0.9930 - val_loss: 0.0021 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 14/20\n",
      "704/704 [==============================] - 29s 41ms/step - loss: 0.0177 - accuracy: 0.9951 - val_loss: 0.0174 - val_accuracy: 0.9940 - lr: 3.0000e-04\n",
      "Epoch 15/20\n",
      "704/704 [==============================] - 29s 41ms/step - loss: 0.0163 - accuracy: 0.9956 - val_loss: 0.0180 - val_accuracy: 0.9940 - lr: 3.0000e-04\n",
      "Epoch 16/20\n",
      "704/704 [==============================] - 29s 41ms/step - loss: 0.0160 - accuracy: 0.9952 - val_loss: 0.0101 - val_accuracy: 0.9920 - lr: 3.0000e-04\n",
      "Epoch 17/20\n",
      "704/704 [==============================] - 29s 41ms/step - loss: 0.0147 - accuracy: 0.9954 - val_loss: 0.0061 - val_accuracy: 0.9980 - lr: 3.0000e-04\n",
      "Epoch 18/20\n",
      "704/704 [==============================] - 29s 42ms/step - loss: 0.0175 - accuracy: 0.9939 - val_loss: 0.0016 - val_accuracy: 0.9980 - lr: 3.0000e-04\n",
      "Epoch 19/20\n",
      "704/704 [==============================] - 29s 42ms/step - loss: 0.0126 - accuracy: 0.9967 - val_loss: 4.3071e-04 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 20/20\n",
      "704/704 [==============================] - 29s 42ms/step - loss: 0.0148 - accuracy: 0.9955 - val_loss: 0.0026 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 1/20\n",
      "712/712 [==============================] - 31s 42ms/step - loss: 0.4352 - accuracy: 0.8255 - val_loss: 0.3053 - val_accuracy: 0.8833 - lr: 3.0000e-04\n",
      "Epoch 2/20\n",
      "712/712 [==============================] - 29s 41ms/step - loss: 0.3366 - accuracy: 0.8687 - val_loss: 0.2586 - val_accuracy: 0.8912 - lr: 3.0000e-04\n",
      "Epoch 3/20\n",
      "712/712 [==============================] - 29s 40ms/step - loss: 0.1820 - accuracy: 0.9336 - val_loss: 0.1611 - val_accuracy: 0.9390 - lr: 3.0000e-04\n",
      "Epoch 4/20\n",
      "712/712 [==============================] - 29s 41ms/step - loss: 0.1138 - accuracy: 0.9604 - val_loss: 0.2257 - val_accuracy: 0.9257 - lr: 3.0000e-04\n",
      "Epoch 5/20\n",
      "712/712 [==============================] - 29s 41ms/step - loss: 0.2311 - accuracy: 0.9101 - val_loss: 0.3716 - val_accuracy: 0.8833 - lr: 3.0000e-04\n",
      "Epoch 6/20\n",
      "712/712 [==============================] - 29s 40ms/step - loss: 0.2997 - accuracy: 0.8808 - val_loss: 0.3781 - val_accuracy: 0.8488 - lr: 3.0000e-04\n",
      "Epoch 7/20\n",
      "712/712 [==============================] - 30s 41ms/step - loss: 0.2318 - accuracy: 0.9085 - val_loss: 0.3146 - val_accuracy: 0.8753 - lr: 3.0000e-04\n",
      "Epoch 8/20\n",
      "712/712 [==============================] - 29s 41ms/step - loss: 0.1729 - accuracy: 0.9332 - val_loss: 0.1521 - val_accuracy: 0.9284 - lr: 3.0000e-04\n",
      "Epoch 9/20\n",
      "712/712 [==============================] - 30s 42ms/step - loss: 0.1514 - accuracy: 0.9451 - val_loss: 0.0538 - val_accuracy: 0.9920 - lr: 3.0000e-04\n",
      "Epoch 10/20\n",
      "712/712 [==============================] - 29s 41ms/step - loss: 0.0716 - accuracy: 0.9740 - val_loss: 0.1302 - val_accuracy: 0.9655 - lr: 3.0000e-04\n",
      "Epoch 11/20\n",
      "712/712 [==============================] - 30s 42ms/step - loss: 0.0523 - accuracy: 0.9811 - val_loss: 0.0452 - val_accuracy: 0.9947 - lr: 3.0000e-04\n",
      "Epoch 12/20\n",
      "712/712 [==============================] - 29s 41ms/step - loss: 0.0428 - accuracy: 0.9853 - val_loss: 0.1056 - val_accuracy: 0.9735 - lr: 3.0000e-04\n",
      "Epoch 13/20\n",
      "712/712 [==============================] - 28s 40ms/step - loss: 0.0353 - accuracy: 0.9876 - val_loss: 0.7911 - val_accuracy: 0.8064 - lr: 3.0000e-04\n",
      "Epoch 14/20\n",
      "712/712 [==============================] - 29s 41ms/step - loss: 0.1161 - accuracy: 0.9594 - val_loss: 0.9354 - val_accuracy: 0.7003 - lr: 3.0000e-04\n",
      "Epoch 15/20\n",
      "712/712 [==============================] - 30s 41ms/step - loss: 0.2991 - accuracy: 0.8828 - val_loss: 0.3608 - val_accuracy: 0.8647 - lr: 3.0000e-04\n",
      "Epoch 16/20\n",
      "712/712 [==============================] - 29s 41ms/step - loss: 0.1227 - accuracy: 0.9556 - val_loss: 0.1220 - val_accuracy: 0.9443 - lr: 3.0000e-04\n",
      "Epoch 17/20\n",
      "712/712 [==============================] - 29s 41ms/step - loss: 0.0581 - accuracy: 0.9803 - val_loss: 0.0800 - val_accuracy: 0.9629 - lr: 3.0000e-04\n",
      "Epoch 18/20\n",
      "712/712 [==============================] - 29s 41ms/step - loss: 0.0362 - accuracy: 0.9881 - val_loss: 0.0972 - val_accuracy: 0.9602 - lr: 3.0000e-04\n",
      "Epoch 19/20\n",
      "712/712 [==============================] - 29s 41ms/step - loss: 0.0267 - accuracy: 0.9906 - val_loss: 0.0373 - val_accuracy: 0.9947 - lr: 3.0000e-04\n",
      "Epoch 20/20\n",
      "712/712 [==============================] - 29s 41ms/step - loss: 0.0249 - accuracy: 0.9905 - val_loss: 0.2044 - val_accuracy: 0.9204 - lr: 3.0000e-04\n",
      "Epoch 1/20\n",
      "712/712 [==============================] - 32s 42ms/step - loss: 0.4182 - accuracy: 0.8314 - val_loss: 0.2436 - val_accuracy: 0.8790 - lr: 3.0000e-04\n",
      "Epoch 2/20\n",
      "712/712 [==============================] - 29s 41ms/step - loss: 0.2586 - accuracy: 0.9015 - val_loss: 0.1599 - val_accuracy: 0.9355 - lr: 3.0000e-04\n",
      "Epoch 3/20\n",
      "712/712 [==============================] - 29s 40ms/step - loss: 0.1461 - accuracy: 0.9493 - val_loss: 0.4042 - val_accuracy: 0.9194 - lr: 3.0000e-04\n",
      "Epoch 4/20\n",
      "712/712 [==============================] - 29s 41ms/step - loss: 0.0932 - accuracy: 0.9670 - val_loss: 0.1574 - val_accuracy: 0.9651 - lr: 3.0000e-04\n",
      "Epoch 5/20\n",
      "712/712 [==============================] - 29s 41ms/step - loss: 0.0665 - accuracy: 0.9777 - val_loss: 0.0724 - val_accuracy: 0.9704 - lr: 3.0000e-04\n",
      "Epoch 6/20\n",
      "712/712 [==============================] - 30s 42ms/step - loss: 0.0477 - accuracy: 0.9841 - val_loss: 0.0291 - val_accuracy: 0.9892 - lr: 3.0000e-04\n",
      "Epoch 7/20\n",
      "712/712 [==============================] - 30s 42ms/step - loss: 0.0468 - accuracy: 0.9847 - val_loss: 0.0047 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 8/20\n",
      "712/712 [==============================] - 29s 41ms/step - loss: 0.0501 - accuracy: 0.9830 - val_loss: 0.0397 - val_accuracy: 0.9866 - lr: 3.0000e-04\n",
      "Epoch 9/20\n",
      "712/712 [==============================] - 30s 42ms/step - loss: 0.0348 - accuracy: 0.9901 - val_loss: 0.0363 - val_accuracy: 0.9892 - lr: 3.0000e-04\n",
      "Epoch 10/20\n",
      "712/712 [==============================] - 29s 41ms/step - loss: 0.0224 - accuracy: 0.9940 - val_loss: 0.0189 - val_accuracy: 0.9973 - lr: 3.0000e-04\n",
      "Epoch 11/20\n",
      "712/712 [==============================] - 29s 41ms/step - loss: 0.0282 - accuracy: 0.9925 - val_loss: 0.0073 - val_accuracy: 0.9973 - lr: 3.0000e-04\n",
      "Epoch 12/20\n",
      "712/712 [==============================] - 29s 41ms/step - loss: 0.0221 - accuracy: 0.9932 - val_loss: 0.0030 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 13/20\n",
      "712/712 [==============================] - 30s 42ms/step - loss: 0.0136 - accuracy: 0.9960 - val_loss: 0.0209 - val_accuracy: 0.9946 - lr: 3.0000e-04\n",
      "Epoch 14/20\n",
      "712/712 [==============================] - 29s 41ms/step - loss: 0.0139 - accuracy: 0.9955 - val_loss: 0.0110 - val_accuracy: 0.9919 - lr: 3.0000e-04\n",
      "Epoch 15/20\n",
      "712/712 [==============================] - 30s 42ms/step - loss: 0.0177 - accuracy: 0.9936 - val_loss: 0.0047 - val_accuracy: 0.9973 - lr: 3.0000e-04\n",
      "Epoch 16/20\n",
      "712/712 [==============================] - 29s 41ms/step - loss: 0.0140 - accuracy: 0.9959 - val_loss: 0.0069 - val_accuracy: 0.9973 - lr: 3.0000e-04\n",
      "Epoch 17/20\n",
      "712/712 [==============================] - 30s 42ms/step - loss: 0.0090 - accuracy: 0.9977 - val_loss: 0.0049 - val_accuracy: 0.9973 - lr: 3.0000e-04\n",
      "Epoch 18/20\n",
      "712/712 [==============================] - 29s 41ms/step - loss: 0.0127 - accuracy: 0.9968 - val_loss: 0.0157 - val_accuracy: 0.9946 - lr: 3.0000e-04\n",
      "Epoch 19/20\n",
      "712/712 [==============================] - 29s 41ms/step - loss: 0.0060 - accuracy: 0.9982 - val_loss: 0.0298 - val_accuracy: 0.9973 - lr: 3.0000e-04\n",
      "Epoch 20/20\n",
      "712/712 [==============================] - 29s 40ms/step - loss: 0.0102 - accuracy: 0.9973 - val_loss: 0.0080 - val_accuracy: 0.9973 - lr: 3.0000e-04\n",
      "Epoch 1/20\n",
      "711/711 [==============================] - 29s 40ms/step - loss: 0.4236 - accuracy: 0.8316 - val_loss: 0.5264 - val_accuracy: 0.8025 - lr: 3.0000e-04\n",
      "Epoch 2/20\n",
      "711/711 [==============================] - 27s 38ms/step - loss: 0.2908 - accuracy: 0.8884 - val_loss: 0.5441 - val_accuracy: 0.7646 - lr: 3.0000e-04\n",
      "Epoch 3/20\n",
      "711/711 [==============================] - 28s 39ms/step - loss: 0.1532 - accuracy: 0.9432 - val_loss: 0.1829 - val_accuracy: 0.9418 - lr: 3.0000e-04\n",
      "Epoch 4/20\n",
      "711/711 [==============================] - 28s 39ms/step - loss: 0.3278 - accuracy: 0.8679 - val_loss: 0.3231 - val_accuracy: 0.8380 - lr: 3.0000e-04\n",
      "Epoch 5/20\n",
      "711/711 [==============================] - 28s 40ms/step - loss: 0.1748 - accuracy: 0.9380 - val_loss: 0.1808 - val_accuracy: 0.9544 - lr: 3.0000e-04\n",
      "Epoch 6/20\n",
      "711/711 [==============================] - 28s 40ms/step - loss: 0.0877 - accuracy: 0.9691 - val_loss: 0.1353 - val_accuracy: 0.9494 - lr: 3.0000e-04\n",
      "Epoch 7/20\n",
      "711/711 [==============================] - 28s 39ms/step - loss: 0.1044 - accuracy: 0.9630 - val_loss: 0.2526 - val_accuracy: 0.9190 - lr: 3.0000e-04\n",
      "Epoch 8/20\n",
      "711/711 [==============================] - 28s 39ms/step - loss: 0.0719 - accuracy: 0.9743 - val_loss: 0.2831 - val_accuracy: 0.9519 - lr: 3.0000e-04\n",
      "Epoch 9/20\n",
      "711/711 [==============================] - 28s 40ms/step - loss: 0.0497 - accuracy: 0.9825 - val_loss: 0.2697 - val_accuracy: 0.9443 - lr: 3.0000e-04\n",
      "Epoch 10/20\n",
      "711/711 [==============================] - 28s 40ms/step - loss: 0.0430 - accuracy: 0.9843 - val_loss: 0.6658 - val_accuracy: 0.8354 - lr: 3.0000e-04\n",
      "Epoch 11/20\n",
      "711/711 [==============================] - 28s 40ms/step - loss: 0.0339 - accuracy: 0.9889 - val_loss: 0.1799 - val_accuracy: 0.9544 - lr: 3.0000e-04\n",
      "Epoch 12/20\n",
      "711/711 [==============================] - 28s 39ms/step - loss: 0.0260 - accuracy: 0.9909 - val_loss: 0.4088 - val_accuracy: 0.8759 - lr: 3.0000e-04\n",
      "Epoch 13/20\n",
      "711/711 [==============================] - 28s 40ms/step - loss: 0.0289 - accuracy: 0.9891 - val_loss: 0.3143 - val_accuracy: 0.9519 - lr: 3.0000e-04\n",
      "Epoch 14/20\n",
      "711/711 [==============================] - 28s 40ms/step - loss: 0.0256 - accuracy: 0.9924 - val_loss: 0.2073 - val_accuracy: 0.9570 - lr: 3.0000e-04\n",
      "Epoch 15/20\n",
      "711/711 [==============================] - 28s 39ms/step - loss: 0.0274 - accuracy: 0.9916 - val_loss: 0.1127 - val_accuracy: 0.9519 - lr: 3.0000e-04\n",
      "Epoch 16/20\n",
      "711/711 [==============================] - 28s 39ms/step - loss: 0.0177 - accuracy: 0.9942 - val_loss: 0.4414 - val_accuracy: 0.9392 - lr: 3.0000e-04\n",
      "Epoch 17/20\n",
      "711/711 [==============================] - 28s 40ms/step - loss: 0.0217 - accuracy: 0.9930 - val_loss: 0.1282 - val_accuracy: 0.9722 - lr: 3.0000e-04\n",
      "Epoch 18/20\n",
      "711/711 [==============================] - 28s 39ms/step - loss: 0.0201 - accuracy: 0.9945 - val_loss: 0.0715 - val_accuracy: 0.9696 - lr: 3.0000e-04\n",
      "Epoch 19/20\n",
      "711/711 [==============================] - 28s 39ms/step - loss: 0.0167 - accuracy: 0.9938 - val_loss: 0.0430 - val_accuracy: 0.9823 - lr: 3.0000e-04\n",
      "Epoch 20/20\n",
      "711/711 [==============================] - 28s 39ms/step - loss: 0.0150 - accuracy: 0.9949 - val_loss: 0.0091 - val_accuracy: 0.9975 - lr: 3.0000e-04\n",
      "Epoch 1/20\n",
      "706/706 [==============================] - 30s 41ms/step - loss: 0.4167 - accuracy: 0.8291 - val_loss: 0.4195 - val_accuracy: 0.8376 - lr: 3.0000e-04\n",
      "Epoch 2/20\n",
      "706/706 [==============================] - 28s 40ms/step - loss: 0.2758 - accuracy: 0.8951 - val_loss: 0.2052 - val_accuracy: 0.9017 - lr: 3.0000e-04\n",
      "Epoch 3/20\n",
      "706/706 [==============================] - 28s 40ms/step - loss: 0.2002 - accuracy: 0.9230 - val_loss: 0.4142 - val_accuracy: 0.8312 - lr: 3.0000e-04\n",
      "Epoch 4/20\n",
      "706/706 [==============================] - 29s 41ms/step - loss: 0.0901 - accuracy: 0.9695 - val_loss: 0.0751 - val_accuracy: 0.9722 - lr: 3.0000e-04\n",
      "Epoch 5/20\n",
      "706/706 [==============================] - 29s 41ms/step - loss: 0.0539 - accuracy: 0.9815 - val_loss: 0.0248 - val_accuracy: 0.9893 - lr: 3.0000e-04\n",
      "Epoch 6/20\n",
      "706/706 [==============================] - 29s 41ms/step - loss: 0.0373 - accuracy: 0.9882 - val_loss: 0.0186 - val_accuracy: 0.9915 - lr: 3.0000e-04\n",
      "Epoch 7/20\n",
      "706/706 [==============================] - 29s 41ms/step - loss: 0.0261 - accuracy: 0.9923 - val_loss: 0.0132 - val_accuracy: 0.9979 - lr: 3.0000e-04\n",
      "Epoch 8/20\n",
      "706/706 [==============================] - 29s 41ms/step - loss: 0.0217 - accuracy: 0.9932 - val_loss: 0.0072 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 9/20\n",
      "706/706 [==============================] - 29s 41ms/step - loss: 0.0226 - accuracy: 0.9924 - val_loss: 0.0269 - val_accuracy: 0.9829 - lr: 3.0000e-04\n",
      "Epoch 10/20\n",
      "706/706 [==============================] - 29s 41ms/step - loss: 0.0154 - accuracy: 0.9953 - val_loss: 0.0376 - val_accuracy: 0.9936 - lr: 3.0000e-04\n",
      "Epoch 11/20\n",
      "706/706 [==============================] - 29s 41ms/step - loss: 0.0140 - accuracy: 0.9953 - val_loss: 9.0648e-04 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 12/20\n",
      "706/706 [==============================] - 29s 41ms/step - loss: 0.0164 - accuracy: 0.9943 - val_loss: 0.0173 - val_accuracy: 0.9957 - lr: 3.0000e-04\n",
      "Epoch 13/20\n",
      "706/706 [==============================] - 29s 41ms/step - loss: 0.0103 - accuracy: 0.9967 - val_loss: 0.0032 - val_accuracy: 0.9979 - lr: 3.0000e-04\n",
      "Epoch 14/20\n",
      "706/706 [==============================] - 29s 41ms/step - loss: 0.0131 - accuracy: 0.9963 - val_loss: 0.0057 - val_accuracy: 0.9979 - lr: 3.0000e-04\n",
      "Epoch 15/20\n",
      "706/706 [==============================] - 29s 41ms/step - loss: 0.0183 - accuracy: 0.9945 - val_loss: 0.0133 - val_accuracy: 0.9957 - lr: 3.0000e-04\n",
      "Epoch 16/20\n",
      "706/706 [==============================] - 29s 40ms/step - loss: 0.0079 - accuracy: 0.9973 - val_loss: 0.0158 - val_accuracy: 0.9936 - lr: 3.0000e-04\n",
      "Epoch 17/20\n",
      "706/706 [==============================] - 29s 42ms/step - loss: 0.0104 - accuracy: 0.9971 - val_loss: 0.0049 - val_accuracy: 0.9979 - lr: 3.0000e-04\n",
      "Epoch 18/20\n",
      "706/706 [==============================] - 29s 41ms/step - loss: 0.0105 - accuracy: 0.9967 - val_loss: 0.0206 - val_accuracy: 0.9893 - lr: 3.0000e-04\n",
      "Epoch 19/20\n",
      "706/706 [==============================] - 29s 41ms/step - loss: 0.0085 - accuracy: 0.9976 - val_loss: 0.0128 - val_accuracy: 0.9915 - lr: 3.0000e-04\n",
      "Epoch 20/20\n",
      "706/706 [==============================] - 30s 42ms/step - loss: 0.0108 - accuracy: 0.9966 - val_loss: 0.0076 - val_accuracy: 0.9936 - lr: 3.0000e-04\n",
      "Epoch 1/20\n",
      "706/706 [==============================] - 30s 41ms/step - loss: 0.4258 - accuracy: 0.8291 - val_loss: 0.2856 - val_accuracy: 0.9036 - lr: 3.0000e-04\n",
      "Epoch 2/20\n",
      "706/706 [==============================] - 29s 40ms/step - loss: 0.2847 - accuracy: 0.8922 - val_loss: 0.0857 - val_accuracy: 0.9722 - lr: 3.0000e-04\n",
      "Epoch 3/20\n",
      "706/706 [==============================] - 29s 41ms/step - loss: 0.1536 - accuracy: 0.9446 - val_loss: 0.0735 - val_accuracy: 0.9829 - lr: 3.0000e-04\n",
      "Epoch 4/20\n",
      "706/706 [==============================] - 29s 41ms/step - loss: 0.1197 - accuracy: 0.9568 - val_loss: 0.0411 - val_accuracy: 0.9914 - lr: 3.0000e-04\n",
      "Epoch 5/20\n",
      "706/706 [==============================] - 29s 41ms/step - loss: 0.0945 - accuracy: 0.9657 - val_loss: 0.0312 - val_accuracy: 0.9829 - lr: 3.0000e-04\n",
      "Epoch 6/20\n",
      "706/706 [==============================] - 29s 42ms/step - loss: 0.0596 - accuracy: 0.9804 - val_loss: 0.0372 - val_accuracy: 0.9807 - lr: 3.0000e-04\n",
      "Epoch 7/20\n",
      "706/706 [==============================] - 29s 41ms/step - loss: 0.0579 - accuracy: 0.9806 - val_loss: 0.0373 - val_accuracy: 0.9829 - lr: 3.0000e-04\n",
      "Epoch 8/20\n",
      "706/706 [==============================] - 29s 41ms/step - loss: 0.0433 - accuracy: 0.9855 - val_loss: 0.0111 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 9/20\n",
      "706/706 [==============================] - 28s 40ms/step - loss: 0.0332 - accuracy: 0.9899 - val_loss: 0.0034 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 10/20\n",
      "706/706 [==============================] - 29s 41ms/step - loss: 0.0271 - accuracy: 0.9927 - val_loss: 0.0076 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 11/20\n",
      "706/706 [==============================] - 29s 41ms/step - loss: 0.0224 - accuracy: 0.9932 - val_loss: 0.0018 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 12/20\n",
      "706/706 [==============================] - 29s 41ms/step - loss: 0.0215 - accuracy: 0.9939 - val_loss: 0.0026 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 13/20\n",
      "706/706 [==============================] - 29s 41ms/step - loss: 0.0271 - accuracy: 0.9914 - val_loss: 0.0154 - val_accuracy: 0.9914 - lr: 3.0000e-04\n",
      "Epoch 14/20\n",
      "706/706 [==============================] - 29s 41ms/step - loss: 0.0110 - accuracy: 0.9965 - val_loss: 0.0012 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 15/20\n",
      "706/706 [==============================] - 29s 41ms/step - loss: 0.0192 - accuracy: 0.9942 - val_loss: 0.0156 - val_accuracy: 0.9914 - lr: 3.0000e-04\n",
      "Epoch 16/20\n",
      "706/706 [==============================] - 29s 42ms/step - loss: 0.0102 - accuracy: 0.9966 - val_loss: 0.0096 - val_accuracy: 0.9979 - lr: 3.0000e-04\n",
      "Epoch 17/20\n",
      "706/706 [==============================] - 29s 41ms/step - loss: 0.0098 - accuracy: 0.9972 - val_loss: 0.0018 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 18/20\n",
      "706/706 [==============================] - 30s 42ms/step - loss: 0.0164 - accuracy: 0.9957 - val_loss: 0.0048 - val_accuracy: 0.9936 - lr: 3.0000e-04\n",
      "Epoch 19/20\n",
      "706/706 [==============================] - 29s 42ms/step - loss: 0.0158 - accuracy: 0.9950 - val_loss: 0.0019 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 20/20\n",
      "706/706 [==============================] - 29s 41ms/step - loss: 0.0067 - accuracy: 0.9981 - val_loss: 1.3857e-04 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 1/20\n",
      "717/717 [==============================] - 31s 41ms/step - loss: 0.4240 - accuracy: 0.8278 - val_loss: 0.6905 - val_accuracy: 0.8000 - lr: 3.0000e-04\n",
      "Epoch 2/20\n",
      "717/717 [==============================] - 29s 40ms/step - loss: 0.3048 - accuracy: 0.8841 - val_loss: 0.3916 - val_accuracy: 0.8793 - lr: 3.0000e-04\n",
      "Epoch 3/20\n",
      "717/717 [==============================] - 29s 40ms/step - loss: 0.1937 - accuracy: 0.9257 - val_loss: 1.3098 - val_accuracy: 0.5310 - lr: 3.0000e-04\n",
      "Epoch 4/20\n",
      "717/717 [==============================] - 30s 41ms/step - loss: 0.1158 - accuracy: 0.9600 - val_loss: 0.0642 - val_accuracy: 0.9724 - lr: 3.0000e-04\n",
      "Epoch 5/20\n",
      "717/717 [==============================] - 30s 41ms/step - loss: 0.0401 - accuracy: 0.9861 - val_loss: 0.0216 - val_accuracy: 0.9862 - lr: 3.0000e-04\n",
      "Epoch 6/20\n",
      "717/717 [==============================] - 29s 40ms/step - loss: 0.0363 - accuracy: 0.9875 - val_loss: 0.0061 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 7/20\n",
      "717/717 [==============================] - 29s 40ms/step - loss: 0.0244 - accuracy: 0.9924 - val_loss: 0.0028 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 8/20\n",
      "717/717 [==============================] - 29s 41ms/step - loss: 0.0221 - accuracy: 0.9927 - val_loss: 0.0013 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 9/20\n",
      "717/717 [==============================] - 30s 41ms/step - loss: 0.0187 - accuracy: 0.9938 - val_loss: 0.0051 - val_accuracy: 0.9966 - lr: 3.0000e-04\n",
      "Epoch 10/20\n",
      "717/717 [==============================] - 29s 41ms/step - loss: 0.0211 - accuracy: 0.9924 - val_loss: 0.0149 - val_accuracy: 0.9966 - lr: 3.0000e-04\n",
      "Epoch 11/20\n",
      "717/717 [==============================] - 29s 41ms/step - loss: 0.0156 - accuracy: 0.9948 - val_loss: 0.0042 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 12/20\n",
      "717/717 [==============================] - 30s 41ms/step - loss: 0.0113 - accuracy: 0.9964 - val_loss: 0.0022 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 13/20\n",
      "717/717 [==============================] - 30s 41ms/step - loss: 0.0168 - accuracy: 0.9946 - val_loss: 0.0084 - val_accuracy: 0.9966 - lr: 3.0000e-04\n",
      "Epoch 14/20\n",
      "717/717 [==============================] - 29s 41ms/step - loss: 0.0053 - accuracy: 0.9982 - val_loss: 6.0681e-04 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 15/20\n",
      "717/717 [==============================] - 29s 41ms/step - loss: 0.0181 - accuracy: 0.9956 - val_loss: 0.0037 - val_accuracy: 0.9966 - lr: 3.0000e-04\n",
      "Epoch 16/20\n",
      "717/717 [==============================] - 29s 40ms/step - loss: 0.0054 - accuracy: 0.9981 - val_loss: 0.0030 - val_accuracy: 0.9966 - lr: 3.0000e-04\n",
      "Epoch 17/20\n",
      "717/717 [==============================] - 29s 40ms/step - loss: 0.0142 - accuracy: 0.9958 - val_loss: 0.0031 - val_accuracy: 0.9966 - lr: 3.0000e-04\n",
      "Epoch 18/20\n",
      "717/717 [==============================] - 29s 40ms/step - loss: 0.0058 - accuracy: 0.9984 - val_loss: 1.5750e-05 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 19/20\n",
      "717/717 [==============================] - 30s 41ms/step - loss: 0.0058 - accuracy: 0.9981 - val_loss: 2.8103e-04 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 20/20\n",
      "717/717 [==============================] - 29s 41ms/step - loss: 0.0070 - accuracy: 0.9976 - val_loss: 0.0016 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 1/20\n",
      "719/719 [==============================] - 31s 41ms/step - loss: 0.4258 - accuracy: 0.8333 - val_loss: 0.7526 - val_accuracy: 0.7143 - lr: 3.0000e-04\n",
      "Epoch 2/20\n",
      "719/719 [==============================] - 29s 41ms/step - loss: 0.2989 - accuracy: 0.8860 - val_loss: 0.3853 - val_accuracy: 0.8722 - lr: 3.0000e-04\n",
      "Epoch 3/20\n",
      "719/719 [==============================] - 29s 40ms/step - loss: 0.1977 - accuracy: 0.9251 - val_loss: 0.0661 - val_accuracy: 0.9962 - lr: 3.0000e-04\n",
      "Epoch 4/20\n",
      "719/719 [==============================] - 29s 40ms/step - loss: 0.0676 - accuracy: 0.9781 - val_loss: 0.0890 - val_accuracy: 0.9662 - lr: 3.0000e-04\n",
      "Epoch 5/20\n",
      "719/719 [==============================] - 29s 41ms/step - loss: 0.0353 - accuracy: 0.9876 - val_loss: 0.0823 - val_accuracy: 0.9699 - lr: 3.0000e-04\n",
      "Epoch 6/20\n",
      "719/719 [==============================] - 30s 41ms/step - loss: 0.0301 - accuracy: 0.9905 - val_loss: 0.0204 - val_accuracy: 0.9812 - lr: 3.0000e-04\n",
      "Epoch 7/20\n",
      "719/719 [==============================] - 30s 41ms/step - loss: 0.0228 - accuracy: 0.9929 - val_loss: 0.0280 - val_accuracy: 0.9925 - lr: 3.0000e-04\n",
      "Epoch 8/20\n",
      "719/719 [==============================] - 29s 40ms/step - loss: 0.0250 - accuracy: 0.9923 - val_loss: 0.1547 - val_accuracy: 0.9398 - lr: 3.0000e-04\n",
      "Epoch 9/20\n",
      "719/719 [==============================] - 29s 40ms/step - loss: 0.0202 - accuracy: 0.9933 - val_loss: 0.0024 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 10/20\n",
      "719/719 [==============================] - 29s 40ms/step - loss: 0.0204 - accuracy: 0.9937 - val_loss: 0.0082 - val_accuracy: 0.9962 - lr: 3.0000e-04\n",
      "Epoch 11/20\n",
      "719/719 [==============================] - 30s 41ms/step - loss: 0.0129 - accuracy: 0.9960 - val_loss: 0.0092 - val_accuracy: 0.9962 - lr: 3.0000e-04\n",
      "Epoch 12/20\n",
      "719/719 [==============================] - 30s 41ms/step - loss: 0.0130 - accuracy: 0.9961 - val_loss: 0.0104 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 13/20\n",
      "719/719 [==============================] - 29s 41ms/step - loss: 0.0123 - accuracy: 0.9958 - val_loss: 0.0086 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 14/20\n",
      "719/719 [==============================] - 30s 41ms/step - loss: 0.0120 - accuracy: 0.9962 - val_loss: 0.0670 - val_accuracy: 0.9662 - lr: 3.0000e-04\n",
      "Epoch 15/20\n",
      "719/719 [==============================] - 30s 42ms/step - loss: 0.0158 - accuracy: 0.9952 - val_loss: 0.0032 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 16/20\n",
      "719/719 [==============================] - 30s 41ms/step - loss: 0.0066 - accuracy: 0.9976 - val_loss: 0.0665 - val_accuracy: 0.9887 - lr: 3.0000e-04\n",
      "Epoch 17/20\n",
      "719/719 [==============================] - 30s 41ms/step - loss: 0.0157 - accuracy: 0.9955 - val_loss: 0.0048 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 18/20\n",
      "719/719 [==============================] - 29s 41ms/step - loss: 0.0041 - accuracy: 0.9987 - val_loss: 0.0293 - val_accuracy: 0.9774 - lr: 3.0000e-04\n",
      "Epoch 19/20\n",
      "719/719 [==============================] - 29s 40ms/step - loss: 0.0098 - accuracy: 0.9966 - val_loss: 8.8576e-04 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 20/20\n",
      "719/719 [==============================] - 29s 40ms/step - loss: 0.0046 - accuracy: 0.9985 - val_loss: 0.0094 - val_accuracy: 0.9887 - lr: 3.0000e-04\n",
      "Epoch 1/20\n",
      "706/706 [==============================] - 30s 40ms/step - loss: 0.4272 - accuracy: 0.8291 - val_loss: 0.3066 - val_accuracy: 0.9024 - lr: 3.0000e-04\n",
      "Epoch 2/20\n",
      "706/706 [==============================] - 29s 42ms/step - loss: 0.2905 - accuracy: 0.8895 - val_loss: 0.1554 - val_accuracy: 0.9241 - lr: 3.0000e-04\n",
      "Epoch 3/20\n",
      "706/706 [==============================] - 29s 42ms/step - loss: 0.1766 - accuracy: 0.9355 - val_loss: 0.2168 - val_accuracy: 0.9111 - lr: 3.0000e-04\n",
      "Epoch 4/20\n",
      "706/706 [==============================] - 29s 41ms/step - loss: 0.1109 - accuracy: 0.9604 - val_loss: 0.0703 - val_accuracy: 0.9631 - lr: 3.0000e-04\n",
      "Epoch 5/20\n",
      "706/706 [==============================] - 29s 41ms/step - loss: 0.0749 - accuracy: 0.9752 - val_loss: 0.0281 - val_accuracy: 0.9848 - lr: 3.0000e-04\n",
      "Epoch 6/20\n",
      "706/706 [==============================] - 30s 42ms/step - loss: 0.1062 - accuracy: 0.9639 - val_loss: 0.1252 - val_accuracy: 0.9501 - lr: 3.0000e-04\n",
      "Epoch 7/20\n",
      "706/706 [==============================] - 29s 41ms/step - loss: 0.0529 - accuracy: 0.9816 - val_loss: 0.0677 - val_accuracy: 0.9696 - lr: 3.0000e-04\n",
      "Epoch 8/20\n",
      "706/706 [==============================] - 29s 42ms/step - loss: 0.0437 - accuracy: 0.9865 - val_loss: 0.0843 - val_accuracy: 0.9718 - lr: 3.0000e-04\n",
      "Epoch 9/20\n",
      "706/706 [==============================] - 29s 41ms/step - loss: 0.0364 - accuracy: 0.9886 - val_loss: 0.0236 - val_accuracy: 0.9848 - lr: 3.0000e-04\n",
      "Epoch 10/20\n",
      "706/706 [==============================] - 29s 41ms/step - loss: 0.0427 - accuracy: 0.9857 - val_loss: 0.0221 - val_accuracy: 0.9913 - lr: 3.0000e-04\n",
      "Epoch 11/20\n",
      "706/706 [==============================] - 29s 41ms/step - loss: 0.0211 - accuracy: 0.9943 - val_loss: 0.0065 - val_accuracy: 0.9978 - lr: 3.0000e-04\n",
      "Epoch 12/20\n",
      "706/706 [==============================] - 29s 41ms/step - loss: 0.0152 - accuracy: 0.9948 - val_loss: 0.0676 - val_accuracy: 0.9783 - lr: 3.0000e-04\n",
      "Epoch 13/20\n",
      "706/706 [==============================] - 29s 41ms/step - loss: 0.0219 - accuracy: 0.9926 - val_loss: 0.0175 - val_accuracy: 0.9935 - lr: 3.0000e-04\n",
      "Epoch 14/20\n",
      "706/706 [==============================] - 29s 41ms/step - loss: 0.0269 - accuracy: 0.9914 - val_loss: 0.1752 - val_accuracy: 0.9740 - lr: 3.0000e-04\n",
      "Epoch 15/20\n",
      "706/706 [==============================] - 29s 41ms/step - loss: 0.0126 - accuracy: 0.9965 - val_loss: 0.0489 - val_accuracy: 0.9892 - lr: 3.0000e-04\n",
      "Epoch 16/20\n",
      "706/706 [==============================] - 29s 41ms/step - loss: 0.0184 - accuracy: 0.9950 - val_loss: 0.0961 - val_accuracy: 0.9783 - lr: 3.0000e-04\n",
      "Epoch 17/20\n",
      "706/706 [==============================] - 29s 41ms/step - loss: 0.0084 - accuracy: 0.9978 - val_loss: 0.0667 - val_accuracy: 0.9783 - lr: 3.0000e-04\n",
      "Epoch 18/20\n",
      "706/706 [==============================] - 29s 41ms/step - loss: 0.0087 - accuracy: 0.9972 - val_loss: 0.0381 - val_accuracy: 0.9783 - lr: 3.0000e-04\n",
      "Epoch 19/20\n",
      "706/706 [==============================] - 29s 42ms/step - loss: 0.0119 - accuracy: 0.9966 - val_loss: 0.0627 - val_accuracy: 0.9805 - lr: 3.0000e-04\n",
      "Epoch 20/20\n",
      "706/706 [==============================] - 30s 42ms/step - loss: 0.0094 - accuracy: 0.9976 - val_loss: 0.0176 - val_accuracy: 0.9957 - lr: 3.0000e-04\n",
      "Epoch 1/20\n",
      "718/718 [==============================] - 31s 42ms/step - loss: 0.4185 - accuracy: 0.8316 - val_loss: 0.4544 - val_accuracy: 0.8286 - lr: 3.0000e-04\n",
      "Epoch 2/20\n",
      "718/718 [==============================] - 29s 41ms/step - loss: 0.3096 - accuracy: 0.8789 - val_loss: 0.1416 - val_accuracy: 0.9607 - lr: 3.0000e-04\n",
      "Epoch 3/20\n",
      "718/718 [==============================] - 29s 41ms/step - loss: 0.1708 - accuracy: 0.9380 - val_loss: 0.0952 - val_accuracy: 0.9679 - lr: 3.0000e-04\n",
      "Epoch 4/20\n",
      "718/718 [==============================] - 29s 40ms/step - loss: 0.1467 - accuracy: 0.9449 - val_loss: 0.1841 - val_accuracy: 0.9321 - lr: 3.0000e-04\n",
      "Epoch 5/20\n",
      "718/718 [==============================] - 29s 40ms/step - loss: 0.0962 - accuracy: 0.9651 - val_loss: 0.0790 - val_accuracy: 0.9750 - lr: 3.0000e-04\n",
      "Epoch 6/20\n",
      "718/718 [==============================] - 30s 41ms/step - loss: 0.0700 - accuracy: 0.9776 - val_loss: 0.0439 - val_accuracy: 0.9893 - lr: 3.0000e-04\n",
      "Epoch 7/20\n",
      "718/718 [==============================] - 30s 41ms/step - loss: 0.0416 - accuracy: 0.9867 - val_loss: 0.0661 - val_accuracy: 0.9893 - lr: 3.0000e-04\n",
      "Epoch 8/20\n",
      "718/718 [==============================] - 30s 42ms/step - loss: 0.0384 - accuracy: 0.9869 - val_loss: 0.0691 - val_accuracy: 0.9857 - lr: 3.0000e-04\n",
      "Epoch 9/20\n",
      "718/718 [==============================] - 30s 41ms/step - loss: 0.0309 - accuracy: 0.9914 - val_loss: 0.0542 - val_accuracy: 0.9929 - lr: 3.0000e-04\n",
      "Epoch 10/20\n",
      "718/718 [==============================] - 29s 41ms/step - loss: 0.0633 - accuracy: 0.9756 - val_loss: 2.6912 - val_accuracy: 0.6536 - lr: 3.0000e-04\n",
      "Epoch 11/20\n",
      "718/718 [==============================] - 29s 41ms/step - loss: 0.2153 - accuracy: 0.9216 - val_loss: 0.0795 - val_accuracy: 0.9821 - lr: 3.0000e-04\n",
      "Epoch 12/20\n",
      "718/718 [==============================] - 30s 42ms/step - loss: 0.1328 - accuracy: 0.9537 - val_loss: 0.1228 - val_accuracy: 0.9750 - lr: 3.0000e-04\n",
      "Epoch 13/20\n",
      "718/718 [==============================] - 30s 41ms/step - loss: 0.0941 - accuracy: 0.9662 - val_loss: 0.0456 - val_accuracy: 0.9929 - lr: 3.0000e-04\n",
      "Epoch 14/20\n",
      "718/718 [==============================] - 30s 41ms/step - loss: 0.3097 - accuracy: 0.8747 - val_loss: 0.4181 - val_accuracy: 0.7750 - lr: 3.0000e-04\n",
      "Epoch 15/20\n",
      "718/718 [==============================] - 30s 41ms/step - loss: 0.2131 - accuracy: 0.9183 - val_loss: 0.7859 - val_accuracy: 0.7929 - lr: 3.0000e-04\n",
      "Epoch 16/20\n",
      "718/718 [==============================] - 29s 41ms/step - loss: 0.1905 - accuracy: 0.9269 - val_loss: 0.0773 - val_accuracy: 0.9821 - lr: 3.0000e-04\n",
      "Epoch 17/20\n",
      "718/718 [==============================] - 30s 41ms/step - loss: 0.1144 - accuracy: 0.9569 - val_loss: 0.0551 - val_accuracy: 0.9964 - lr: 6.0000e-05\n",
      "Epoch 18/20\n",
      "718/718 [==============================] - 29s 40ms/step - loss: 0.0943 - accuracy: 0.9657 - val_loss: 0.0590 - val_accuracy: 0.9964 - lr: 6.0000e-05\n",
      "Epoch 19/20\n",
      "718/718 [==============================] - 29s 41ms/step - loss: 0.0790 - accuracy: 0.9714 - val_loss: 0.0445 - val_accuracy: 0.9964 - lr: 6.0000e-05\n",
      "Epoch 20/20\n",
      "718/718 [==============================] - 29s 41ms/step - loss: 0.0678 - accuracy: 0.9767 - val_loss: 0.0408 - val_accuracy: 0.9964 - lr: 6.0000e-05\n",
      "Epoch 1/20\n",
      "705/705 [==============================] - 30s 41ms/step - loss: 0.4316 - accuracy: 0.8243 - val_loss: 0.3920 - val_accuracy: 0.8471 - lr: 3.0000e-04\n",
      "Epoch 2/20\n",
      "705/705 [==============================] - 29s 41ms/step - loss: 0.2332 - accuracy: 0.9136 - val_loss: 0.1490 - val_accuracy: 0.9401 - lr: 3.0000e-04\n",
      "Epoch 3/20\n",
      "705/705 [==============================] - 29s 41ms/step - loss: 0.0725 - accuracy: 0.9755 - val_loss: 0.0762 - val_accuracy: 0.9669 - lr: 3.0000e-04\n",
      "Epoch 4/20\n",
      "705/705 [==============================] - 29s 41ms/step - loss: 0.0498 - accuracy: 0.9831 - val_loss: 0.1393 - val_accuracy: 0.9669 - lr: 3.0000e-04\n",
      "Epoch 5/20\n",
      "705/705 [==============================] - 29s 41ms/step - loss: 0.0384 - accuracy: 0.9877 - val_loss: 0.1020 - val_accuracy: 0.9587 - lr: 3.0000e-04\n",
      "Epoch 6/20\n",
      "705/705 [==============================] - 29s 41ms/step - loss: 0.0323 - accuracy: 0.9901 - val_loss: 0.0255 - val_accuracy: 0.9938 - lr: 3.0000e-04\n",
      "Epoch 7/20\n",
      "705/705 [==============================] - 29s 41ms/step - loss: 0.0216 - accuracy: 0.9932 - val_loss: 0.0737 - val_accuracy: 0.9855 - lr: 3.0000e-04\n",
      "Epoch 8/20\n",
      "705/705 [==============================] - 29s 42ms/step - loss: 0.0218 - accuracy: 0.9931 - val_loss: 0.0960 - val_accuracy: 0.9855 - lr: 3.0000e-04\n",
      "Epoch 9/20\n",
      "705/705 [==============================] - 29s 41ms/step - loss: 0.0169 - accuracy: 0.9950 - val_loss: 0.0609 - val_accuracy: 0.9731 - lr: 3.0000e-04\n",
      "Epoch 10/20\n",
      "705/705 [==============================] - 30s 42ms/step - loss: 0.0136 - accuracy: 0.9956 - val_loss: 0.1194 - val_accuracy: 0.9690 - lr: 3.0000e-04\n",
      "Epoch 11/20\n",
      "705/705 [==============================] - 29s 41ms/step - loss: 0.0174 - accuracy: 0.9949 - val_loss: 0.0391 - val_accuracy: 0.9938 - lr: 3.0000e-04\n",
      "Epoch 12/20\n",
      "705/705 [==============================] - 29s 40ms/step - loss: 0.0146 - accuracy: 0.9957 - val_loss: 0.1293 - val_accuracy: 0.9649 - lr: 3.0000e-04\n",
      "Epoch 13/20\n",
      "705/705 [==============================] - 29s 40ms/step - loss: 0.0110 - accuracy: 0.9963 - val_loss: 0.0407 - val_accuracy: 0.9917 - lr: 3.0000e-04\n",
      "Epoch 14/20\n",
      "705/705 [==============================] - 29s 41ms/step - loss: 0.0104 - accuracy: 0.9965 - val_loss: 0.0429 - val_accuracy: 0.9897 - lr: 3.0000e-04\n",
      "Epoch 15/20\n",
      "705/705 [==============================] - 29s 41ms/step - loss: 0.0106 - accuracy: 0.9973 - val_loss: 0.0954 - val_accuracy: 0.9917 - lr: 3.0000e-04\n",
      "Epoch 16/20\n",
      "705/705 [==============================] - 29s 41ms/step - loss: 0.0115 - accuracy: 0.9961 - val_loss: 0.0150 - val_accuracy: 0.9959 - lr: 3.0000e-04\n",
      "Epoch 17/20\n",
      "705/705 [==============================] - 29s 41ms/step - loss: 0.0071 - accuracy: 0.9975 - val_loss: 0.0325 - val_accuracy: 0.9959 - lr: 3.0000e-04\n",
      "Epoch 18/20\n",
      "705/705 [==============================] - 29s 41ms/step - loss: 0.0104 - accuracy: 0.9968 - val_loss: 0.0409 - val_accuracy: 0.9938 - lr: 3.0000e-04\n",
      "Epoch 19/20\n",
      "705/705 [==============================] - 28s 40ms/step - loss: 0.0084 - accuracy: 0.9977 - val_loss: 0.0319 - val_accuracy: 0.9938 - lr: 3.0000e-04\n",
      "Epoch 20/20\n",
      "705/705 [==============================] - 29s 41ms/step - loss: 0.0080 - accuracy: 0.9973 - val_loss: 0.0577 - val_accuracy: 0.9855 - lr: 3.0000e-04\n",
      "Epoch 1/20\n",
      "715/715 [==============================] - 31s 42ms/step - loss: 0.4344 - accuracy: 0.8253 - val_loss: 0.3100 - val_accuracy: 0.8160 - lr: 3.0000e-04\n",
      "Epoch 2/20\n",
      "715/715 [==============================] - 29s 41ms/step - loss: 0.3222 - accuracy: 0.8752 - val_loss: 0.4526 - val_accuracy: 0.8067 - lr: 3.0000e-04\n",
      "Epoch 3/20\n",
      "715/715 [==============================] - 29s 41ms/step - loss: 0.2040 - accuracy: 0.9242 - val_loss: 0.2286 - val_accuracy: 0.8957 - lr: 3.0000e-04\n",
      "Epoch 4/20\n",
      "715/715 [==============================] - 29s 41ms/step - loss: 0.1215 - accuracy: 0.9550 - val_loss: 0.0384 - val_accuracy: 0.9908 - lr: 3.0000e-04\n",
      "Epoch 5/20\n",
      "715/715 [==============================] - 28s 40ms/step - loss: 0.0764 - accuracy: 0.9733 - val_loss: 0.0121 - val_accuracy: 0.9939 - lr: 3.0000e-04\n",
      "Epoch 6/20\n",
      "715/715 [==============================] - 29s 41ms/step - loss: 0.0572 - accuracy: 0.9812 - val_loss: 0.0581 - val_accuracy: 0.9785 - lr: 3.0000e-04\n",
      "Epoch 7/20\n",
      "715/715 [==============================] - 30s 41ms/step - loss: 0.0428 - accuracy: 0.9862 - val_loss: 0.0543 - val_accuracy: 0.9785 - lr: 3.0000e-04\n",
      "Epoch 8/20\n",
      "715/715 [==============================] - 29s 41ms/step - loss: 0.0463 - accuracy: 0.9847 - val_loss: 0.0677 - val_accuracy: 0.9724 - lr: 3.0000e-04\n",
      "Epoch 9/20\n",
      "715/715 [==============================] - 29s 41ms/step - loss: 0.0341 - accuracy: 0.9894 - val_loss: 0.0040 - val_accuracy: 0.9969 - lr: 3.0000e-04\n",
      "Epoch 10/20\n",
      "715/715 [==============================] - 29s 41ms/step - loss: 0.2392 - accuracy: 0.9111 - val_loss: 0.1482 - val_accuracy: 0.9479 - lr: 3.0000e-04\n",
      "Epoch 11/20\n",
      "715/715 [==============================] - 29s 41ms/step - loss: 0.1728 - accuracy: 0.9369 - val_loss: 0.1161 - val_accuracy: 0.9479 - lr: 3.0000e-04\n",
      "Epoch 12/20\n",
      "715/715 [==============================] - 29s 41ms/step - loss: 0.0869 - accuracy: 0.9699 - val_loss: 0.2902 - val_accuracy: 0.9202 - lr: 3.0000e-04\n",
      "Epoch 13/20\n",
      "715/715 [==============================] - 29s 41ms/step - loss: 0.0574 - accuracy: 0.9810 - val_loss: 0.0798 - val_accuracy: 0.9693 - lr: 3.0000e-04\n",
      "Epoch 14/20\n",
      "715/715 [==============================] - 29s 40ms/step - loss: 0.0453 - accuracy: 0.9853 - val_loss: 0.0111 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 15/20\n",
      "715/715 [==============================] - 29s 40ms/step - loss: 0.0401 - accuracy: 0.9876 - val_loss: 0.0812 - val_accuracy: 0.9663 - lr: 3.0000e-04\n",
      "Epoch 16/20\n",
      "715/715 [==============================] - 29s 41ms/step - loss: 0.0310 - accuracy: 0.9902 - val_loss: 0.0532 - val_accuracy: 0.9816 - lr: 3.0000e-04\n",
      "Epoch 17/20\n",
      "715/715 [==============================] - 29s 41ms/step - loss: 0.0378 - accuracy: 0.9880 - val_loss: 0.0595 - val_accuracy: 0.9755 - lr: 3.0000e-04\n",
      "Epoch 18/20\n",
      "715/715 [==============================] - 29s 41ms/step - loss: 0.0285 - accuracy: 0.9906 - val_loss: 0.0108 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 19/20\n",
      "715/715 [==============================] - 29s 41ms/step - loss: 0.0259 - accuracy: 0.9921 - val_loss: 0.0284 - val_accuracy: 0.9908 - lr: 3.0000e-04\n",
      "Epoch 20/20\n",
      "715/715 [==============================] - 30s 42ms/step - loss: 0.0123 - accuracy: 0.9961 - val_loss: 0.0170 - val_accuracy: 0.9939 - lr: 6.0000e-05\n",
      "Epoch 1/20\n",
      "697/697 [==============================] - 30s 42ms/step - loss: 0.4310 - accuracy: 0.8272 - val_loss: 0.4356 - val_accuracy: 0.8412 - lr: 3.0000e-04\n",
      "Epoch 2/20\n",
      "697/697 [==============================] - 29s 41ms/step - loss: 0.3105 - accuracy: 0.8807 - val_loss: 0.2083 - val_accuracy: 0.9427 - lr: 3.0000e-04\n",
      "Epoch 3/20\n",
      "697/697 [==============================] - 29s 42ms/step - loss: 0.1954 - accuracy: 0.9302 - val_loss: 0.1008 - val_accuracy: 0.9607 - lr: 3.0000e-04\n",
      "Epoch 4/20\n",
      "697/697 [==============================] - 29s 42ms/step - loss: 0.0873 - accuracy: 0.9691 - val_loss: 0.1006 - val_accuracy: 0.9591 - lr: 3.0000e-04\n",
      "Epoch 5/20\n",
      "697/697 [==============================] - 29s 41ms/step - loss: 0.0450 - accuracy: 0.9857 - val_loss: 0.0470 - val_accuracy: 0.9836 - lr: 3.0000e-04\n",
      "Epoch 6/20\n",
      "697/697 [==============================] - 29s 41ms/step - loss: 0.0269 - accuracy: 0.9920 - val_loss: 0.0747 - val_accuracy: 0.9771 - lr: 3.0000e-04\n",
      "Epoch 7/20\n",
      "697/697 [==============================] - 29s 42ms/step - loss: 0.0266 - accuracy: 0.9908 - val_loss: 0.1000 - val_accuracy: 0.9673 - lr: 3.0000e-04\n",
      "Epoch 8/20\n",
      "697/697 [==============================] - 30s 43ms/step - loss: 0.0201 - accuracy: 0.9932 - val_loss: 0.1453 - val_accuracy: 0.9542 - lr: 3.0000e-04\n",
      "Epoch 9/20\n",
      "697/697 [==============================] - 29s 41ms/step - loss: 0.0179 - accuracy: 0.9943 - val_loss: 0.1989 - val_accuracy: 0.9394 - lr: 3.0000e-04\n",
      "Epoch 10/20\n",
      "697/697 [==============================] - 29s 41ms/step - loss: 0.0163 - accuracy: 0.9952 - val_loss: 0.0304 - val_accuracy: 0.9902 - lr: 3.0000e-04\n",
      "Epoch 11/20\n",
      "697/697 [==============================] - 29s 42ms/step - loss: 0.0101 - accuracy: 0.9968 - val_loss: 0.1146 - val_accuracy: 0.9705 - lr: 3.0000e-04\n",
      "Epoch 12/20\n",
      "697/697 [==============================] - 29s 42ms/step - loss: 0.0098 - accuracy: 0.9970 - val_loss: 0.1634 - val_accuracy: 0.9476 - lr: 3.0000e-04\n",
      "Epoch 13/20\n",
      "697/697 [==============================] - 29s 41ms/step - loss: 0.0149 - accuracy: 0.9950 - val_loss: 0.1571 - val_accuracy: 0.9607 - lr: 3.0000e-04\n",
      "Epoch 14/20\n",
      "697/697 [==============================] - 28s 41ms/step - loss: 0.0082 - accuracy: 0.9980 - val_loss: 0.0532 - val_accuracy: 0.9885 - lr: 3.0000e-04\n",
      "Epoch 15/20\n",
      "697/697 [==============================] - 29s 42ms/step - loss: 0.0111 - accuracy: 0.9969 - val_loss: 0.1015 - val_accuracy: 0.9755 - lr: 3.0000e-04\n",
      "Epoch 16/20\n",
      "697/697 [==============================] - 29s 41ms/step - loss: 0.0052 - accuracy: 0.9983 - val_loss: 0.0611 - val_accuracy: 0.9836 - lr: 3.0000e-04\n",
      "Epoch 17/20\n",
      "697/697 [==============================] - 29s 41ms/step - loss: 0.0118 - accuracy: 0.9955 - val_loss: 0.0426 - val_accuracy: 0.9869 - lr: 3.0000e-04\n",
      "Epoch 18/20\n",
      "697/697 [==============================] - 29s 41ms/step - loss: 0.0072 - accuracy: 0.9982 - val_loss: 0.0620 - val_accuracy: 0.9869 - lr: 3.0000e-04\n",
      "Epoch 19/20\n",
      "697/697 [==============================] - 29s 41ms/step - loss: 0.0030 - accuracy: 0.9991 - val_loss: 0.1066 - val_accuracy: 0.9787 - lr: 3.0000e-04\n",
      "Epoch 20/20\n",
      "697/697 [==============================] - 29s 42ms/step - loss: 0.0105 - accuracy: 0.9974 - val_loss: 0.0739 - val_accuracy: 0.9804 - lr: 3.0000e-04\n",
      "Epoch 1/20\n",
      "710/710 [==============================] - 31s 42ms/step - loss: 0.4358 - accuracy: 0.8258 - val_loss: 0.2656 - val_accuracy: 0.8963 - lr: 3.0000e-04\n",
      "Epoch 2/20\n",
      "710/710 [==============================] - 29s 41ms/step - loss: 0.2924 - accuracy: 0.8842 - val_loss: 0.2092 - val_accuracy: 0.9259 - lr: 3.0000e-04\n",
      "Epoch 3/20\n",
      "710/710 [==============================] - 29s 41ms/step - loss: 0.1640 - accuracy: 0.9399 - val_loss: 0.0759 - val_accuracy: 0.9852 - lr: 3.0000e-04\n",
      "Epoch 4/20\n",
      "710/710 [==============================] - 29s 41ms/step - loss: 0.2849 - accuracy: 0.8838 - val_loss: 0.1057 - val_accuracy: 0.9679 - lr: 3.0000e-04\n",
      "Epoch 5/20\n",
      "710/710 [==============================] - 30s 42ms/step - loss: 0.1409 - accuracy: 0.9486 - val_loss: 0.0905 - val_accuracy: 0.9753 - lr: 3.0000e-04\n",
      "Epoch 6/20\n",
      "710/710 [==============================] - 28s 40ms/step - loss: 0.0828 - accuracy: 0.9703 - val_loss: 0.0879 - val_accuracy: 0.9580 - lr: 3.0000e-04\n",
      "Epoch 7/20\n",
      "710/710 [==============================] - 28s 40ms/step - loss: 0.0498 - accuracy: 0.9827 - val_loss: 0.0326 - val_accuracy: 0.9802 - lr: 3.0000e-04\n",
      "Epoch 8/20\n",
      "710/710 [==============================] - 29s 41ms/step - loss: 0.0467 - accuracy: 0.9852 - val_loss: 0.0981 - val_accuracy: 0.9679 - lr: 3.0000e-04\n",
      "Epoch 9/20\n",
      "710/710 [==============================] - 29s 41ms/step - loss: 0.0341 - accuracy: 0.9893 - val_loss: 0.1166 - val_accuracy: 0.9654 - lr: 3.0000e-04\n",
      "Epoch 10/20\n",
      "710/710 [==============================] - 29s 41ms/step - loss: 0.0308 - accuracy: 0.9902 - val_loss: 0.0340 - val_accuracy: 0.9901 - lr: 3.0000e-04\n",
      "Epoch 11/20\n",
      "710/710 [==============================] - 29s 41ms/step - loss: 0.0285 - accuracy: 0.9907 - val_loss: 0.0370 - val_accuracy: 0.9852 - lr: 3.0000e-04\n",
      "Epoch 12/20\n",
      "710/710 [==============================] - 29s 41ms/step - loss: 0.0262 - accuracy: 0.9915 - val_loss: 0.0392 - val_accuracy: 0.9901 - lr: 3.0000e-04\n",
      "Epoch 13/20\n",
      "710/710 [==============================] - 30s 43ms/step - loss: 0.0201 - accuracy: 0.9935 - val_loss: 0.1587 - val_accuracy: 0.9654 - lr: 3.0000e-04\n",
      "Epoch 14/20\n",
      "710/710 [==============================] - 29s 41ms/step - loss: 0.1585 - accuracy: 0.9396 - val_loss: 1.3672 - val_accuracy: 0.6000 - lr: 3.0000e-04\n",
      "Epoch 15/20\n",
      "710/710 [==============================] - 29s 41ms/step - loss: 0.2619 - accuracy: 0.8966 - val_loss: 0.0998 - val_accuracy: 0.9580 - lr: 3.0000e-04\n",
      "Epoch 16/20\n",
      "710/710 [==============================] - 29s 40ms/step - loss: 0.1311 - accuracy: 0.9524 - val_loss: 0.0584 - val_accuracy: 0.9679 - lr: 3.0000e-04\n",
      "Epoch 17/20\n",
      "710/710 [==============================] - 29s 41ms/step - loss: 0.0920 - accuracy: 0.9670 - val_loss: 0.0328 - val_accuracy: 0.9877 - lr: 3.0000e-04\n",
      "Epoch 18/20\n",
      "710/710 [==============================] - 29s 41ms/step - loss: 0.0583 - accuracy: 0.9796 - val_loss: 0.0192 - val_accuracy: 0.9926 - lr: 6.0000e-05\n",
      "Epoch 19/20\n",
      "710/710 [==============================] - 30s 42ms/step - loss: 0.0515 - accuracy: 0.9823 - val_loss: 0.0137 - val_accuracy: 0.9951 - lr: 6.0000e-05\n",
      "Epoch 20/20\n",
      "710/710 [==============================] - 29s 41ms/step - loss: 0.0453 - accuracy: 0.9856 - val_loss: 0.0230 - val_accuracy: 0.9877 - lr: 6.0000e-05\n",
      "Epoch 1/20\n",
      "714/714 [==============================] - 31s 41ms/step - loss: 0.4290 - accuracy: 0.8262 - val_loss: 0.3993 - val_accuracy: 0.8929 - lr: 3.0000e-04\n",
      "Epoch 2/20\n",
      "714/714 [==============================] - 29s 40ms/step - loss: 0.3040 - accuracy: 0.8836 - val_loss: 0.2834 - val_accuracy: 0.9137 - lr: 3.0000e-04\n",
      "Epoch 3/20\n",
      "714/714 [==============================] - 29s 41ms/step - loss: 0.1643 - accuracy: 0.9394 - val_loss: 0.1751 - val_accuracy: 0.9345 - lr: 3.0000e-04\n",
      "Epoch 4/20\n",
      "714/714 [==============================] - 29s 41ms/step - loss: 0.1131 - accuracy: 0.9609 - val_loss: 0.2281 - val_accuracy: 0.9077 - lr: 3.0000e-04\n",
      "Epoch 5/20\n",
      "714/714 [==============================] - 29s 41ms/step - loss: 0.2047 - accuracy: 0.9228 - val_loss: 0.3979 - val_accuracy: 0.8423 - lr: 3.0000e-04\n",
      "Epoch 6/20\n",
      "714/714 [==============================] - 29s 41ms/step - loss: 0.3295 - accuracy: 0.8655 - val_loss: 0.3667 - val_accuracy: 0.8601 - lr: 3.0000e-04\n",
      "Epoch 7/20\n",
      "714/714 [==============================] - 30s 41ms/step - loss: 0.2205 - accuracy: 0.9138 - val_loss: 0.2602 - val_accuracy: 0.9077 - lr: 3.0000e-04\n",
      "Epoch 8/20\n",
      "714/714 [==============================] - 29s 41ms/step - loss: 0.1262 - accuracy: 0.9536 - val_loss: 0.2093 - val_accuracy: 0.9345 - lr: 3.0000e-04\n",
      "Epoch 9/20\n",
      "714/714 [==============================] - 32s 45ms/step - loss: 0.0882 - accuracy: 0.9695 - val_loss: 0.1033 - val_accuracy: 0.9583 - lr: 3.0000e-04\n",
      "Epoch 10/20\n",
      "714/714 [==============================] - 29s 40ms/step - loss: 0.0777 - accuracy: 0.9723 - val_loss: 0.3152 - val_accuracy: 0.9226 - lr: 3.0000e-04\n",
      "Epoch 11/20\n",
      "714/714 [==============================] - 29s 41ms/step - loss: 0.0620 - accuracy: 0.9769 - val_loss: 0.0925 - val_accuracy: 0.9643 - lr: 3.0000e-04\n",
      "Epoch 12/20\n",
      "714/714 [==============================] - 29s 41ms/step - loss: 0.0543 - accuracy: 0.9829 - val_loss: 0.1375 - val_accuracy: 0.9732 - lr: 3.0000e-04\n",
      "Epoch 13/20\n",
      "714/714 [==============================] - 29s 41ms/step - loss: 0.0497 - accuracy: 0.9830 - val_loss: 0.1032 - val_accuracy: 0.9643 - lr: 3.0000e-04\n",
      "Epoch 14/20\n",
      "714/714 [==============================] - 28s 40ms/step - loss: 0.0403 - accuracy: 0.9851 - val_loss: 0.1211 - val_accuracy: 0.9613 - lr: 3.0000e-04\n",
      "Epoch 15/20\n",
      "714/714 [==============================] - 29s 41ms/step - loss: 0.0385 - accuracy: 0.9871 - val_loss: 0.2918 - val_accuracy: 0.9256 - lr: 3.0000e-04\n",
      "Epoch 16/20\n",
      "714/714 [==============================] - 29s 40ms/step - loss: 0.0323 - accuracy: 0.9883 - val_loss: 0.0718 - val_accuracy: 0.9732 - lr: 3.0000e-04\n",
      "Epoch 17/20\n",
      "714/714 [==============================] - 29s 41ms/step - loss: 0.0315 - accuracy: 0.9896 - val_loss: 0.1013 - val_accuracy: 0.9762 - lr: 3.0000e-04\n",
      "Epoch 18/20\n",
      "714/714 [==============================] - 29s 41ms/step - loss: 0.0331 - accuracy: 0.9891 - val_loss: 0.1259 - val_accuracy: 0.9673 - lr: 3.0000e-04\n",
      "Epoch 19/20\n",
      "714/714 [==============================] - 29s 41ms/step - loss: 0.0256 - accuracy: 0.9914 - val_loss: 0.1540 - val_accuracy: 0.9554 - lr: 3.0000e-04\n",
      "Epoch 20/20\n",
      "714/714 [==============================] - 30s 42ms/step - loss: 0.0197 - accuracy: 0.9932 - val_loss: 0.0411 - val_accuracy: 0.9792 - lr: 3.0000e-04\n",
      "Epoch 1/20\n",
      "713/713 [==============================] - 31s 41ms/step - loss: 0.4154 - accuracy: 0.8346 - val_loss: 0.1597 - val_accuracy: 0.9438 - lr: 3.0000e-04\n",
      "Epoch 2/20\n",
      "713/713 [==============================] - 29s 41ms/step - loss: 0.2943 - accuracy: 0.8865 - val_loss: 0.0568 - val_accuracy: 0.9860 - lr: 3.0000e-04\n",
      "Epoch 3/20\n",
      "713/713 [==============================] - 29s 41ms/step - loss: 0.2057 - accuracy: 0.9218 - val_loss: 0.5824 - val_accuracy: 0.8230 - lr: 3.0000e-04\n",
      "Epoch 4/20\n",
      "713/713 [==============================] - 29s 40ms/step - loss: 0.1478 - accuracy: 0.9463 - val_loss: 0.0236 - val_accuracy: 0.9944 - lr: 3.0000e-04\n",
      "Epoch 5/20\n",
      "713/713 [==============================] - 29s 41ms/step - loss: 0.0541 - accuracy: 0.9831 - val_loss: 0.0041 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 6/20\n",
      "713/713 [==============================] - 30s 42ms/step - loss: 0.0402 - accuracy: 0.9858 - val_loss: 0.0083 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 7/20\n",
      "713/713 [==============================] - 29s 41ms/step - loss: 0.0359 - accuracy: 0.9885 - val_loss: 0.0037 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 8/20\n",
      "713/713 [==============================] - 30s 41ms/step - loss: 0.0224 - accuracy: 0.9929 - val_loss: 0.0019 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 9/20\n",
      "713/713 [==============================] - 30s 42ms/step - loss: 0.0212 - accuracy: 0.9939 - val_loss: 0.0013 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 10/20\n",
      "713/713 [==============================] - 29s 41ms/step - loss: 0.0217 - accuracy: 0.9930 - val_loss: 0.0019 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 11/20\n",
      "713/713 [==============================] - 29s 40ms/step - loss: 0.0197 - accuracy: 0.9943 - val_loss: 7.2086e-04 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 12/20\n",
      "713/713 [==============================] - 29s 41ms/step - loss: 0.0165 - accuracy: 0.9943 - val_loss: 0.0244 - val_accuracy: 0.9944 - lr: 3.0000e-04\n",
      "Epoch 13/20\n",
      "713/713 [==============================] - 30s 42ms/step - loss: 0.0100 - accuracy: 0.9969 - val_loss: 0.0019 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 14/20\n",
      "713/713 [==============================] - 30s 42ms/step - loss: 0.0105 - accuracy: 0.9968 - val_loss: 0.0109 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 15/20\n",
      "713/713 [==============================] - 29s 41ms/step - loss: 0.0131 - accuracy: 0.9961 - val_loss: 7.6321e-04 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 16/20\n",
      "713/713 [==============================] - 30s 42ms/step - loss: 0.0081 - accuracy: 0.9974 - val_loss: 0.0015 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 17/20\n",
      "713/713 [==============================] - 29s 41ms/step - loss: 0.0108 - accuracy: 0.9972 - val_loss: 0.0030 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 18/20\n",
      "713/713 [==============================] - 30s 42ms/step - loss: 0.0086 - accuracy: 0.9972 - val_loss: 0.0190 - val_accuracy: 0.9972 - lr: 3.0000e-04\n",
      "Epoch 19/20\n",
      "713/713 [==============================] - 30s 42ms/step - loss: 0.0087 - accuracy: 0.9973 - val_loss: 0.0029 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 20/20\n",
      "713/713 [==============================] - 29s 41ms/step - loss: 0.0094 - accuracy: 0.9968 - val_loss: 7.3212e-04 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 1/20\n",
      "715/715 [==============================] - 31s 41ms/step - loss: 0.4234 - accuracy: 0.8285 - val_loss: 0.5171 - val_accuracy: 0.8576 - lr: 3.0000e-04\n",
      "Epoch 2/20\n",
      "715/715 [==============================] - 29s 41ms/step - loss: 0.2994 - accuracy: 0.8833 - val_loss: 0.1182 - val_accuracy: 0.9525 - lr: 3.0000e-04\n",
      "Epoch 3/20\n",
      "715/715 [==============================] - 30s 42ms/step - loss: 0.1924 - accuracy: 0.9278 - val_loss: 0.5141 - val_accuracy: 0.8703 - lr: 3.0000e-04\n",
      "Epoch 4/20\n",
      "715/715 [==============================] - 29s 41ms/step - loss: 0.2246 - accuracy: 0.9142 - val_loss: 0.1466 - val_accuracy: 0.9684 - lr: 3.0000e-04\n",
      "Epoch 5/20\n",
      "715/715 [==============================] - 29s 41ms/step - loss: 0.1361 - accuracy: 0.9489 - val_loss: 0.1121 - val_accuracy: 0.9715 - lr: 3.0000e-04\n",
      "Epoch 6/20\n",
      "715/715 [==============================] - 29s 40ms/step - loss: 0.1414 - accuracy: 0.9484 - val_loss: 0.0636 - val_accuracy: 0.9873 - lr: 3.0000e-04\n",
      "Epoch 7/20\n",
      "715/715 [==============================] - 30s 41ms/step - loss: 0.0768 - accuracy: 0.9731 - val_loss: 0.0072 - val_accuracy: 0.9968 - lr: 3.0000e-04\n",
      "Epoch 8/20\n",
      "715/715 [==============================] - 30s 42ms/step - loss: 0.0529 - accuracy: 0.9818 - val_loss: 0.0156 - val_accuracy: 0.9968 - lr: 3.0000e-04\n",
      "Epoch 9/20\n",
      "715/715 [==============================] - 29s 41ms/step - loss: 0.0371 - accuracy: 0.9882 - val_loss: 0.0836 - val_accuracy: 0.9873 - lr: 3.0000e-04\n",
      "Epoch 10/20\n",
      "715/715 [==============================] - 30s 41ms/step - loss: 0.0317 - accuracy: 0.9902 - val_loss: 0.0074 - val_accuracy: 0.9968 - lr: 3.0000e-04\n",
      "Epoch 11/20\n",
      "715/715 [==============================] - 30s 42ms/step - loss: 0.0281 - accuracy: 0.9915 - val_loss: 0.0038 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 12/20\n",
      "715/715 [==============================] - 29s 41ms/step - loss: 0.0226 - accuracy: 0.9924 - val_loss: 0.0122 - val_accuracy: 0.9968 - lr: 3.0000e-04\n",
      "Epoch 13/20\n",
      "715/715 [==============================] - 29s 41ms/step - loss: 0.0175 - accuracy: 0.9946 - val_loss: 9.4774e-04 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 14/20\n",
      "715/715 [==============================] - 31s 43ms/step - loss: 0.0228 - accuracy: 0.9935 - val_loss: 0.5578 - val_accuracy: 0.8987 - lr: 3.0000e-04\n",
      "Epoch 15/20\n",
      "715/715 [==============================] - 29s 41ms/step - loss: 0.0215 - accuracy: 0.9931 - val_loss: 0.0016 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 16/20\n",
      "715/715 [==============================] - 30s 41ms/step - loss: 0.0172 - accuracy: 0.9944 - val_loss: 0.0134 - val_accuracy: 0.9968 - lr: 3.0000e-04\n",
      "Epoch 17/20\n",
      "715/715 [==============================] - 30s 42ms/step - loss: 0.0141 - accuracy: 0.9958 - val_loss: 0.0023 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 18/20\n",
      "715/715 [==============================] - 29s 41ms/step - loss: 0.0148 - accuracy: 0.9958 - val_loss: 0.0094 - val_accuracy: 0.9937 - lr: 3.0000e-04\n",
      "Epoch 19/20\n",
      "715/715 [==============================] - 29s 41ms/step - loss: 0.0126 - accuracy: 0.9958 - val_loss: 0.0476 - val_accuracy: 0.9842 - lr: 3.0000e-04\n",
      "Epoch 20/20\n",
      "715/715 [==============================] - 30s 42ms/step - loss: 0.0134 - accuracy: 0.9958 - val_loss: 4.8832e-04 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 1/20\n",
      "703/703 [==============================] - 31s 42ms/step - loss: 0.4469 - accuracy: 0.8169 - val_loss: 0.4303 - val_accuracy: 0.8291 - lr: 3.0000e-04\n",
      "Epoch 2/20\n",
      "703/703 [==============================] - 29s 42ms/step - loss: 0.3073 - accuracy: 0.8825 - val_loss: 0.1133 - val_accuracy: 0.9476 - lr: 3.0000e-04\n",
      "Epoch 3/20\n",
      "703/703 [==============================] - 29s 41ms/step - loss: 0.1753 - accuracy: 0.9352 - val_loss: 0.0793 - val_accuracy: 0.9709 - lr: 3.0000e-04\n",
      "Epoch 4/20\n",
      "703/703 [==============================] - 29s 41ms/step - loss: 0.1188 - accuracy: 0.9559 - val_loss: 0.0508 - val_accuracy: 0.9883 - lr: 3.0000e-04\n",
      "Epoch 5/20\n",
      "703/703 [==============================] - 29s 42ms/step - loss: 0.0810 - accuracy: 0.9714 - val_loss: 0.0231 - val_accuracy: 0.9922 - lr: 3.0000e-04\n",
      "Epoch 6/20\n",
      "703/703 [==============================] - 29s 41ms/step - loss: 0.0575 - accuracy: 0.9819 - val_loss: 0.0608 - val_accuracy: 0.9845 - lr: 3.0000e-04\n",
      "Epoch 7/20\n",
      "703/703 [==============================] - 29s 42ms/step - loss: 0.0534 - accuracy: 0.9812 - val_loss: 0.0272 - val_accuracy: 0.9922 - lr: 3.0000e-04\n",
      "Epoch 8/20\n",
      "703/703 [==============================] - 30s 42ms/step - loss: 0.0356 - accuracy: 0.9887 - val_loss: 0.0174 - val_accuracy: 0.9922 - lr: 3.0000e-04\n",
      "Epoch 9/20\n",
      "703/703 [==============================] - 30s 42ms/step - loss: 0.0335 - accuracy: 0.9885 - val_loss: 0.0163 - val_accuracy: 0.9981 - lr: 3.0000e-04\n",
      "Epoch 10/20\n",
      "703/703 [==============================] - 30s 42ms/step - loss: 0.0227 - accuracy: 0.9931 - val_loss: 0.0159 - val_accuracy: 0.9942 - lr: 3.0000e-04\n",
      "Epoch 11/20\n",
      "703/703 [==============================] - 29s 42ms/step - loss: 0.0264 - accuracy: 0.9923 - val_loss: 0.0702 - val_accuracy: 0.9825 - lr: 3.0000e-04\n",
      "Epoch 12/20\n",
      "703/703 [==============================] - 29s 41ms/step - loss: 0.0224 - accuracy: 0.9923 - val_loss: 0.0500 - val_accuracy: 0.9883 - lr: 3.0000e-04\n",
      "Epoch 13/20\n",
      "703/703 [==============================] - 29s 41ms/step - loss: 0.0528 - accuracy: 0.9824 - val_loss: 0.0088 - val_accuracy: 0.9981 - lr: 3.0000e-04\n",
      "Epoch 14/20\n",
      "703/703 [==============================] - 30s 42ms/step - loss: 0.0143 - accuracy: 0.9961 - val_loss: 0.0214 - val_accuracy: 0.9961 - lr: 3.0000e-04\n",
      "Epoch 15/20\n",
      "703/703 [==============================] - 29s 41ms/step - loss: 0.0149 - accuracy: 0.9953 - val_loss: 0.0146 - val_accuracy: 0.9981 - lr: 3.0000e-04\n",
      "Epoch 16/20\n",
      "703/703 [==============================] - 29s 41ms/step - loss: 0.0143 - accuracy: 0.9961 - val_loss: 0.0048 - val_accuracy: 0.9961 - lr: 3.0000e-04\n",
      "Epoch 17/20\n",
      "703/703 [==============================] - 29s 41ms/step - loss: 0.0090 - accuracy: 0.9971 - val_loss: 0.0019 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 18/20\n",
      "703/703 [==============================] - 30s 42ms/step - loss: 0.0094 - accuracy: 0.9970 - val_loss: 0.0086 - val_accuracy: 0.9942 - lr: 3.0000e-04\n",
      "Epoch 19/20\n",
      "703/703 [==============================] - 29s 42ms/step - loss: 0.0139 - accuracy: 0.9955 - val_loss: 0.0154 - val_accuracy: 0.9981 - lr: 3.0000e-04\n",
      "Epoch 20/20\n",
      "703/703 [==============================] - 29s 41ms/step - loss: 0.0163 - accuracy: 0.9950 - val_loss: 0.0174 - val_accuracy: 0.9961 - lr: 3.0000e-04\n",
      "Epoch 1/20\n",
      "710/710 [==============================] - 31s 42ms/step - loss: 0.4226 - accuracy: 0.8341 - val_loss: 0.3289 - val_accuracy: 0.8505 - lr: 3.0000e-04\n",
      "Epoch 2/20\n",
      "710/710 [==============================] - 29s 41ms/step - loss: 0.2808 - accuracy: 0.8947 - val_loss: 0.1439 - val_accuracy: 0.9289 - lr: 3.0000e-04\n",
      "Epoch 3/20\n",
      "710/710 [==============================] - 29s 41ms/step - loss: 0.1636 - accuracy: 0.9424 - val_loss: 0.3538 - val_accuracy: 0.9093 - lr: 3.0000e-04\n",
      "Epoch 4/20\n",
      "710/710 [==============================] - 29s 41ms/step - loss: 0.1038 - accuracy: 0.9648 - val_loss: 0.1112 - val_accuracy: 0.9412 - lr: 3.0000e-04\n",
      "Epoch 5/20\n",
      "710/710 [==============================] - 29s 41ms/step - loss: 0.0847 - accuracy: 0.9723 - val_loss: 0.0825 - val_accuracy: 0.9706 - lr: 3.0000e-04\n",
      "Epoch 6/20\n",
      "710/710 [==============================] - 29s 41ms/step - loss: 0.0745 - accuracy: 0.9735 - val_loss: 0.1444 - val_accuracy: 0.9289 - lr: 3.0000e-04\n",
      "Epoch 7/20\n",
      "710/710 [==============================] - 30s 42ms/step - loss: 0.0525 - accuracy: 0.9833 - val_loss: 0.0542 - val_accuracy: 0.9804 - lr: 3.0000e-04\n",
      "Epoch 8/20\n",
      "710/710 [==============================] - 30s 42ms/step - loss: 0.0429 - accuracy: 0.9861 - val_loss: 0.0383 - val_accuracy: 0.9804 - lr: 3.0000e-04\n",
      "Epoch 9/20\n",
      "710/710 [==============================] - 29s 41ms/step - loss: 0.0351 - accuracy: 0.9894 - val_loss: 0.0215 - val_accuracy: 0.9975 - lr: 3.0000e-04\n",
      "Epoch 10/20\n",
      "710/710 [==============================] - 29s 41ms/step - loss: 0.0309 - accuracy: 0.9896 - val_loss: 0.0436 - val_accuracy: 0.9853 - lr: 3.0000e-04\n",
      "Epoch 11/20\n",
      "710/710 [==============================] - 29s 41ms/step - loss: 0.0240 - accuracy: 0.9924 - val_loss: 0.0117 - val_accuracy: 0.9975 - lr: 3.0000e-04\n",
      "Epoch 12/20\n",
      "710/710 [==============================] - 30s 42ms/step - loss: 0.0248 - accuracy: 0.9929 - val_loss: 0.0043 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 13/20\n",
      "710/710 [==============================] - 30s 42ms/step - loss: 0.0169 - accuracy: 0.9949 - val_loss: 0.0373 - val_accuracy: 0.9804 - lr: 3.0000e-04\n",
      "Epoch 14/20\n",
      "710/710 [==============================] - 29s 41ms/step - loss: 0.0256 - accuracy: 0.9918 - val_loss: 0.1031 - val_accuracy: 0.9681 - lr: 3.0000e-04\n",
      "Epoch 15/20\n",
      "710/710 [==============================] - 30s 42ms/step - loss: 0.0206 - accuracy: 0.9937 - val_loss: 3.6787e-04 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 16/20\n",
      "710/710 [==============================] - 30s 42ms/step - loss: 0.0092 - accuracy: 0.9977 - val_loss: 0.0030 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 17/20\n",
      "710/710 [==============================] - 30s 42ms/step - loss: 0.0088 - accuracy: 0.9977 - val_loss: 0.1242 - val_accuracy: 0.9779 - lr: 3.0000e-04\n",
      "Epoch 18/20\n",
      "710/710 [==============================] - 29s 42ms/step - loss: 0.0269 - accuracy: 0.9917 - val_loss: 0.0036 - val_accuracy: 1.0000 - lr: 3.0000e-04\n",
      "Epoch 19/20\n",
      "710/710 [==============================] - 30s 42ms/step - loss: 0.0093 - accuracy: 0.9974 - val_loss: 0.0182 - val_accuracy: 0.9951 - lr: 3.0000e-04\n",
      "Epoch 20/20\n",
      "710/710 [==============================] - 29s 41ms/step - loss: 0.0126 - accuracy: 0.9959 - val_loss: 0.0061 - val_accuracy: 0.9975 - lr: 3.0000e-04\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "batch_size = 16\n",
    "\n",
    "val_acc_per_fold = []\n",
    "acc_per_fold = []\n",
    "val_loss_per_fold = []\n",
    "loss_per_fold = []\n",
    "\n",
    "for n in range(1, 31):\n",
    "    test_x = pd.DataFrame()\n",
    "    test_y = pd.DataFrame()\n",
    "\n",
    "    idxs_test = df_fall[df_fall['subject'] == n].index.to_list()\n",
    "    idxs_test_2 = df_adl[df_adl['subject'] == n].index.to_list()\n",
    "    test_x = pd.concat([test_x, fall_data.loc[idxs_test], adl_data.loc[idxs_test_2]])\n",
    "    test_y = pd.concat([test_y, df_fall.loc[idxs_test]['label'], df_adl.loc[idxs_test_2]['label']])\n",
    "    #print(test_x.shape, test_y.shape)\n",
    "\n",
    "    train_x = pd.DataFrame()\n",
    "    train_y = pd.DataFrame()\n",
    "\n",
    "    fall_data_c = fall_data.copy()\n",
    "    adl_data_c = adl_data.copy()\n",
    "    fall_data_c.drop(idxs_test, axis=0, inplace=True)\n",
    "    adl_data_c.drop(idxs_test_2, axis=0, inplace=True)\n",
    "    train_x = pd.concat([train_x, fall_data_c, adl_data_c])\n",
    "\n",
    "    df_fall_c = df_fall.copy()\n",
    "    df_adl_c = df_adl.copy()\n",
    "    df_fall_c.drop(idxs_test, axis=0, inplace=True)\n",
    "    df_adl_c.drop(idxs_test_2, axis=0, inplace=True)\n",
    "    train_y = pd.concat([train_y, df_fall_c['label'], df_adl_c['label']])\n",
    "    #print(train_x.shape, train_y.shape)\n",
    "\n",
    "    classifier_RNN_leave = build_model_RNN()\n",
    "\n",
    "    #print(np.array(train_y.astype(int)))\n",
    "    \n",
    "    history_RNN_leave = classifier_RNN_leave.fit(\n",
    "        np.array(train_x), np.array(train_y),\n",
    "        epochs=epochs,\n",
    "        batch_size = batch_size,\n",
    "        steps_per_epoch=(train_x.shape[0]) // batch_size,\n",
    "        validation_data=(np.array(test_x), np.array(test_y)),\n",
    "        callbacks=[tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=10, min_lr=0.00003)]\n",
    "    )\n",
    "\n",
    "    val_acc_per_fold.append(history_RNN_leave.history['val_accuracy'])\n",
    "    acc_per_fold.append(history_RNN_leave.history['accuracy'])\n",
    "    val_loss_per_fold.append(history_RNN_leave.history['val_loss'])\n",
    "    loss_per_fold.append(history_RNN_leave.history['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[0.8203125,\n",
       "   0.9505208134651184,\n",
       "   0.9583333134651184,\n",
       "   0.9505208134651184,\n",
       "   0.9973958134651184,\n",
       "   0.9791666865348816,\n",
       "   0.9817708134651184,\n",
       "   0.9791666865348816,\n",
       "   0.8046875,\n",
       "   0.8932291865348816,\n",
       "   0.9244791865348816,\n",
       "   0.9427083134651184,\n",
       "   0.9453125,\n",
       "   0.9765625,\n",
       "   0.9791666865348816,\n",
       "   0.9895833134651184,\n",
       "   0.984375,\n",
       "   0.9921875,\n",
       "   0.9895833134651184,\n",
       "   0.9895833134651184],\n",
       "  [0.8164665699005127,\n",
       "   0.8713550567626953,\n",
       "   0.9416809678077698,\n",
       "   0.7873070240020752,\n",
       "   0.9811320900917053,\n",
       "   0.9674099683761597,\n",
       "   0.9502573013305664,\n",
       "   0.9879931211471558,\n",
       "   0.9777015447616577,\n",
       "   0.9691252112388611,\n",
       "   0.5591766834259033,\n",
       "   0.7958833575248718,\n",
       "   0.958833634853363,\n",
       "   0.9828473329544067,\n",
       "   0.9691252112388611,\n",
       "   0.9656946659088135,\n",
       "   0.9777015447616577,\n",
       "   0.9965694546699524,\n",
       "   0.9691252112388611,\n",
       "   0.994854211807251],\n",
       "  [0.7664473652839661,\n",
       "   0.8585526347160339,\n",
       "   0.9769737124443054,\n",
       "   0.9835526347160339,\n",
       "   0.9901315569877625,\n",
       "   0.9769737124443054,\n",
       "   0.9440789222717285,\n",
       "   0.9736841917037964,\n",
       "   0.9243420958518982,\n",
       "   0.9539473652839661,\n",
       "   0.9572368264198303,\n",
       "   0.9671052694320679,\n",
       "   0.9835526347160339,\n",
       "   0.9703947305679321,\n",
       "   0.9473684430122375,\n",
       "   0.9703947305679321,\n",
       "   0.9835526347160339,\n",
       "   0.9671052694320679,\n",
       "   0.9769737124443054,\n",
       "   0.9769737124443054],\n",
       "  [0.8456591367721558,\n",
       "   0.9003215432167053,\n",
       "   0.9517685174942017,\n",
       "   0.7813504934310913,\n",
       "   0.938906729221344,\n",
       "   0.9517685174942017,\n",
       "   0.9356912970542908,\n",
       "   0.9485530257225037,\n",
       "   0.9003215432167053,\n",
       "   0.971061110496521,\n",
       "   0.9646302461624146,\n",
       "   0.9453375935554504,\n",
       "   0.9292604327201843,\n",
       "   0.9163987040519714,\n",
       "   0.9131832718849182,\n",
       "   0.9228295683860779,\n",
       "   0.9614148139953613,\n",
       "   0.9774919748306274,\n",
       "   0.9935691356658936,\n",
       "   0.9421221613883972],\n",
       "  [0.8557047247886658,\n",
       "   0.7986577153205872,\n",
       "   0.9563758373260498,\n",
       "   0.9530201554298401,\n",
       "   0.9697986841201782,\n",
       "   0.9765100479125977,\n",
       "   0.9832214713096619,\n",
       "   0.9899328947067261,\n",
       "   0.9865771532058716,\n",
       "   0.9899328947067261,\n",
       "   0.9966443181037903,\n",
       "   0.9932885766029358,\n",
       "   0.9899328947067261,\n",
       "   0.9899328947067261,\n",
       "   0.9832214713096619,\n",
       "   0.9899328947067261,\n",
       "   0.9865771532058716,\n",
       "   0.9899328947067261,\n",
       "   1.0,\n",
       "   0.9966443181037903],\n",
       "  [0.9150485396385193,\n",
       "   0.9101941585540771,\n",
       "   0.9854369163513184,\n",
       "   0.9975728392601013,\n",
       "   1.0,\n",
       "   1.0,\n",
       "   0.9830096960067749,\n",
       "   1.0,\n",
       "   1.0,\n",
       "   0.9975728392601013,\n",
       "   0.9951456189155579,\n",
       "   1.0,\n",
       "   0.9927184581756592,\n",
       "   1.0,\n",
       "   0.9975728392601013,\n",
       "   0.9927184581756592,\n",
       "   1.0,\n",
       "   1.0,\n",
       "   0.9975728392601013,\n",
       "   1.0],\n",
       "  [0.9121212363243103,\n",
       "   0.9757575988769531,\n",
       "   0.8939393758773804,\n",
       "   0.9848484992980957,\n",
       "   0.9909090995788574,\n",
       "   0.9909090995788574,\n",
       "   0.9636363387107849,\n",
       "   0.9727272987365723,\n",
       "   0.9939393997192383,\n",
       "   0.9939393997192383,\n",
       "   0.9939393997192383,\n",
       "   0.9909090995788574,\n",
       "   1.0,\n",
       "   0.9939393997192383,\n",
       "   0.9696969985961914,\n",
       "   0.978787899017334,\n",
       "   1.0,\n",
       "   1.0,\n",
       "   1.0,\n",
       "   0.9909090995788574],\n",
       "  [0.8790931701660156,\n",
       "   0.9294710159301758,\n",
       "   0.9672544002532959,\n",
       "   0.9773299694061279,\n",
       "   0.9521410465240479,\n",
       "   0.9773299694061279,\n",
       "   0.9697732925415039,\n",
       "   0.9798488616943359,\n",
       "   0.9647355079650879,\n",
       "   0.9722921848297119,\n",
       "   0.98740553855896,\n",
       "   0.9697732925415039,\n",
       "   0.9697732925415039,\n",
       "   0.982367753982544,\n",
       "   0.9773299694061279,\n",
       "   0.9722921848297119,\n",
       "   0.9798488616943359,\n",
       "   0.9748110771179199,\n",
       "   0.98740553855896,\n",
       "   0.9722921848297119],\n",
       "  [0.7560386657714844,\n",
       "   0.9178743958473206,\n",
       "   0.9323671460151672,\n",
       "   0.9468598961830139,\n",
       "   0.9541062712669373,\n",
       "   0.7077294588088989,\n",
       "   0.8888888955116272,\n",
       "   0.9323671460151672,\n",
       "   0.9830917716026306,\n",
       "   0.9758453965187073,\n",
       "   0.9855072498321533,\n",
       "   0.987922728061676,\n",
       "   0.987922728061676,\n",
       "   0.9830917716026306,\n",
       "   0.9758453965187073,\n",
       "   0.9855072498321533,\n",
       "   0.9758453965187073,\n",
       "   0.990338146686554,\n",
       "   0.9830917716026306,\n",
       "   0.990338146686554],\n",
       "  [0.9648562073707581,\n",
       "   0.9872204661369324,\n",
       "   0.9840255379676819,\n",
       "   0.9872204661369324,\n",
       "   0.9968051314353943,\n",
       "   0.9872204661369324,\n",
       "   0.9776358008384705,\n",
       "   0.9872204661369324,\n",
       "   1.0,\n",
       "   0.9904153347015381,\n",
       "   1.0,\n",
       "   0.9968051314353943,\n",
       "   0.9968051314353943,\n",
       "   1.0,\n",
       "   1.0,\n",
       "   1.0,\n",
       "   1.0,\n",
       "   1.0,\n",
       "   1.0,\n",
       "   1.0],\n",
       "  [0.8852040767669678,\n",
       "   0.954081654548645,\n",
       "   0.9515306353569031,\n",
       "   0.9744898080825806,\n",
       "   0.9846938848495483,\n",
       "   0.9693877696990967,\n",
       "   0.9923469424247742,\n",
       "   0.9515306353569031,\n",
       "   0.9974489808082581,\n",
       "   0.9974489808082581,\n",
       "   1.0,\n",
       "   1.0,\n",
       "   0.9795918464660645,\n",
       "   0.9642857313156128,\n",
       "   1.0,\n",
       "   0.9923469424247742,\n",
       "   1.0,\n",
       "   0.9846938848495483,\n",
       "   1.0,\n",
       "   0.9948979616165161],\n",
       "  [0.8360000252723694,\n",
       "   0.9520000219345093,\n",
       "   0.9679999947547913,\n",
       "   0.9480000138282776,\n",
       "   0.9879999756813049,\n",
       "   0.9959999918937683,\n",
       "   0.9879999756813049,\n",
       "   0.9919999837875366,\n",
       "   0.9940000176429749,\n",
       "   0.9980000257492065,\n",
       "   1.0,\n",
       "   0.9980000257492065,\n",
       "   1.0,\n",
       "   0.9940000176429749,\n",
       "   0.9940000176429749,\n",
       "   0.9919999837875366,\n",
       "   0.9980000257492065,\n",
       "   0.9980000257492065,\n",
       "   1.0,\n",
       "   1.0],\n",
       "  [0.883289098739624,\n",
       "   0.8912466764450073,\n",
       "   0.9389920234680176,\n",
       "   0.9257294535636902,\n",
       "   0.883289098739624,\n",
       "   0.8488063812255859,\n",
       "   0.8753315806388855,\n",
       "   0.9283819794654846,\n",
       "   0.9920424222946167,\n",
       "   0.9655172228813171,\n",
       "   0.9946949481964111,\n",
       "   0.9734748005867004,\n",
       "   0.8063660264015198,\n",
       "   0.7002652287483215,\n",
       "   0.8647214770317078,\n",
       "   0.9442970752716064,\n",
       "   0.9628646969795227,\n",
       "   0.960212230682373,\n",
       "   0.9946949481964111,\n",
       "   0.9204244017601013],\n",
       "  [0.8790322542190552,\n",
       "   0.9354838728904724,\n",
       "   0.9193548560142517,\n",
       "   0.9650537371635437,\n",
       "   0.9704301357269287,\n",
       "   0.9892473220825195,\n",
       "   1.0,\n",
       "   0.9865591526031494,\n",
       "   0.9892473220825195,\n",
       "   0.9973118305206299,\n",
       "   0.9973118305206299,\n",
       "   1.0,\n",
       "   0.9946236610412598,\n",
       "   0.9919354915618896,\n",
       "   0.9973118305206299,\n",
       "   0.9973118305206299,\n",
       "   0.9973118305206299,\n",
       "   0.9946236610412598,\n",
       "   0.9973118305206299,\n",
       "   0.9973118305206299],\n",
       "  [0.8025316596031189,\n",
       "   0.7645569443702698,\n",
       "   0.9417721629142761,\n",
       "   0.8379746675491333,\n",
       "   0.9544304013252258,\n",
       "   0.949367105960846,\n",
       "   0.9189873337745667,\n",
       "   0.9518987536430359,\n",
       "   0.9443038105964661,\n",
       "   0.8354430198669434,\n",
       "   0.9544304013252258,\n",
       "   0.8759493827819824,\n",
       "   0.9518987536430359,\n",
       "   0.9569620490074158,\n",
       "   0.9518987536430359,\n",
       "   0.9392405152320862,\n",
       "   0.9721518754959106,\n",
       "   0.9696202278137207,\n",
       "   0.9822784662246704,\n",
       "   0.9974683523178101],\n",
       "  [0.8376068472862244,\n",
       "   0.9017093777656555,\n",
       "   0.8311966061592102,\n",
       "   0.9722222089767456,\n",
       "   0.9893162250518799,\n",
       "   0.9914529919624329,\n",
       "   0.997863233089447,\n",
       "   1.0,\n",
       "   0.9829059839248657,\n",
       "   0.9935897588729858,\n",
       "   1.0,\n",
       "   0.995726466178894,\n",
       "   0.997863233089447,\n",
       "   0.997863233089447,\n",
       "   0.995726466178894,\n",
       "   0.9935897588729858,\n",
       "   0.997863233089447,\n",
       "   0.9893162250518799,\n",
       "   0.9914529919624329,\n",
       "   0.9935897588729858],\n",
       "  [0.9036402702331543,\n",
       "   0.9721627235412598,\n",
       "   0.9828693866729736,\n",
       "   0.9914346933364868,\n",
       "   0.9828693866729736,\n",
       "   0.980728030204773,\n",
       "   0.9828693866729736,\n",
       "   1.0,\n",
       "   1.0,\n",
       "   1.0,\n",
       "   1.0,\n",
       "   1.0,\n",
       "   0.9914346933364868,\n",
       "   1.0,\n",
       "   0.9914346933364868,\n",
       "   0.9978586435317993,\n",
       "   1.0,\n",
       "   0.9935759902000427,\n",
       "   1.0,\n",
       "   1.0],\n",
       "  [0.800000011920929,\n",
       "   0.8793103694915771,\n",
       "   0.5310344696044922,\n",
       "   0.9724137783050537,\n",
       "   0.9862068891525269,\n",
       "   1.0,\n",
       "   1.0,\n",
       "   1.0,\n",
       "   0.9965517520904541,\n",
       "   0.9965517520904541,\n",
       "   1.0,\n",
       "   1.0,\n",
       "   0.9965517520904541,\n",
       "   1.0,\n",
       "   0.9965517520904541,\n",
       "   0.9965517520904541,\n",
       "   0.9965517520904541,\n",
       "   1.0,\n",
       "   1.0,\n",
       "   1.0],\n",
       "  [0.7142857313156128,\n",
       "   0.8721804618835449,\n",
       "   0.9962406158447266,\n",
       "   0.9661654233932495,\n",
       "   0.969924807548523,\n",
       "   0.981203019618988,\n",
       "   0.9924812316894531,\n",
       "   0.9398496150970459,\n",
       "   1.0,\n",
       "   0.9962406158447266,\n",
       "   0.9962406158447266,\n",
       "   1.0,\n",
       "   1.0,\n",
       "   0.9661654233932495,\n",
       "   1.0,\n",
       "   0.9887217879295349,\n",
       "   1.0,\n",
       "   0.9774436354637146,\n",
       "   1.0,\n",
       "   0.9887217879295349],\n",
       "  [0.9023861289024353,\n",
       "   0.924078106880188,\n",
       "   0.9110628962516785,\n",
       "   0.963123619556427,\n",
       "   0.9848155975341797,\n",
       "   0.9501084685325623,\n",
       "   0.9696312546730042,\n",
       "   0.9718004465103149,\n",
       "   0.9848155975341797,\n",
       "   0.9913232326507568,\n",
       "   0.9978308081626892,\n",
       "   0.9783080220222473,\n",
       "   0.9934924244880676,\n",
       "   0.9739696383476257,\n",
       "   0.989154040813446,\n",
       "   0.9783080220222473,\n",
       "   0.9783080220222473,\n",
       "   0.9783080220222473,\n",
       "   0.9804772138595581,\n",
       "   0.9956616163253784],\n",
       "  [0.8285714387893677,\n",
       "   0.9607142806053162,\n",
       "   0.9678571224212646,\n",
       "   0.9321428537368774,\n",
       "   0.9750000238418579,\n",
       "   0.9892857074737549,\n",
       "   0.9892857074737549,\n",
       "   0.9857142567634583,\n",
       "   0.9928571581840515,\n",
       "   0.6535714268684387,\n",
       "   0.9821428656578064,\n",
       "   0.9750000238418579,\n",
       "   0.9928571581840515,\n",
       "   0.7749999761581421,\n",
       "   0.7928571701049805,\n",
       "   0.9821428656578064,\n",
       "   0.9964285492897034,\n",
       "   0.9964285492897034,\n",
       "   0.9964285492897034,\n",
       "   0.9964285492897034],\n",
       "  [0.8471074104309082,\n",
       "   0.9400826692581177,\n",
       "   0.9669421315193176,\n",
       "   0.9669421315193176,\n",
       "   0.9586777091026306,\n",
       "   0.9938016533851624,\n",
       "   0.9855371713638306,\n",
       "   0.9855371713638306,\n",
       "   0.9731404781341553,\n",
       "   0.9690082669258118,\n",
       "   0.9938016533851624,\n",
       "   0.9648760557174683,\n",
       "   0.9917355179786682,\n",
       "   0.9896694421768188,\n",
       "   0.9917355179786682,\n",
       "   0.9958677887916565,\n",
       "   0.9958677887916565,\n",
       "   0.9938016533851624,\n",
       "   0.9938016533851624,\n",
       "   0.9855371713638306],\n",
       "  [0.8159509301185608,\n",
       "   0.8067484498023987,\n",
       "   0.89570552110672,\n",
       "   0.9907975196838379,\n",
       "   0.9938650131225586,\n",
       "   0.9785276055335999,\n",
       "   0.9785276055335999,\n",
       "   0.9723926186561584,\n",
       "   0.9969325065612793,\n",
       "   0.9478527903556824,\n",
       "   0.9478527903556824,\n",
       "   0.9202454090118408,\n",
       "   0.9693251252174377,\n",
       "   1.0,\n",
       "   0.9662576913833618,\n",
       "   0.9815950989723206,\n",
       "   0.9754601120948792,\n",
       "   1.0,\n",
       "   0.9907975196838379,\n",
       "   0.9938650131225586],\n",
       "  [0.8412438631057739,\n",
       "   0.9427168369293213,\n",
       "   0.9607201218605042,\n",
       "   0.9590834975242615,\n",
       "   0.9836333990097046,\n",
       "   0.9770867228507996,\n",
       "   0.9672667980194092,\n",
       "   0.9541735053062439,\n",
       "   0.9394435286521912,\n",
       "   0.9901800155639648,\n",
       "   0.9705401062965393,\n",
       "   0.9476268291473389,\n",
       "   0.9607201218605042,\n",
       "   0.9885433912277222,\n",
       "   0.9754500985145569,\n",
       "   0.9836333990097046,\n",
       "   0.9869067072868347,\n",
       "   0.9869067072868347,\n",
       "   0.978723406791687,\n",
       "   0.9803600907325745],\n",
       "  [0.8962963223457336,\n",
       "   0.9259259104728699,\n",
       "   0.9851852059364319,\n",
       "   0.9679012298583984,\n",
       "   0.9753086566925049,\n",
       "   0.9580246806144714,\n",
       "   0.980246901512146,\n",
       "   0.9679012298583984,\n",
       "   0.9654321074485779,\n",
       "   0.990123450756073,\n",
       "   0.9851852059364319,\n",
       "   0.990123450756073,\n",
       "   0.9654321074485779,\n",
       "   0.6000000238418579,\n",
       "   0.9580246806144714,\n",
       "   0.9679012298583984,\n",
       "   0.9876543283462524,\n",
       "   0.9925925731658936,\n",
       "   0.9950617551803589,\n",
       "   0.9876543283462524],\n",
       "  [0.8928571343421936,\n",
       "   0.913690447807312,\n",
       "   0.9345238208770752,\n",
       "   0.9077380895614624,\n",
       "   0.8422619104385376,\n",
       "   0.8601190447807312,\n",
       "   0.9077380895614624,\n",
       "   0.9345238208770752,\n",
       "   0.9583333134651184,\n",
       "   0.9226190447807312,\n",
       "   0.9642857313156128,\n",
       "   0.9732142686843872,\n",
       "   0.9642857313156128,\n",
       "   0.961309552192688,\n",
       "   0.925595223903656,\n",
       "   0.9732142686843872,\n",
       "   0.976190447807312,\n",
       "   0.9672619104385376,\n",
       "   0.9553571343421936,\n",
       "   0.9791666865348816],\n",
       "  [0.9438202381134033,\n",
       "   0.9859550595283508,\n",
       "   0.8230336904525757,\n",
       "   0.9943820238113403,\n",
       "   1.0,\n",
       "   1.0,\n",
       "   1.0,\n",
       "   1.0,\n",
       "   1.0,\n",
       "   1.0,\n",
       "   1.0,\n",
       "   0.9943820238113403,\n",
       "   1.0,\n",
       "   1.0,\n",
       "   1.0,\n",
       "   1.0,\n",
       "   1.0,\n",
       "   0.9971910119056702,\n",
       "   1.0,\n",
       "   1.0],\n",
       "  [0.857594907283783,\n",
       "   0.952531635761261,\n",
       "   0.8702531456947327,\n",
       "   0.9683544039726257,\n",
       "   0.9715189933776855,\n",
       "   0.9873417615890503,\n",
       "   0.996835470199585,\n",
       "   0.996835470199585,\n",
       "   0.9873417615890503,\n",
       "   0.996835470199585,\n",
       "   1.0,\n",
       "   0.996835470199585,\n",
       "   1.0,\n",
       "   0.8987341523170471,\n",
       "   1.0,\n",
       "   0.996835470199585,\n",
       "   1.0,\n",
       "   0.9936708807945251,\n",
       "   0.9841772317886353,\n",
       "   1.0],\n",
       "  [0.829126238822937,\n",
       "   0.9475728273391724,\n",
       "   0.9708737730979919,\n",
       "   0.9883494973182678,\n",
       "   0.9922330379486084,\n",
       "   0.984466016292572,\n",
       "   0.9922330379486084,\n",
       "   0.9922330379486084,\n",
       "   0.9980582594871521,\n",
       "   0.9941747784614563,\n",
       "   0.9825242757797241,\n",
       "   0.9883494973182678,\n",
       "   0.9980582594871521,\n",
       "   0.9961165189743042,\n",
       "   0.9980582594871521,\n",
       "   0.9961165189743042,\n",
       "   1.0,\n",
       "   0.9941747784614563,\n",
       "   0.9980582594871521,\n",
       "   0.9961165189743042],\n",
       "  [0.8504902124404907,\n",
       "   0.9289215803146362,\n",
       "   0.9093137383460999,\n",
       "   0.9411764740943909,\n",
       "   0.970588207244873,\n",
       "   0.9289215803146362,\n",
       "   0.9803921580314636,\n",
       "   0.9803921580314636,\n",
       "   0.9975489974021912,\n",
       "   0.9852941036224365,\n",
       "   0.9975489974021912,\n",
       "   1.0,\n",
       "   0.9803921580314636,\n",
       "   0.968137264251709,\n",
       "   1.0,\n",
       "   1.0,\n",
       "   0.9779411554336548,\n",
       "   1.0,\n",
       "   0.9950980544090271,\n",
       "   0.9975489974021912]],\n",
       " [[0.825421929359436,\n",
       "   0.8953478336334229,\n",
       "   0.9424852728843689,\n",
       "   0.9653504490852356,\n",
       "   0.9761674404144287,\n",
       "   0.9795092940330505,\n",
       "   0.986720621585846,\n",
       "   0.959722101688385,\n",
       "   0.9267434477806091,\n",
       "   0.9093307256698608,\n",
       "   0.9411661028862,\n",
       "   0.9555008411407471,\n",
       "   0.9595462083816528,\n",
       "   0.963239848613739,\n",
       "   0.9697476029396057,\n",
       "   0.9814440011978149,\n",
       "   0.9841702580451965,\n",
       "   0.9835546612739563,\n",
       "   0.986720621585846,\n",
       "   0.9858411550521851],\n",
       "  [0.8233190178871155,\n",
       "   0.8716433644294739,\n",
       "   0.9345685839653015,\n",
       "   0.8288578391075134,\n",
       "   0.9016290903091431,\n",
       "   0.9389545321464539,\n",
       "   0.9539026021957397,\n",
       "   0.9623165130615234,\n",
       "   0.9701933264732361,\n",
       "   0.9767275452613831,\n",
       "   0.9796813726425171,\n",
       "   0.9814715385437012,\n",
       "   0.9833512306213379,\n",
       "   0.9838882684707642,\n",
       "   0.981203019618988,\n",
       "   0.9801288843154907,\n",
       "   0.9883637428283691,\n",
       "   0.990153968334198,\n",
       "   0.9923021793365479,\n",
       "   0.9885427951812744],\n",
       "  [0.8296962380409241,\n",
       "   0.8807091116905212,\n",
       "   0.9542397856712341,\n",
       "   0.9813116788864136,\n",
       "   0.9856781363487244,\n",
       "   0.9891712665557861,\n",
       "   0.9930137395858765,\n",
       "   0.9927517175674438,\n",
       "   0.9786044955253601,\n",
       "   0.9836695194244385,\n",
       "   0.9890839457511902,\n",
       "   0.9896078705787659,\n",
       "   0.9868133664131165,\n",
       "   0.9937996864318848,\n",
       "   0.9920530915260315,\n",
       "   0.997729480266571,\n",
       "   0.9985154271125793,\n",
       "   0.9981660842895508,\n",
       "   0.9981660842895508,\n",
       "   0.9982534050941467],\n",
       "  [0.825419008731842,\n",
       "   0.8810730576515198,\n",
       "   0.9401432871818542,\n",
       "   0.9662705063819885,\n",
       "   0.9553477764129639,\n",
       "   0.9689793586730957,\n",
       "   0.9843586087226868,\n",
       "   0.9761447310447693,\n",
       "   0.9854945540428162,\n",
       "   0.9915239214897156,\n",
       "   0.9915239214897156,\n",
       "   0.9912617802619934,\n",
       "   0.9927473068237305,\n",
       "   0.9958930611610413,\n",
       "   0.9932715892791748,\n",
       "   0.9958056807518005,\n",
       "   0.9944075345993042,\n",
       "   0.99510657787323,\n",
       "   0.9977280497550964,\n",
       "   0.9979028105735779],\n",
       "  [0.8301952481269836,\n",
       "   0.8929039239883423,\n",
       "   0.9304355382919312,\n",
       "   0.9504233002662659,\n",
       "   0.9752116799354553,\n",
       "   0.9830670952796936,\n",
       "   0.9813214540481567,\n",
       "   0.9794885516166687,\n",
       "   0.9925809502601624,\n",
       "   0.9858601689338684,\n",
       "   0.9949375987052917,\n",
       "   0.9936283230781555,\n",
       "   0.9950248599052429,\n",
       "   0.9928427934646606,\n",
       "   0.9976433515548706,\n",
       "   0.9958977103233337,\n",
       "   0.9970324039459229,\n",
       "   0.9965959787368774,\n",
       "   0.9965086579322815,\n",
       "   0.9960722923278809],\n",
       "  [0.8281911015510559,\n",
       "   0.8717270493507385,\n",
       "   0.9367892146110535,\n",
       "   0.9802521467208862,\n",
       "   0.9867759943008423,\n",
       "   0.9854536056518555,\n",
       "   0.9921537637710571,\n",
       "   0.9916247725486755,\n",
       "   0.9944459199905396,\n",
       "   0.9949748516082764,\n",
       "   0.995944619178772,\n",
       "   0.996738076210022,\n",
       "   0.9953275322914124,\n",
       "   0.997178852558136,\n",
       "   0.9955919981002808,\n",
       "   0.9964736104011536,\n",
       "   0.995944619178772,\n",
       "   0.9981486201286316,\n",
       "   0.9993828535079956,\n",
       "   0.9996473789215088],\n",
       "  [0.8229895234107971,\n",
       "   0.8945295214653015,\n",
       "   0.9518599510192871,\n",
       "   0.971991240978241,\n",
       "   0.9817067980766296,\n",
       "   0.9815317392349243,\n",
       "   0.9876586198806763,\n",
       "   0.9890590906143188,\n",
       "   0.9929102659225464,\n",
       "   0.9930853247642517,\n",
       "   0.9960612654685974,\n",
       "   0.9948359131813049,\n",
       "   0.9942231774330139,\n",
       "   0.9938730597496033,\n",
       "   0.9962363243103027,\n",
       "   0.9954485893249512,\n",
       "   0.9982494711875916,\n",
       "   0.99746173620224,\n",
       "   0.9934354424476624,\n",
       "   0.9978117942810059],\n",
       "  [0.8289612531661987,\n",
       "   0.8851910829544067,\n",
       "   0.93731290102005,\n",
       "   0.9683923125267029,\n",
       "   0.9729706048965454,\n",
       "   0.9808945059776306,\n",
       "   0.9854727983474731,\n",
       "   0.9878499507904053,\n",
       "   0.9907554388046265,\n",
       "   0.9853847622871399,\n",
       "   0.9908434748649597,\n",
       "   0.9944532513618469,\n",
       "   0.9963021874427795,\n",
       "   0.9939249753952026,\n",
       "   0.9949815273284912,\n",
       "   0.995685875415802,\n",
       "   0.9973586797714233,\n",
       "   0.9957739114761353,\n",
       "   0.9955097436904907,\n",
       "   0.9974467158317566],\n",
       "  [0.8371826410293579,\n",
       "   0.8948946595191956,\n",
       "   0.9457719922065735,\n",
       "   0.9667577743530273,\n",
       "   0.9005378484725952,\n",
       "   0.9271669387817383,\n",
       "   0.9585574269294739,\n",
       "   0.9716074466705322,\n",
       "   0.9790142178535461,\n",
       "   0.9817476272583008,\n",
       "   0.9861564040184021,\n",
       "   0.9873908758163452,\n",
       "   0.9878317713737488,\n",
       "   0.9901243448257446,\n",
       "   0.9895070791244507,\n",
       "   0.991358757019043,\n",
       "   0.9926814436912537,\n",
       "   0.9909179210662842,\n",
       "   0.9925050735473633,\n",
       "   0.9941803812980652],\n",
       "  [0.8360684514045715,\n",
       "   0.8956475853919983,\n",
       "   0.9438035488128662,\n",
       "   0.9621569514274597,\n",
       "   0.978500247001648,\n",
       "   0.9845306873321533,\n",
       "   0.9887257218360901,\n",
       "   0.9883761405944824,\n",
       "   0.9923964142799377,\n",
       "   0.9931830167770386,\n",
       "   0.9934452176094055,\n",
       "   0.99667888879776,\n",
       "   0.9951931238174438,\n",
       "   0.9960671067237854,\n",
       "   0.996941089630127,\n",
       "   0.9981646537780762,\n",
       "   0.9944065809249878,\n",
       "   0.9945813417434692,\n",
       "   0.9990386366844177,\n",
       "   0.9987764358520508],\n",
       "  [0.8216420412063599,\n",
       "   0.8857696056365967,\n",
       "   0.9401566386222839,\n",
       "   0.9669981598854065,\n",
       "   0.9743025898933411,\n",
       "   0.9807268977165222,\n",
       "   0.9834550619125366,\n",
       "   0.9867992401123047,\n",
       "   0.9897914528846741,\n",
       "   0.9913755059242249,\n",
       "   0.9918155670166016,\n",
       "   0.9919915795326233,\n",
       "   0.9953357577323914,\n",
       "   0.9926955699920654,\n",
       "   0.9952477216720581,\n",
       "   0.9967437982559204,\n",
       "   0.994279682636261,\n",
       "   0.998327910900116,\n",
       "   0.9952477216720581,\n",
       "   0.9986799359321594],\n",
       "  [0.8298118114471436,\n",
       "   0.8889382481575012,\n",
       "   0.9397600889205933,\n",
       "   0.9481119513511658,\n",
       "   0.9646379351615906,\n",
       "   0.9795646667480469,\n",
       "   0.9827632308006287,\n",
       "   0.9848955869674683,\n",
       "   0.98809415102005,\n",
       "   0.9887161254882812,\n",
       "   0.9922701120376587,\n",
       "   0.994313657283783,\n",
       "   0.9929808974266052,\n",
       "   0.9951132535934448,\n",
       "   0.9956463575363159,\n",
       "   0.9952021241188049,\n",
       "   0.9953798055648804,\n",
       "   0.9938693642616272,\n",
       "   0.9967125654220581,\n",
       "   0.9954686760902405],\n",
       "  [0.8254915475845337,\n",
       "   0.8686939477920532,\n",
       "   0.9336438775062561,\n",
       "   0.9604499936103821,\n",
       "   0.9100896716117859,\n",
       "   0.8808226585388184,\n",
       "   0.9085076451301575,\n",
       "   0.9332044124603271,\n",
       "   0.945069432258606,\n",
       "   0.9739848971366882,\n",
       "   0.9811038970947266,\n",
       "   0.9853225350379944,\n",
       "   0.9876076579093933,\n",
       "   0.9593953490257263,\n",
       "   0.8827561736106873,\n",
       "   0.9556161165237427,\n",
       "   0.9803128838539124,\n",
       "   0.9881349802017212,\n",
       "   0.9905958771705627,\n",
       "   0.9905080199241638],\n",
       "  [0.8313729166984558,\n",
       "   0.9015198349952698,\n",
       "   0.9493103623390198,\n",
       "   0.966968297958374,\n",
       "   0.9776860475540161,\n",
       "   0.9840990900993347,\n",
       "   0.9847140312194824,\n",
       "   0.9830448627471924,\n",
       "   0.9900729060173035,\n",
       "   0.9940261840820312,\n",
       "   0.9925327301025391,\n",
       "   0.9932355284690857,\n",
       "   0.9959588646888733,\n",
       "   0.9955196380615234,\n",
       "   0.9935869574546814,\n",
       "   0.9958710074424744,\n",
       "   0.9977158904075623,\n",
       "   0.9968373775482178,\n",
       "   0.998242974281311,\n",
       "   0.9972766637802124],\n",
       "  [0.8315752744674683,\n",
       "   0.8883614540100098,\n",
       "   0.9432137608528137,\n",
       "   0.8678797483444214,\n",
       "   0.9380274415016174,\n",
       "   0.9691455960273743,\n",
       "   0.962992250919342,\n",
       "   0.9743319153785706,\n",
       "   0.982507050037384,\n",
       "   0.984265148639679,\n",
       "   0.9889240264892578,\n",
       "   0.9909458756446838,\n",
       "   0.9890998601913452,\n",
       "   0.992352306842804,\n",
       "   0.9915611743927002,\n",
       "   0.9941983222961426,\n",
       "   0.9929676651954651,\n",
       "   0.9945499300956726,\n",
       "   0.9937587976455688,\n",
       "   0.9949015378952026],\n",
       "  [0.8290545344352722,\n",
       "   0.8951005339622498,\n",
       "   0.9230087995529175,\n",
       "   0.9695224761962891,\n",
       "   0.9814831018447876,\n",
       "   0.9882165193557739,\n",
       "   0.992292046546936,\n",
       "   0.9931780099868774,\n",
       "   0.9923806190490723,\n",
       "   0.9953043460845947,\n",
       "   0.9953043460845947,\n",
       "   0.9943297505378723,\n",
       "   0.9967218637466431,\n",
       "   0.9962788820266724,\n",
       "   0.9945069551467896,\n",
       "   0.9972534775733948,\n",
       "   0.9970762729644775,\n",
       "   0.9967218637466431,\n",
       "   0.9976078867912292,\n",
       "   0.9966332912445068],\n",
       "  [0.8291430473327637,\n",
       "   0.8921864032745361,\n",
       "   0.9446314573287964,\n",
       "   0.956768274307251,\n",
       "   0.9657158255577087,\n",
       "   0.9804216623306274,\n",
       "   0.9805988669395447,\n",
       "   0.9854713082313538,\n",
       "   0.9899007678031921,\n",
       "   0.9927356243133545,\n",
       "   0.9931786060333252,\n",
       "   0.9938873052597046,\n",
       "   0.9914067983627319,\n",
       "   0.9965450167655945,\n",
       "   0.9942416548728943,\n",
       "   0.9966335892677307,\n",
       "   0.9971651434898376,\n",
       "   0.9957476854324341,\n",
       "   0.9950389862060547,\n",
       "   0.9980510473251343],\n",
       "  [0.827754557132721,\n",
       "   0.8840819597244263,\n",
       "   0.9256868958473206,\n",
       "   0.9599651098251343,\n",
       "   0.9861317276954651,\n",
       "   0.9875272512435913,\n",
       "   0.9924116730690002,\n",
       "   0.9926733374595642,\n",
       "   0.9938072562217712,\n",
       "   0.9924116730690002,\n",
       "   0.9947666525840759,\n",
       "   0.9964239001274109,\n",
       "   0.9945922493934631,\n",
       "   0.9981683492660522,\n",
       "   0.9955517053604126,\n",
       "   0.9980810880661011,\n",
       "   0.9958133697509766,\n",
       "   0.9984300136566162,\n",
       "   0.9980810880661011,\n",
       "   0.9975577592849731],\n",
       "  [0.8332753777503967,\n",
       "   0.8859778642654419,\n",
       "   0.9250587224960327,\n",
       "   0.9780659675598145,\n",
       "   0.987640380859375,\n",
       "   0.9905126690864563,\n",
       "   0.9929497838020325,\n",
       "   0.9923405051231384,\n",
       "   0.9932979345321655,\n",
       "   0.9937331080436707,\n",
       "   0.9959961771965027,\n",
       "   0.9960831999778748,\n",
       "   0.9958220720291138,\n",
       "   0.9961702227592468,\n",
       "   0.9952127933502197,\n",
       "   0.9975628852844238,\n",
       "   0.9954739212989807,\n",
       "   0.9986944198608398,\n",
       "   0.9966054558753967,\n",
       "   0.9985203146934509],\n",
       "  [0.8290545344352722,\n",
       "   0.8894988298416138,\n",
       "   0.9354524612426758,\n",
       "   0.9604214429855347,\n",
       "   0.9752081036567688,\n",
       "   0.9638746380805969,\n",
       "   0.9815831184387207,\n",
       "   0.9865415096282959,\n",
       "   0.9885780215263367,\n",
       "   0.9857446551322937,\n",
       "   0.9943332672119141,\n",
       "   0.9947760105133057,\n",
       "   0.9925624132156372,\n",
       "   0.9914113879203796,\n",
       "   0.9965468645095825,\n",
       "   0.9949530959129333,\n",
       "   0.9977864623069763,\n",
       "   0.997166633605957,\n",
       "   0.996635377407074,\n",
       "   0.9976093769073486],\n",
       "  [0.8315633535385132,\n",
       "   0.8788670897483826,\n",
       "   0.938039243221283,\n",
       "   0.9449237585067749,\n",
       "   0.9651415944099426,\n",
       "   0.9776034951210022,\n",
       "   0.9866666793823242,\n",
       "   0.9869281053543091,\n",
       "   0.9913725256919861,\n",
       "   0.9755991101264954,\n",
       "   0.9215686321258545,\n",
       "   0.9537255167961121,\n",
       "   0.9661873579025269,\n",
       "   0.8746840953826904,\n",
       "   0.9182570576667786,\n",
       "   0.9268845319747925,\n",
       "   0.9569498896598816,\n",
       "   0.9656645059585571,\n",
       "   0.9714161157608032,\n",
       "   0.9767320156097412],\n",
       "  [0.8242907524108887,\n",
       "   0.913583517074585,\n",
       "   0.9755123853683472,\n",
       "   0.9830538630485535,\n",
       "   0.987667441368103,\n",
       "   0.9900630116462708,\n",
       "   0.993168294429779,\n",
       "   0.9930796027183533,\n",
       "   0.995031476020813,\n",
       "   0.9955638647079468,\n",
       "   0.9949427843093872,\n",
       "   0.9957413077354431,\n",
       "   0.9962736368179321,\n",
       "   0.9965397715568542,\n",
       "   0.9973382949829102,\n",
       "   0.9960961937904358,\n",
       "   0.9975157380104065,\n",
       "   0.9968059659004211,\n",
       "   0.9976931810379028,\n",
       "   0.9973382949829102],\n",
       "  [0.8252622485160828,\n",
       "   0.8752296566963196,\n",
       "   0.9242278337478638,\n",
       "   0.9550266861915588,\n",
       "   0.9733135104179382,\n",
       "   0.981188178062439,\n",
       "   0.986175537109375,\n",
       "   0.9846881031990051,\n",
       "   0.9894129037857056,\n",
       "   0.9111033082008362,\n",
       "   0.93691486120224,\n",
       "   0.9699011445045471,\n",
       "   0.9810132384300232,\n",
       "   0.9853005409240723,\n",
       "   0.9875754714012146,\n",
       "   0.9902003407478333,\n",
       "   0.988012969493866,\n",
       "   0.9905503392219543,\n",
       "   0.9921252727508545,\n",
       "   0.9960626363754272],\n",
       "  [0.8272058963775635,\n",
       "   0.8806532621383667,\n",
       "   0.9301866292953491,\n",
       "   0.969131350517273,\n",
       "   0.9857322573661804,\n",
       "   0.9920136332511902,\n",
       "   0.990847110748291,\n",
       "   0.9931802153587341,\n",
       "   0.9943467378616333,\n",
       "   0.9951543211936951,\n",
       "   0.9967695474624634,\n",
       "   0.9970387816429138,\n",
       "   0.9949748516082764,\n",
       "   0.9980258345603943,\n",
       "   0.9969490170478821,\n",
       "   0.9982950687408447,\n",
       "   0.9955132603645325,\n",
       "   0.998205304145813,\n",
       "   0.9991026520729065,\n",
       "   0.9973977208137512],\n",
       "  [0.8257922530174255,\n",
       "   0.8842290639877319,\n",
       "   0.9399119019508362,\n",
       "   0.883788526058197,\n",
       "   0.9486343860626221,\n",
       "   0.9703083634376526,\n",
       "   0.982731282711029,\n",
       "   0.9851982593536377,\n",
       "   0.9892510771751404,\n",
       "   0.9902202486991882,\n",
       "   0.9906607866287231,\n",
       "   0.991541862487793,\n",
       "   0.9934802055358887,\n",
       "   0.9396475553512573,\n",
       "   0.8965638875961304,\n",
       "   0.9524229168891907,\n",
       "   0.9669603705406189,\n",
       "   0.9796475768089294,\n",
       "   0.9822907447814941,\n",
       "   0.9855506420135498],\n",
       "  [0.826242983341217,\n",
       "   0.8836150169372559,\n",
       "   0.939399242401123,\n",
       "   0.960942268371582,\n",
       "   0.9227603077888489,\n",
       "   0.8654873371124268,\n",
       "   0.9138278365135193,\n",
       "   0.9535861015319824,\n",
       "   0.9695245027542114,\n",
       "   0.9723268151283264,\n",
       "   0.9768806099891663,\n",
       "   0.9829232096672058,\n",
       "   0.9830107688903809,\n",
       "   0.9851125478744507,\n",
       "   0.9871267080307007,\n",
       "   0.9882651567459106,\n",
       "   0.9895787835121155,\n",
       "   0.9891409277915955,\n",
       "   0.9914178252220154,\n",
       "   0.9931693077087402],\n",
       "  [0.8345897793769836,\n",
       "   0.8864812850952148,\n",
       "   0.9218352437019348,\n",
       "   0.9463110566139221,\n",
       "   0.9830687046051025,\n",
       "   0.9857882261276245,\n",
       "   0.9885077476501465,\n",
       "   0.9928941130638123,\n",
       "   0.9939468502998352,\n",
       "   0.9929818511009216,\n",
       "   0.9942977428436279,\n",
       "   0.9942977428436279,\n",
       "   0.9969295263290405,\n",
       "   0.9967541098594666,\n",
       "   0.9961400032043457,\n",
       "   0.9973682165145874,\n",
       "   0.9971927404403687,\n",
       "   0.9971927404403687,\n",
       "   0.997280478477478,\n",
       "   0.9967541098594666],\n",
       "  [0.828496515750885,\n",
       "   0.8832939863204956,\n",
       "   0.9277908802032471,\n",
       "   0.9141533374786377,\n",
       "   0.9489465951919556,\n",
       "   0.9484220743179321,\n",
       "   0.9730745553970337,\n",
       "   0.9818165898323059,\n",
       "   0.9881982803344727,\n",
       "   0.9902089238166809,\n",
       "   0.9915202260017395,\n",
       "   0.9923944473266602,\n",
       "   0.9945799708366394,\n",
       "   0.9935309290885925,\n",
       "   0.9930937886238098,\n",
       "   0.9944050908088684,\n",
       "   0.9958038330078125,\n",
       "   0.9958038330078125,\n",
       "   0.9958038330078125,\n",
       "   0.9958038330078125],\n",
       "  [0.8169452548027039,\n",
       "   0.8824732899665833,\n",
       "   0.9352313280105591,\n",
       "   0.9558718800544739,\n",
       "   0.9713523387908936,\n",
       "   0.9819394946098328,\n",
       "   0.9812277555465698,\n",
       "   0.9887010455131531,\n",
       "   0.9885231256484985,\n",
       "   0.9930604696273804,\n",
       "   0.9923487305641174,\n",
       "   0.9923487305641174,\n",
       "   0.9823843240737915,\n",
       "   0.9960854053497314,\n",
       "   0.9952846765518188,\n",
       "   0.9960854053497314,\n",
       "   0.9970640540122986,\n",
       "   0.9969750642776489,\n",
       "   0.9954626560211182,\n",
       "   0.9950177669525146],\n",
       "  [0.8340669274330139,\n",
       "   0.8946858048439026,\n",
       "   0.9423636198043823,\n",
       "   0.9648365378379822,\n",
       "   0.972327470779419,\n",
       "   0.9734731912612915,\n",
       "   0.9832555055618286,\n",
       "   0.9860756397247314,\n",
       "   0.9894245266914368,\n",
       "   0.9896007776260376,\n",
       "   0.9924209117889404,\n",
       "   0.9928615689277649,\n",
       "   0.9948885440826416,\n",
       "   0.9918040037155151,\n",
       "   0.993742823600769,\n",
       "   0.9977086186408997,\n",
       "   0.9977086186408997,\n",
       "   0.9917158484458923,\n",
       "   0.997356116771698,\n",
       "   0.9958579540252686]],\n",
       " [[0.4675550162792206,\n",
       "   0.13857969641685486,\n",
       "   0.12297149747610092,\n",
       "   0.168773353099823,\n",
       "   0.017564212903380394,\n",
       "   0.055447813123464584,\n",
       "   0.0693073496222496,\n",
       "   0.061269864439964294,\n",
       "   0.4053420126438141,\n",
       "   0.21330644190311432,\n",
       "   0.1683509200811386,\n",
       "   0.11517035961151123,\n",
       "   0.15404193103313446,\n",
       "   0.07447058707475662,\n",
       "   0.05613596364855766,\n",
       "   0.03415996953845024,\n",
       "   0.05607190355658531,\n",
       "   0.03068753518164158,\n",
       "   0.03923797234892845,\n",
       "   0.029116785153746605],\n",
       "  [0.37725090980529785,\n",
       "   0.23405739665031433,\n",
       "   0.13293446600437164,\n",
       "   0.3981103301048279,\n",
       "   0.0848507434129715,\n",
       "   0.09753463417291641,\n",
       "   0.11749715358018875,\n",
       "   0.05317321792244911,\n",
       "   0.04943593963980675,\n",
       "   0.07029267400503159,\n",
       "   2.5465750694274902,\n",
       "   0.4612528383731842,\n",
       "   0.09329184889793396,\n",
       "   0.052904386073350906,\n",
       "   0.06722305715084076,\n",
       "   0.10389366745948792,\n",
       "   0.04248318448662758,\n",
       "   0.013921484351158142,\n",
       "   0.08817178010940552,\n",
       "   0.016934044659137726],\n",
       "  [0.5164374113082886,\n",
       "   0.32770296931266785,\n",
       "   0.07260328531265259,\n",
       "   0.09545016288757324,\n",
       "   0.021829454228281975,\n",
       "   0.07798615097999573,\n",
       "   0.1768501102924347,\n",
       "   0.0950222834944725,\n",
       "   0.2739924490451813,\n",
       "   0.1552116423845291,\n",
       "   0.15958480536937714,\n",
       "   0.07814725488424301,\n",
       "   0.05568689480423927,\n",
       "   0.12707076966762543,\n",
       "   0.24640007317066193,\n",
       "   0.13312174379825592,\n",
       "   0.06283435970544815,\n",
       "   0.10082138329744339,\n",
       "   0.07191342115402222,\n",
       "   0.10299668461084366],\n",
       "  [0.3692246079444885,\n",
       "   0.2331671267747879,\n",
       "   0.10832652449607849,\n",
       "   0.6490727066993713,\n",
       "   0.17890718579292297,\n",
       "   0.10447122156620026,\n",
       "   0.1997603476047516,\n",
       "   0.2111579179763794,\n",
       "   0.21250805258750916,\n",
       "   0.08571881055831909,\n",
       "   0.09971225261688232,\n",
       "   0.10937321186065674,\n",
       "   0.1943518966436386,\n",
       "   0.6082820296287537,\n",
       "   0.3049955368041992,\n",
       "   0.46547573804855347,\n",
       "   0.17348825931549072,\n",
       "   0.055846910923719406,\n",
       "   0.01998557522892952,\n",
       "   0.15727777779102325],\n",
       "  [0.4236282408237457,\n",
       "   0.40778475999832153,\n",
       "   0.13684408366680145,\n",
       "   0.15707960724830627,\n",
       "   0.1024278849363327,\n",
       "   0.11329776048660278,\n",
       "   0.0306191798299551,\n",
       "   0.028494589030742645,\n",
       "   0.028281698003411293,\n",
       "   0.0320109985768795,\n",
       "   0.015371745452284813,\n",
       "   0.03549043834209442,\n",
       "   0.05686095729470253,\n",
       "   0.05265726521611214,\n",
       "   0.02480190247297287,\n",
       "   0.05298350006341934,\n",
       "   0.03510183468461037,\n",
       "   0.033688776195049286,\n",
       "   0.0030207110103219748,\n",
       "   0.017914868891239166],\n",
       "  [0.26011496782302856,\n",
       "   0.2979015111923218,\n",
       "   0.04685328155755997,\n",
       "   0.01501974742859602,\n",
       "   0.004096117336302996,\n",
       "   0.004936232231557369,\n",
       "   0.028022801503539085,\n",
       "   0.00029627999174408615,\n",
       "   0.007877380587160587,\n",
       "   0.008337503299117088,\n",
       "   0.014450683258473873,\n",
       "   0.0007004202343523502,\n",
       "   0.01793350838124752,\n",
       "   0.0005641857860609889,\n",
       "   0.0027749184519052505,\n",
       "   0.015434824861586094,\n",
       "   0.0008299257606267929,\n",
       "   0.0011835366021841764,\n",
       "   0.003016631817445159,\n",
       "   0.0001729350333334878],\n",
       "  [0.23236384987831116,\n",
       "   0.10111265629529953,\n",
       "   0.3193284869194031,\n",
       "   0.03079272247850895,\n",
       "   0.03594231605529785,\n",
       "   0.028644604608416557,\n",
       "   0.08763246983289719,\n",
       "   0.04906731843948364,\n",
       "   0.010524320416152477,\n",
       "   0.010139587335288525,\n",
       "   0.008267797529697418,\n",
       "   0.010885031893849373,\n",
       "   0.0009997892193496227,\n",
       "   0.0287700854241848,\n",
       "   0.11591240018606186,\n",
       "   0.05122595280408859,\n",
       "   0.00021799089154228568,\n",
       "   0.0007620149408467114,\n",
       "   0.0012955786660313606,\n",
       "   0.021988416090607643],\n",
       "  [0.35515451431274414,\n",
       "   0.1901537925004959,\n",
       "   0.186251699924469,\n",
       "   0.08387544751167297,\n",
       "   0.12558791041374207,\n",
       "   0.0732785165309906,\n",
       "   0.11723079532384872,\n",
       "   0.11703956127166748,\n",
       "   0.14577990770339966,\n",
       "   0.08583097904920578,\n",
       "   0.05701122805476189,\n",
       "   0.1260194629430771,\n",
       "   0.24640105664730072,\n",
       "   0.10175644606351852,\n",
       "   0.07537121325731277,\n",
       "   0.10374217480421066,\n",
       "   0.13728393614292145,\n",
       "   0.08802224695682526,\n",
       "   0.07380159944295883,\n",
       "   0.18451452255249023],\n",
       "  [0.5779807567596436,\n",
       "   0.21224017441272736,\n",
       "   0.13364291191101074,\n",
       "   0.21495850384235382,\n",
       "   0.1311839520931244,\n",
       "   1.0365852117538452,\n",
       "   0.3399200439453125,\n",
       "   0.26355862617492676,\n",
       "   0.040469806641340256,\n",
       "   0.043113671243190765,\n",
       "   0.06492389738559723,\n",
       "   0.05298081040382385,\n",
       "   0.04719407111406326,\n",
       "   0.05640050023794174,\n",
       "   0.05783892422914505,\n",
       "   0.042088501155376434,\n",
       "   0.05268646404147148,\n",
       "   0.028055598959326744,\n",
       "   0.03954514488577843,\n",
       "   0.03044539876282215],\n",
       "  [0.16115008294582367,\n",
       "   0.03987155854701996,\n",
       "   0.036674533039331436,\n",
       "   0.049162741750478745,\n",
       "   0.01910736784338951,\n",
       "   0.033260997384786606,\n",
       "   0.056008439511060715,\n",
       "   0.026716578751802444,\n",
       "   0.002628323622047901,\n",
       "   0.01194848958402872,\n",
       "   0.0017926845466718078,\n",
       "   0.008367243222892284,\n",
       "   0.011339598335325718,\n",
       "   0.00010233277134830132,\n",
       "   0.0005449867458082736,\n",
       "   0.0001153429111582227,\n",
       "   0.00048419367522001266,\n",
       "   0.00014546787133440375,\n",
       "   0.00026407174300402403,\n",
       "   7.425753574352711e-05],\n",
       "  [0.2688261866569519,\n",
       "   0.1299259066581726,\n",
       "   0.11743899434804916,\n",
       "   0.06166886165738106,\n",
       "   0.05861881375312805,\n",
       "   0.06229632720351219,\n",
       "   0.02226497232913971,\n",
       "   0.1333487629890442,\n",
       "   0.009466386400163174,\n",
       "   0.004483201075345278,\n",
       "   0.0010193666676059365,\n",
       "   0.005400415975600481,\n",
       "   0.06623812764883041,\n",
       "   0.09820986539125443,\n",
       "   0.0020669326186180115,\n",
       "   0.021355487406253815,\n",
       "   0.0007863829378038645,\n",
       "   0.14412584900856018,\n",
       "   0.0014467769069597125,\n",
       "   0.018284788355231285],\n",
       "  [0.3810936510562897,\n",
       "   0.14114801585674286,\n",
       "   0.08235719799995422,\n",
       "   0.16016918420791626,\n",
       "   0.03169470652937889,\n",
       "   0.019084764644503593,\n",
       "   0.037862133234739304,\n",
       "   0.01651516743004322,\n",
       "   0.025471219792962074,\n",
       "   0.0059481291100382805,\n",
       "   0.0006941051105968654,\n",
       "   0.0042657675221562386,\n",
       "   0.0020617074333131313,\n",
       "   0.017410069704055786,\n",
       "   0.01803731545805931,\n",
       "   0.010120715945959091,\n",
       "   0.0060892291367053986,\n",
       "   0.0015663343947380781,\n",
       "   0.00043070942047052085,\n",
       "   0.002565037924796343],\n",
       "  [0.30525580048561096,\n",
       "   0.25864994525909424,\n",
       "   0.16112685203552246,\n",
       "   0.22571350634098053,\n",
       "   0.37163594365119934,\n",
       "   0.378088116645813,\n",
       "   0.31458088755607605,\n",
       "   0.15211789309978485,\n",
       "   0.053835440427064896,\n",
       "   0.13019835948944092,\n",
       "   0.04516468569636345,\n",
       "   0.10559628158807755,\n",
       "   0.7911150455474854,\n",
       "   0.935400664806366,\n",
       "   0.36080843210220337,\n",
       "   0.1220492273569107,\n",
       "   0.08004161715507507,\n",
       "   0.09724907577037811,\n",
       "   0.03726127743721008,\n",
       "   0.20442374050617218],\n",
       "  [0.24361543357372284,\n",
       "   0.15993037819862366,\n",
       "   0.4041917324066162,\n",
       "   0.15744034945964813,\n",
       "   0.0724433958530426,\n",
       "   0.029060907661914825,\n",
       "   0.0046933311969041824,\n",
       "   0.03973715379834175,\n",
       "   0.0363382063806057,\n",
       "   0.01885017193853855,\n",
       "   0.007317616604268551,\n",
       "   0.0030153407715260983,\n",
       "   0.0208849236369133,\n",
       "   0.011019386351108551,\n",
       "   0.0047346861101686954,\n",
       "   0.006906676106154919,\n",
       "   0.004889501724392176,\n",
       "   0.015731649473309517,\n",
       "   0.02978748083114624,\n",
       "   0.007965118624269962],\n",
       "  [0.5263828039169312,\n",
       "   0.5440629124641418,\n",
       "   0.18293482065200806,\n",
       "   0.32307934761047363,\n",
       "   0.18083131313323975,\n",
       "   0.1353222280740738,\n",
       "   0.2526031732559204,\n",
       "   0.2831375002861023,\n",
       "   0.26971524953842163,\n",
       "   0.6657832860946655,\n",
       "   0.1799330711364746,\n",
       "   0.40876415371894836,\n",
       "   0.31434622406959534,\n",
       "   0.20730888843536377,\n",
       "   0.11265195906162262,\n",
       "   0.4413588047027588,\n",
       "   0.12820938229560852,\n",
       "   0.071455217897892,\n",
       "   0.04303055629134178,\n",
       "   0.00912685040384531],\n",
       "  [0.4194624722003937,\n",
       "   0.205200657248497,\n",
       "   0.4141816794872284,\n",
       "   0.07513287663459778,\n",
       "   0.024842236191034317,\n",
       "   0.01860032044351101,\n",
       "   0.013214212842285633,\n",
       "   0.007240072824060917,\n",
       "   0.026917774230241776,\n",
       "   0.03757292032241821,\n",
       "   0.0009064754121936858,\n",
       "   0.01729896292090416,\n",
       "   0.003236170159652829,\n",
       "   0.005694563500583172,\n",
       "   0.013325986452400684,\n",
       "   0.015784194692969322,\n",
       "   0.004939560312777758,\n",
       "   0.020623736083507538,\n",
       "   0.012786795385181904,\n",
       "   0.007576553616672754],\n",
       "  [0.2856031060218811,\n",
       "   0.08571784198284149,\n",
       "   0.07351775467395782,\n",
       "   0.041144587099552155,\n",
       "   0.031176291406154633,\n",
       "   0.0372040756046772,\n",
       "   0.03733624145388603,\n",
       "   0.011092696338891983,\n",
       "   0.003370804712176323,\n",
       "   0.0075769745744764805,\n",
       "   0.0018321606330573559,\n",
       "   0.002564767375588417,\n",
       "   0.015353509224951267,\n",
       "   0.0012320216046646237,\n",
       "   0.015602169558405876,\n",
       "   0.009632743895053864,\n",
       "   0.0017871957970783114,\n",
       "   0.004830559715628624,\n",
       "   0.0018558138981461525,\n",
       "   0.0001385713112540543],\n",
       "  [0.6905349493026733,\n",
       "   0.3916172385215759,\n",
       "   1.3097844123840332,\n",
       "   0.06421834230422974,\n",
       "   0.021556761115789413,\n",
       "   0.006149319466203451,\n",
       "   0.0027900824788957834,\n",
       "   0.001340324291959405,\n",
       "   0.005090175196528435,\n",
       "   0.014863106422126293,\n",
       "   0.004220209084451199,\n",
       "   0.0022231212351471186,\n",
       "   0.00838956143707037,\n",
       "   0.0006068118382245302,\n",
       "   0.003743330715224147,\n",
       "   0.0030060717836022377,\n",
       "   0.0030856223311275244,\n",
       "   1.5750247257528827e-05,\n",
       "   0.000281032727798447,\n",
       "   0.0015557289589196444],\n",
       "  [0.7526105642318726,\n",
       "   0.38534602522850037,\n",
       "   0.06612709164619446,\n",
       "   0.08898486942052841,\n",
       "   0.08225557208061218,\n",
       "   0.020360996946692467,\n",
       "   0.02797793038189411,\n",
       "   0.15472416579723358,\n",
       "   0.002428501844406128,\n",
       "   0.00824323296546936,\n",
       "   0.009158487431704998,\n",
       "   0.010404317639768124,\n",
       "   0.008626195602118969,\n",
       "   0.06696198135614395,\n",
       "   0.0031891532707959414,\n",
       "   0.0664658471941948,\n",
       "   0.004809555131942034,\n",
       "   0.029273808002471924,\n",
       "   0.0008857602369971573,\n",
       "   0.009399406611919403],\n",
       "  [0.3065851628780365,\n",
       "   0.15536965429782867,\n",
       "   0.21682262420654297,\n",
       "   0.07033636420965195,\n",
       "   0.02806992270052433,\n",
       "   0.12518736720085144,\n",
       "   0.06765970587730408,\n",
       "   0.08432794362306595,\n",
       "   0.023608146235346794,\n",
       "   0.022109709680080414,\n",
       "   0.006478046532720327,\n",
       "   0.06756960600614548,\n",
       "   0.017468474805355072,\n",
       "   0.1751638650894165,\n",
       "   0.048873014748096466,\n",
       "   0.09613727778196335,\n",
       "   0.06666684150695801,\n",
       "   0.03812015429139137,\n",
       "   0.06269242614507675,\n",
       "   0.017615865916013718],\n",
       "  [0.45435193181037903,\n",
       "   0.14158780872821808,\n",
       "   0.09523124992847443,\n",
       "   0.184144526720047,\n",
       "   0.07897168397903442,\n",
       "   0.04389747604727745,\n",
       "   0.06613680720329285,\n",
       "   0.06912966817617416,\n",
       "   0.0541745126247406,\n",
       "   2.691189765930176,\n",
       "   0.0795028805732727,\n",
       "   0.12280145287513733,\n",
       "   0.04564708471298218,\n",
       "   0.41813352704048157,\n",
       "   0.7858799695968628,\n",
       "   0.07727497071027756,\n",
       "   0.055128175765275955,\n",
       "   0.0590265616774559,\n",
       "   0.04447029158473015,\n",
       "   0.040778446942567825],\n",
       "  [0.39201077818870544,\n",
       "   0.14897382259368896,\n",
       "   0.07617772370576859,\n",
       "   0.13929536938667297,\n",
       "   0.10197950899600983,\n",
       "   0.0254632830619812,\n",
       "   0.0737200379371643,\n",
       "   0.09602625668048859,\n",
       "   0.060882434248924255,\n",
       "   0.119380421936512,\n",
       "   0.03909219801425934,\n",
       "   0.1292528361082077,\n",
       "   0.04074233025312424,\n",
       "   0.04288996383547783,\n",
       "   0.09535321593284607,\n",
       "   0.01495475322008133,\n",
       "   0.03245405852794647,\n",
       "   0.040910858660936356,\n",
       "   0.03185112774372101,\n",
       "   0.057716868817806244],\n",
       "  [0.3100414276123047,\n",
       "   0.45257943868637085,\n",
       "   0.22856563329696655,\n",
       "   0.03836200013756752,\n",
       "   0.012090609408915043,\n",
       "   0.05811082944273949,\n",
       "   0.05425887927412987,\n",
       "   0.06767600774765015,\n",
       "   0.004044928587973118,\n",
       "   0.14820542931556702,\n",
       "   0.11608704179525375,\n",
       "   0.29019343852996826,\n",
       "   0.07983342558145523,\n",
       "   0.011096830479800701,\n",
       "   0.08117081224918365,\n",
       "   0.05324651673436165,\n",
       "   0.05947970598936081,\n",
       "   0.010846971534192562,\n",
       "   0.02844810113310814,\n",
       "   0.016988055780529976],\n",
       "  [0.43556466698646545,\n",
       "   0.20829378068447113,\n",
       "   0.10081032663583755,\n",
       "   0.10059716552495956,\n",
       "   0.04698830470442772,\n",
       "   0.07473260909318924,\n",
       "   0.09998857229948044,\n",
       "   0.14530612528324127,\n",
       "   0.19889208674430847,\n",
       "   0.030431916937232018,\n",
       "   0.11459550261497498,\n",
       "   0.16337339580059052,\n",
       "   0.1570505052804947,\n",
       "   0.05320519953966141,\n",
       "   0.10150009393692017,\n",
       "   0.061067745089530945,\n",
       "   0.04260187968611717,\n",
       "   0.06196805089712143,\n",
       "   0.10657148063182831,\n",
       "   0.07385877519845963],\n",
       "  [0.26557862758636475,\n",
       "   0.20923306047916412,\n",
       "   0.07589105516672134,\n",
       "   0.10572996735572815,\n",
       "   0.09048321843147278,\n",
       "   0.08792697638273239,\n",
       "   0.03255783021450043,\n",
       "   0.09807227551937103,\n",
       "   0.1166318729519844,\n",
       "   0.03395315632224083,\n",
       "   0.036955006420612335,\n",
       "   0.03924077749252319,\n",
       "   0.15868917107582092,\n",
       "   1.367211937904358,\n",
       "   0.09984118491411209,\n",
       "   0.05842912569642067,\n",
       "   0.032789744436740875,\n",
       "   0.019183311611413956,\n",
       "   0.013672219589352608,\n",
       "   0.022988704964518547],\n",
       "  [0.3993310332298279,\n",
       "   0.28344693779945374,\n",
       "   0.17511767148971558,\n",
       "   0.22807972133159637,\n",
       "   0.39788609743118286,\n",
       "   0.36666709184646606,\n",
       "   0.26015037298202515,\n",
       "   0.20925864577293396,\n",
       "   0.10326586663722992,\n",
       "   0.3152414858341217,\n",
       "   0.09249303489923477,\n",
       "   0.13752248883247375,\n",
       "   0.10321734100580215,\n",
       "   0.12112992256879807,\n",
       "   0.2917923927307129,\n",
       "   0.07179528474807739,\n",
       "   0.10131058096885681,\n",
       "   0.1259000152349472,\n",
       "   0.15402455627918243,\n",
       "   0.0410878024995327],\n",
       "  [0.15967245399951935,\n",
       "   0.05675768107175827,\n",
       "   0.5823725461959839,\n",
       "   0.023605888709425926,\n",
       "   0.004141775891184807,\n",
       "   0.008331315591931343,\n",
       "   0.0036666246596723795,\n",
       "   0.0018995602149516344,\n",
       "   0.0013077050680294633,\n",
       "   0.001853365101851523,\n",
       "   0.0007208649185486138,\n",
       "   0.024377696216106415,\n",
       "   0.0018751388415694237,\n",
       "   0.010891009122133255,\n",
       "   0.000763208488933742,\n",
       "   0.001511316979303956,\n",
       "   0.0030394962523132563,\n",
       "   0.01899724267423153,\n",
       "   0.002860837150365114,\n",
       "   0.0007321216980926692],\n",
       "  [0.5171385407447815,\n",
       "   0.11824368685483932,\n",
       "   0.5140753388404846,\n",
       "   0.14659029245376587,\n",
       "   0.11211692541837692,\n",
       "   0.06360109895467758,\n",
       "   0.007152274250984192,\n",
       "   0.015599098056554794,\n",
       "   0.08361158519983292,\n",
       "   0.007383397314697504,\n",
       "   0.003841572441160679,\n",
       "   0.01219366118311882,\n",
       "   0.0009477434796281159,\n",
       "   0.557786762714386,\n",
       "   0.001572505454532802,\n",
       "   0.013364482671022415,\n",
       "   0.002259837230667472,\n",
       "   0.009386147372424603,\n",
       "   0.0475575253367424,\n",
       "   0.0004883183282800019],\n",
       "  [0.43031683564186096,\n",
       "   0.11330174654722214,\n",
       "   0.07929478585720062,\n",
       "   0.050831981003284454,\n",
       "   0.02313794568181038,\n",
       "   0.06083543971180916,\n",
       "   0.027158275246620178,\n",
       "   0.01741757057607174,\n",
       "   0.016310447826981544,\n",
       "   0.015930285677313805,\n",
       "   0.070247583091259,\n",
       "   0.04995054006576538,\n",
       "   0.008756620809435844,\n",
       "   0.021395010873675346,\n",
       "   0.014648833312094212,\n",
       "   0.004779281094670296,\n",
       "   0.001891960040666163,\n",
       "   0.008574091829359531,\n",
       "   0.01542872004210949,\n",
       "   0.01738707162439823],\n",
       "  [0.3289172351360321,\n",
       "   0.14393259584903717,\n",
       "   0.3537968695163727,\n",
       "   0.11123312264680862,\n",
       "   0.08252143859863281,\n",
       "   0.1443820595741272,\n",
       "   0.05423298850655556,\n",
       "   0.03832046315073967,\n",
       "   0.021481458097696304,\n",
       "   0.043619006872177124,\n",
       "   0.011714056134223938,\n",
       "   0.004301624838262796,\n",
       "   0.03726769983768463,\n",
       "   0.10307469218969345,\n",
       "   0.0003678668581414968,\n",
       "   0.002980160992592573,\n",
       "   0.12415723502635956,\n",
       "   0.0035696360282599926,\n",
       "   0.01815648190677166,\n",
       "   0.0060753547586500645]],\n",
       " [[0.42932644486427307,\n",
       "   0.28184595704078674,\n",
       "   0.1594974547624588,\n",
       "   0.10142455250024796,\n",
       "   0.06762108951807022,\n",
       "   0.06207400932908058,\n",
       "   0.041553184390068054,\n",
       "   0.10955234616994858,\n",
       "   0.1894669532775879,\n",
       "   0.23029175400733948,\n",
       "   0.15621677041053772,\n",
       "   0.12321005016565323,\n",
       "   0.10643462091684341,\n",
       "   0.10583780705928802,\n",
       "   0.08731242269277573,\n",
       "   0.055766649544239044,\n",
       "   0.050125159323215485,\n",
       "   0.04798401892185211,\n",
       "   0.042558301240205765,\n",
       "   0.04175643250346184],\n",
       "  [0.4346206486225128,\n",
       "   0.32720252871513367,\n",
       "   0.1816050261259079,\n",
       "   0.4063373804092407,\n",
       "   0.24957157671451569,\n",
       "   0.16567611694335938,\n",
       "   0.12690192461013794,\n",
       "   0.10984976589679718,\n",
       "   0.08453191816806793,\n",
       "   0.06969083100557327,\n",
       "   0.05761975049972534,\n",
       "   0.053739842027425766,\n",
       "   0.046320777386426926,\n",
       "   0.04905514419078827,\n",
       "   0.05234205350279808,\n",
       "   0.05984848365187645,\n",
       "   0.03928617388010025,\n",
       "   0.03040199726819992,\n",
       "   0.02646595798432827,\n",
       "   0.031728778034448624],\n",
       "  [0.42104288935661316,\n",
       "   0.3100033402442932,\n",
       "   0.12998881936073303,\n",
       "   0.05796045809984207,\n",
       "   0.04285290837287903,\n",
       "   0.036670539528131485,\n",
       "   0.02381647564470768,\n",
       "   0.02269725129008293,\n",
       "   0.07185971736907959,\n",
       "   0.049890704452991486,\n",
       "   0.03132258355617523,\n",
       "   0.03396552428603172,\n",
       "   0.04369831085205078,\n",
       "   0.017558874562382698,\n",
       "   0.023573828861117363,\n",
       "   0.00833225529640913,\n",
       "   0.005446040071547031,\n",
       "   0.005288615357130766,\n",
       "   0.004839784931391478,\n",
       "   0.005571126937866211],\n",
       "  [0.4292435348033905,\n",
       "   0.3078363537788391,\n",
       "   0.16387246549129486,\n",
       "   0.09828951209783554,\n",
       "   0.12200445681810379,\n",
       "   0.08901872485876083,\n",
       "   0.05006186664104462,\n",
       "   0.0712740421295166,\n",
       "   0.0478275902569294,\n",
       "   0.028463104739785194,\n",
       "   0.02772216685116291,\n",
       "   0.02682115137577057,\n",
       "   0.023594243451952934,\n",
       "   0.014765343628823757,\n",
       "   0.022785678505897522,\n",
       "   0.01520712859928608,\n",
       "   0.02037775330245495,\n",
       "   0.018865959718823433,\n",
       "   0.007700461894273758,\n",
       "   0.009282462298870087],\n",
       "  [0.42467138171195984,\n",
       "   0.28740864992141724,\n",
       "   0.17878611385822296,\n",
       "   0.14071539044380188,\n",
       "   0.0733337476849556,\n",
       "   0.053625721484422684,\n",
       "   0.05904041975736618,\n",
       "   0.0675249844789505,\n",
       "   0.027040377259254456,\n",
       "   0.042906228452920914,\n",
       "   0.01751905493438244,\n",
       "   0.02030593529343605,\n",
       "   0.015224403701722622,\n",
       "   0.024969657883048058,\n",
       "   0.009470340795814991,\n",
       "   0.014360439963638783,\n",
       "   0.01152324490249157,\n",
       "   0.011512389406561852,\n",
       "   0.012202625162899494,\n",
       "   0.012646760791540146],\n",
       "  [0.4299149811267853,\n",
       "   0.32713258266448975,\n",
       "   0.16694118082523346,\n",
       "   0.05810639262199402,\n",
       "   0.03882661461830139,\n",
       "   0.04095514491200447,\n",
       "   0.027720756828784943,\n",
       "   0.025609634816646576,\n",
       "   0.017109237611293793,\n",
       "   0.015424837358295918,\n",
       "   0.014252030290663242,\n",
       "   0.011721995659172535,\n",
       "   0.016844989731907845,\n",
       "   0.008916131220757961,\n",
       "   0.011030040681362152,\n",
       "   0.010445390827953815,\n",
       "   0.010556705296039581,\n",
       "   0.007649475708603859,\n",
       "   0.002618964994326234,\n",
       "   0.0011622300371527672],\n",
       "  [0.4312969446182251,\n",
       "   0.28154534101486206,\n",
       "   0.13274259865283966,\n",
       "   0.0809558629989624,\n",
       "   0.058120790868997574,\n",
       "   0.05670316144824028,\n",
       "   0.038945749402046204,\n",
       "   0.03080078959465027,\n",
       "   0.023199403658509254,\n",
       "   0.023144224658608437,\n",
       "   0.01712602749466896,\n",
       "   0.018971392884850502,\n",
       "   0.018427083268761635,\n",
       "   0.01815260946750641,\n",
       "   0.013893402181565762,\n",
       "   0.017278235405683517,\n",
       "   0.005984561983495951,\n",
       "   0.008787708356976509,\n",
       "   0.02059275470674038,\n",
       "   0.008286813274025917],\n",
       "  [0.4327426552772522,\n",
       "   0.2975441515445709,\n",
       "   0.1642686277627945,\n",
       "   0.09094619750976562,\n",
       "   0.07864242792129517,\n",
       "   0.05953030660748482,\n",
       "   0.044241033494472504,\n",
       "   0.04039885848760605,\n",
       "   0.028616415336728096,\n",
       "   0.04891021549701691,\n",
       "   0.03277992457151413,\n",
       "   0.019481338560581207,\n",
       "   0.012810518965125084,\n",
       "   0.02246715873479843,\n",
       "   0.017249802127480507,\n",
       "   0.013974918983876705,\n",
       "   0.011380445212125778,\n",
       "   0.013573317788541317,\n",
       "   0.014667825773358345,\n",
       "   0.007895913906395435],\n",
       "  [0.4185899794101715,\n",
       "   0.28669941425323486,\n",
       "   0.14532823860645294,\n",
       "   0.0901874527335167,\n",
       "   0.24693098664283752,\n",
       "   0.19031496345996857,\n",
       "   0.1157739907503128,\n",
       "   0.08689792454242706,\n",
       "   0.061720144003629684,\n",
       "   0.05232460796833038,\n",
       "   0.04138163849711418,\n",
       "   0.04021202400326729,\n",
       "   0.037121593952178955,\n",
       "   0.030329953879117966,\n",
       "   0.02888510748744011,\n",
       "   0.025864532217383385,\n",
       "   0.024487199261784554,\n",
       "   0.02650182507932186,\n",
       "   0.02346113510429859,\n",
       "   0.018146736547350883],\n",
       "  [0.41626444458961487,\n",
       "   0.26992547512054443,\n",
       "   0.15013647079467773,\n",
       "   0.10822682082653046,\n",
       "   0.0649934634566307,\n",
       "   0.04927019029855728,\n",
       "   0.03640531003475189,\n",
       "   0.03446994721889496,\n",
       "   0.024824276566505432,\n",
       "   0.021645167842507362,\n",
       "   0.021528983488678932,\n",
       "   0.012560638599097729,\n",
       "   0.013919871300458908,\n",
       "   0.013548455201089382,\n",
       "   0.010508066974580288,\n",
       "   0.007835030555725098,\n",
       "   0.017057089135050774,\n",
       "   0.01641879975795746,\n",
       "   0.004705358762294054,\n",
       "   0.005133630242198706],\n",
       "  [0.4355370104312897,\n",
       "   0.29682400822639465,\n",
       "   0.16840389370918274,\n",
       "   0.09555140137672424,\n",
       "   0.07495094835758209,\n",
       "   0.060406412929296494,\n",
       "   0.05201351270079613,\n",
       "   0.04374510422348976,\n",
       "   0.034615181386470795,\n",
       "   0.027885371819138527,\n",
       "   0.02552006207406521,\n",
       "   0.025191504508256912,\n",
       "   0.01618245616555214,\n",
       "   0.02388663962483406,\n",
       "   0.017095983028411865,\n",
       "   0.010609366931021214,\n",
       "   0.021337896585464478,\n",
       "   0.0057640643790364265,\n",
       "   0.016052672639489174,\n",
       "   0.00442874850705266],\n",
       "  [0.4258424639701843,\n",
       "   0.2993447184562683,\n",
       "   0.16748084127902985,\n",
       "   0.14225853979587555,\n",
       "   0.10049677640199661,\n",
       "   0.06916981935501099,\n",
       "   0.051331546157598495,\n",
       "   0.046043895184993744,\n",
       "   0.03822561725974083,\n",
       "   0.03868252784013748,\n",
       "   0.026277752593159676,\n",
       "   0.01822551339864731,\n",
       "   0.023084355518221855,\n",
       "   0.017659995704889297,\n",
       "   0.016299836337566376,\n",
       "   0.015996862202882767,\n",
       "   0.01469851192086935,\n",
       "   0.017524749040603638,\n",
       "   0.012609749101102352,\n",
       "   0.01479500625282526],\n",
       "  [0.4351721405982971,\n",
       "   0.33655327558517456,\n",
       "   0.18198160827159882,\n",
       "   0.11380689591169357,\n",
       "   0.23108479380607605,\n",
       "   0.29965144395828247,\n",
       "   0.23181433975696564,\n",
       "   0.1729387491941452,\n",
       "   0.1513855755329132,\n",
       "   0.0715922936797142,\n",
       "   0.052285656332969666,\n",
       "   0.042842864990234375,\n",
       "   0.03529690206050873,\n",
       "   0.11605328321456909,\n",
       "   0.2991237938404083,\n",
       "   0.12269122153520584,\n",
       "   0.05813218653202057,\n",
       "   0.03619343042373657,\n",
       "   0.026715772226452827,\n",
       "   0.024866078048944473],\n",
       "  [0.4181974232196808,\n",
       "   0.25856322050094604,\n",
       "   0.14608025550842285,\n",
       "   0.09318133443593979,\n",
       "   0.06648702174425125,\n",
       "   0.04766104742884636,\n",
       "   0.04677857086062431,\n",
       "   0.050068292766809464,\n",
       "   0.034777771681547165,\n",
       "   0.022432371973991394,\n",
       "   0.02817714586853981,\n",
       "   0.022098151966929436,\n",
       "   0.01362977921962738,\n",
       "   0.013889642432332039,\n",
       "   0.017713339999318123,\n",
       "   0.014018572866916656,\n",
       "   0.00902275275439024,\n",
       "   0.012678860686719418,\n",
       "   0.006005328614264727,\n",
       "   0.010206042788922787],\n",
       "  [0.4236167371273041,\n",
       "   0.2907877564430237,\n",
       "   0.15317288041114807,\n",
       "   0.3277623653411865,\n",
       "   0.17476193606853485,\n",
       "   0.087715744972229,\n",
       "   0.10443156212568283,\n",
       "   0.07193110883235931,\n",
       "   0.049709513783454895,\n",
       "   0.04299820959568024,\n",
       "   0.033853452652692795,\n",
       "   0.026018483564257622,\n",
       "   0.028871530666947365,\n",
       "   0.025612469762563705,\n",
       "   0.027398787438869476,\n",
       "   0.017675267532467842,\n",
       "   0.021727530285716057,\n",
       "   0.02006526291370392,\n",
       "   0.016719834879040718,\n",
       "   0.014982220716774464],\n",
       "  [0.4167293608188629,\n",
       "   0.2758355140686035,\n",
       "   0.20022368431091309,\n",
       "   0.09013097733259201,\n",
       "   0.05389251932501793,\n",
       "   0.03730575740337372,\n",
       "   0.026102786883711815,\n",
       "   0.021728700026869774,\n",
       "   0.022626269608736038,\n",
       "   0.015424375422298908,\n",
       "   0.014010708779096603,\n",
       "   0.01644948497414589,\n",
       "   0.010294009000062943,\n",
       "   0.0130690922960639,\n",
       "   0.018332766368985176,\n",
       "   0.007911846973001957,\n",
       "   0.010372120887041092,\n",
       "   0.010494082234799862,\n",
       "   0.008497670292854309,\n",
       "   0.010807818733155727],\n",
       "  [0.4257601499557495,\n",
       "   0.2847493886947632,\n",
       "   0.1536288857460022,\n",
       "   0.11966704577207565,\n",
       "   0.09451083093881607,\n",
       "   0.059553906321525574,\n",
       "   0.05785322189331055,\n",
       "   0.043300747871398926,\n",
       "   0.03324273228645325,\n",
       "   0.0270844716578722,\n",
       "   0.02239249274134636,\n",
       "   0.021492647007107735,\n",
       "   0.02711343765258789,\n",
       "   0.011039501056075096,\n",
       "   0.019205011427402496,\n",
       "   0.010230102576315403,\n",
       "   0.009800146333873272,\n",
       "   0.0164449792355299,\n",
       "   0.01582300290465355,\n",
       "   0.006714722141623497],\n",
       "  [0.42396408319473267,\n",
       "   0.30475571751594543,\n",
       "   0.19367271661758423,\n",
       "   0.11580460518598557,\n",
       "   0.04011289030313492,\n",
       "   0.036283716559410095,\n",
       "   0.02438809722661972,\n",
       "   0.02209720015525818,\n",
       "   0.01866551674902439,\n",
       "   0.02107907645404339,\n",
       "   0.015623598359525204,\n",
       "   0.011259838007390499,\n",
       "   0.016773873940110207,\n",
       "   0.005311576649546623,\n",
       "   0.018103227019309998,\n",
       "   0.005412673112004995,\n",
       "   0.014217756688594818,\n",
       "   0.005812130868434906,\n",
       "   0.0057906173169612885,\n",
       "   0.007004161365330219],\n",
       "  [0.4258155822753906,\n",
       "   0.2989101707935333,\n",
       "   0.19770658016204834,\n",
       "   0.06758458912372589,\n",
       "   0.03529682010412216,\n",
       "   0.0301272701472044,\n",
       "   0.022830357775092125,\n",
       "   0.02496453933417797,\n",
       "   0.020182562991976738,\n",
       "   0.02036060020327568,\n",
       "   0.012908274307847023,\n",
       "   0.012996941804885864,\n",
       "   0.012253751046955585,\n",
       "   0.012039819732308388,\n",
       "   0.01581983081996441,\n",
       "   0.0065926662646234035,\n",
       "   0.0156979039311409,\n",
       "   0.004111087415367365,\n",
       "   0.00979902409017086,\n",
       "   0.004640963394194841],\n",
       "  [0.42720121145248413,\n",
       "   0.2904837727546692,\n",
       "   0.17659762501716614,\n",
       "   0.11087989807128906,\n",
       "   0.07489693909883499,\n",
       "   0.10622894763946533,\n",
       "   0.05290919542312622,\n",
       "   0.043683432042598724,\n",
       "   0.03637981042265892,\n",
       "   0.04272158816456795,\n",
       "   0.021074343472719193,\n",
       "   0.015189801342785358,\n",
       "   0.021901367232203484,\n",
       "   0.02691066451370716,\n",
       "   0.012611146084964275,\n",
       "   0.018352698534727097,\n",
       "   0.008427432738244534,\n",
       "   0.008661464788019657,\n",
       "   0.011863717809319496,\n",
       "   0.009397204034030437],\n",
       "  [0.4184839427471161,\n",
       "   0.30955079197883606,\n",
       "   0.17076095938682556,\n",
       "   0.14665862917900085,\n",
       "   0.09620898216962814,\n",
       "   0.07004442065954208,\n",
       "   0.04155835509300232,\n",
       "   0.038410305976867676,\n",
       "   0.030927132815122604,\n",
       "   0.06328489631414413,\n",
       "   0.2153317928314209,\n",
       "   0.13275685906410217,\n",
       "   0.0940629243850708,\n",
       "   0.30969691276550293,\n",
       "   0.21307949721813202,\n",
       "   0.19052523374557495,\n",
       "   0.11435027420520782,\n",
       "   0.09431799501180649,\n",
       "   0.0790250301361084,\n",
       "   0.06779646128416061],\n",
       "  [0.43160998821258545,\n",
       "   0.233152374625206,\n",
       "   0.07245944440364838,\n",
       "   0.04980642721056938,\n",
       "   0.03838551789522171,\n",
       "   0.032317694276571274,\n",
       "   0.021646371111273766,\n",
       "   0.021820854395627975,\n",
       "   0.016857238486409187,\n",
       "   0.013560470193624496,\n",
       "   0.017412584275007248,\n",
       "   0.01464860513806343,\n",
       "   0.011030678637325764,\n",
       "   0.010392863303422928,\n",
       "   0.01064284797757864,\n",
       "   0.011473247781395912,\n",
       "   0.007120822556316853,\n",
       "   0.010448822751641273,\n",
       "   0.008413602598011494,\n",
       "   0.007952938787639141],\n",
       "  [0.43443918228149414,\n",
       "   0.3221513628959656,\n",
       "   0.20401017367839813,\n",
       "   0.12146003544330597,\n",
       "   0.07642713934183121,\n",
       "   0.05718272179365158,\n",
       "   0.04280729591846466,\n",
       "   0.046265341341495514,\n",
       "   0.034130312502384186,\n",
       "   0.239204540848732,\n",
       "   0.17279599606990814,\n",
       "   0.08692222833633423,\n",
       "   0.057370077818632126,\n",
       "   0.04533081501722336,\n",
       "   0.04012468457221985,\n",
       "   0.031047046184539795,\n",
       "   0.03784513473510742,\n",
       "   0.028543658554553986,\n",
       "   0.02587653510272503,\n",
       "   0.012324732728302479],\n",
       "  [0.43100953102111816,\n",
       "   0.31049564480781555,\n",
       "   0.19535182416439056,\n",
       "   0.08732599020004272,\n",
       "   0.044962551444768906,\n",
       "   0.026930157095193863,\n",
       "   0.026581114158034325,\n",
       "   0.020093224942684174,\n",
       "   0.01788470894098282,\n",
       "   0.016286011785268784,\n",
       "   0.010143648833036423,\n",
       "   0.009786234237253666,\n",
       "   0.014885397627949715,\n",
       "   0.008219565264880657,\n",
       "   0.011092005297541618,\n",
       "   0.005239670630544424,\n",
       "   0.01182788610458374,\n",
       "   0.0071571702137589455,\n",
       "   0.0029711145907640457,\n",
       "   0.010450431145727634],\n",
       "  [0.4357636868953705,\n",
       "   0.2923737168312073,\n",
       "   0.1640302836894989,\n",
       "   0.2848803400993347,\n",
       "   0.14090247452259064,\n",
       "   0.08283224701881409,\n",
       "   0.04979851841926575,\n",
       "   0.04671642556786537,\n",
       "   0.03409501165151596,\n",
       "   0.030808184295892715,\n",
       "   0.028488321229815483,\n",
       "   0.026173768565058708,\n",
       "   0.02010977640748024,\n",
       "   0.1585153043270111,\n",
       "   0.26190894842147827,\n",
       "   0.13107667863368988,\n",
       "   0.09198322892189026,\n",
       "   0.05831301957368851,\n",
       "   0.05149957910180092,\n",
       "   0.04527688026428223],\n",
       "  [0.42901962995529175,\n",
       "   0.3039717376232147,\n",
       "   0.16429495811462402,\n",
       "   0.11314529180526733,\n",
       "   0.20468465983867645,\n",
       "   0.3295075595378876,\n",
       "   0.2205478549003601,\n",
       "   0.12621255218982697,\n",
       "   0.08820294588804245,\n",
       "   0.07766053080558777,\n",
       "   0.06204473599791527,\n",
       "   0.05429704487323761,\n",
       "   0.049722474068403244,\n",
       "   0.04029475525021553,\n",
       "   0.038493603467941284,\n",
       "   0.03225120157003403,\n",
       "   0.031499557197093964,\n",
       "   0.03308181092143059,\n",
       "   0.025645073503255844,\n",
       "   0.01968258246779442],\n",
       "  [0.4154455065727234,\n",
       "   0.2942795753479004,\n",
       "   0.20565657317638397,\n",
       "   0.14778563380241394,\n",
       "   0.05412258207798004,\n",
       "   0.04017486050724983,\n",
       "   0.035921793431043625,\n",
       "   0.022416749969124794,\n",
       "   0.021233415231108665,\n",
       "   0.021706532686948776,\n",
       "   0.019722260534763336,\n",
       "   0.016506308689713478,\n",
       "   0.009951743297278881,\n",
       "   0.010498995892703533,\n",
       "   0.01311936043202877,\n",
       "   0.008125466294586658,\n",
       "   0.01083143800497055,\n",
       "   0.00858813151717186,\n",
       "   0.008721651509404182,\n",
       "   0.009423000738024712],\n",
       "  [0.4233890473842621,\n",
       "   0.2994210422039032,\n",
       "   0.19240079820156097,\n",
       "   0.22464503347873688,\n",
       "   0.13609229028224945,\n",
       "   0.14136376976966858,\n",
       "   0.07676120102405548,\n",
       "   0.052882254123687744,\n",
       "   0.037063322961330414,\n",
       "   0.03174927458167076,\n",
       "   0.028057675808668137,\n",
       "   0.0225844569504261,\n",
       "   0.017454849556088448,\n",
       "   0.022808076813817024,\n",
       "   0.021535325795412064,\n",
       "   0.017156632617115974,\n",
       "   0.014098266139626503,\n",
       "   0.014847351238131523,\n",
       "   0.012557300738990307,\n",
       "   0.013416465371847153],\n",
       "  [0.44685816764831543,\n",
       "   0.3073453903198242,\n",
       "   0.17526453733444214,\n",
       "   0.11882098019123077,\n",
       "   0.08100106567144394,\n",
       "   0.05750667676329613,\n",
       "   0.05341443419456482,\n",
       "   0.035605933517217636,\n",
       "   0.03346240893006325,\n",
       "   0.02266579307615757,\n",
       "   0.026397205889225006,\n",
       "   0.022369781509041786,\n",
       "   0.05278710275888443,\n",
       "   0.014292010106146336,\n",
       "   0.014878155663609505,\n",
       "   0.014277346432209015,\n",
       "   0.009028350003063679,\n",
       "   0.00941123440861702,\n",
       "   0.013919414021074772,\n",
       "   0.016318857669830322],\n",
       "  [0.42260468006134033,\n",
       "   0.2807920277118683,\n",
       "   0.1635662317276001,\n",
       "   0.10381346940994263,\n",
       "   0.08470691740512848,\n",
       "   0.07445427775382996,\n",
       "   0.052508991211652756,\n",
       "   0.042871467769145966,\n",
       "   0.035132117569446564,\n",
       "   0.030944818630814552,\n",
       "   0.024030348286032677,\n",
       "   0.024843841791152954,\n",
       "   0.01693703792989254,\n",
       "   0.025626732036471367,\n",
       "   0.02056989073753357,\n",
       "   0.009236407466232777,\n",
       "   0.008771324530243874,\n",
       "   0.026908408850431442,\n",
       "   0.009297563694417477,\n",
       "   0.012598530389368534]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_acc_per_fold, acc_per_fold, val_loss_per_fold, loss_per_fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "results = {\"val_acc_per_fold\": val_acc_per_fold, \"acc_per_fold\": acc_per_fold, \"val_loss_per_fold\": val_loss_per_fold, \"loss_per_fold\": loss_per_fold}\n",
    "\n",
    "with open('results_model_rnn_leave_one_out.pickle', 'wb') as file:\n",
    " pickle.dump(results, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9895833134651184\n",
      "0.994854211807251\n",
      "0.9769737124443054\n",
      "0.9421221613883972\n",
      "0.9966443181037903\n",
      "1.0\n",
      "0.9909090995788574\n",
      "0.9722921848297119\n",
      "0.990338146686554\n",
      "1.0\n",
      "0.9948979616165161\n",
      "1.0\n",
      "0.9204244017601013\n",
      "0.9973118305206299\n",
      "0.9974683523178101\n",
      "0.9935897588729858\n",
      "1.0\n",
      "1.0\n",
      "0.9887217879295349\n",
      "0.9956616163253784\n",
      "0.9964285492897034\n",
      "0.9855371713638306\n",
      "0.9938650131225586\n",
      "0.9803600907325745\n",
      "0.9876543283462524\n",
      "0.9791666865348816\n",
      "1.0\n",
      "1.0\n",
      "0.9961165189743042\n",
      "0.9975489974021912\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9886156737804412"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 0\n",
    "for fold in val_acc_per_fold:\n",
    "    print(fold[-1])\n",
    "    s += fold[-1]\n",
    "s/30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'classifier_RNN' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Calling `save('my_model')` creates a SavedModel folder `my_model`.\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m classifier_RNN\u001b[39m.\u001b[39msave(\u001b[39m\"\u001b[39m\u001b[39mclassifier_RNN_leave_one_out\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[39m# It can be used to reconstruct the model identically.\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[39m#reconstructed_model = keras.models.load_model(\"classifier_RNN_leave_one_out\")\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'classifier_RNN' is not defined"
     ]
    }
   ],
   "source": [
    "# Calling `save('my_model')` creates a SavedModel folder `my_model`.\n",
    "classifier_RNN_leave.save(\"classifier_RNN_leave_one_out\")\n",
    "\n",
    "# It can be used to reconstruct the model identically.\n",
    "#reconstructed_model = keras.models.load_model(\"classifier_RNN_leave_one_out\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check - SVM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/svm.html"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5-fold**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm, metrics\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:26<00:00,  5.39s/it]\n"
     ]
    }
   ],
   "source": [
    "val_acc_per_fold = []\n",
    "sets = [(fold_1, fold_1_lab), (fold_2, fold_2_lab), (fold_3, fold_3_lab), (fold_4, fold_4_lab), (fold_5, fold_5_lab)]\n",
    "\n",
    "for n in tqdm(range(len(sets))):\n",
    "    test_x = sets[n][0]\n",
    "    test_y = sets[n][1]\n",
    "    train_x = pd.DataFrame()\n",
    "    train_y = pd.DataFrame()\n",
    "    for s in sets[:n] + sets[n+1:]:\n",
    "        train_x = pd.concat([train_x, s[0]])\n",
    "        train_y = pd.concat([train_y, s[1]])\n",
    "\n",
    "    #print(np.array(train_y[:][0]).shape)\n",
    "\n",
    "    clf = svm.SVC(C=0.5, kernel='rbf', gamma='scale')\n",
    "\n",
    "    history_svm = clf.fit(np.array(train_x), np.array(train_y[:][0]))\n",
    "\n",
    "    pred=clf.predict(np.array(test_x))\n",
    "    \n",
    "    val_acc_per_fold.append(metrics.accuracy_score(np.array(test_y[:][0]), pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9797936371453139,\n",
       " 0.9810834049871023,\n",
       " 0.9810834049871023,\n",
       " 0.9785038693035254,\n",
       " 0.9793637145313844]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_acc_per_fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9799656061908857"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(val_acc_per_fold)/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "filename = 'svm_98.sav'\n",
    "pickle.dump(clf, open(filename, 'wb'))\n",
    " \n",
    "#load the model from disk\n",
    "#loaded_model = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVC 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:23<00:00,  4.72s/it]\n"
     ]
    }
   ],
   "source": [
    "val_acc_per_fold = []\n",
    "sets = [(fold_1, fold_1_lab), (fold_2, fold_2_lab), (fold_3, fold_3_lab), (fold_4, fold_4_lab), (fold_5, fold_5_lab)]\n",
    "\n",
    "for n in tqdm(range(len(sets))):\n",
    "    test_x = sets[n][0]\n",
    "    test_y = sets[n][1]\n",
    "    train_x = pd.DataFrame()\n",
    "    train_y = pd.DataFrame()\n",
    "    for s in sets[:n] + sets[n+1:]:\n",
    "        train_x = pd.concat([train_x, s[0]])\n",
    "        train_y = pd.concat([train_y, s[1]])\n",
    "\n",
    "    #print(np.array(train_y[:][0]).shape)\n",
    "\n",
    "    clf = svm.SVC(C=0.7, kernel='rbf', gamma='scale')\n",
    "\n",
    "    history_svm = clf.fit(np.array(train_x), np.array(train_y[:][0]))\n",
    "\n",
    "    pred=clf.predict(np.array(test_x))\n",
    "    \n",
    "    val_acc_per_fold.append(metrics.accuracy_score(np.array(test_y[:][0]), pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9832330180567498,\n",
       " 0.9828030954428203,\n",
       " 0.9845227858985383,\n",
       " 0.9836629406706793,\n",
       " 0.9828030954428203]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_acc_per_fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9834049871023216"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(val_acc_per_fold)/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "filename = 'svm_983.sav'\n",
    "pickle.dump(clf, open(filename, 'wb'))\n",
    " \n",
    "#load the model from disk\n",
    "#loaded_model = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Leave one out**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [02:21<00:00,  4.71s/it]\n"
     ]
    }
   ],
   "source": [
    "val_acc_per_fold = []\n",
    "\n",
    "for n in tqdm(range(1, 31)):\n",
    "    test_x = pd.DataFrame()\n",
    "    test_y = pd.DataFrame()\n",
    "\n",
    "    idxs_test = df_fall[df_fall['subject'] == n].index.to_list()\n",
    "    idxs_test_2 = df_adl[df_adl['subject'] == n].index.to_list()\n",
    "    test_x = pd.concat([test_x, fall_data.loc[idxs_test], adl_data.loc[idxs_test_2]])\n",
    "    test_y = pd.concat([test_y, df_fall.loc[idxs_test]['label'], df_adl.loc[idxs_test_2]['label']])\n",
    "    #print(test_x.shape, test_y.shape)\n",
    "\n",
    "    train_x = pd.DataFrame()\n",
    "    train_y = pd.DataFrame()\n",
    "\n",
    "    fall_data_c = fall_data.copy()\n",
    "    adl_data_c = adl_data.copy()\n",
    "    fall_data_c.drop(idxs_test, axis=0, inplace=True)\n",
    "    adl_data_c.drop(idxs_test_2, axis=0, inplace=True)\n",
    "    train_x = pd.concat([train_x, fall_data_c, adl_data_c])\n",
    "\n",
    "    df_fall_c = df_fall.copy()\n",
    "    df_adl_c = df_adl.copy()\n",
    "    df_fall_c.drop(idxs_test, axis=0, inplace=True)\n",
    "    df_adl_c.drop(idxs_test_2, axis=0, inplace=True)\n",
    "    train_y = pd.concat([train_y, df_fall_c['label'], df_adl_c['label']])\n",
    "    #print(train_x.shape, train_y.shape)\n",
    "\n",
    "    clf = svm.SVC(C=0.7, kernel='rbf', gamma='scale')\n",
    "\n",
    "    history_svm = clf.fit(np.array(train_x), np.array(train_y[:][0]))\n",
    "\n",
    "    pred=clf.predict(np.array(test_x))\n",
    "    \n",
    "    val_acc_per_fold.append(metrics.accuracy_score(np.array(test_y[:][0]), pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9479166666666666,\n",
       " 0.934819897084048,\n",
       " 0.993421052631579,\n",
       " 0.9646302250803859,\n",
       " 0.9966442953020134,\n",
       " 0.9563106796116505,\n",
       " 0.9545454545454546,\n",
       " 0.9496221662468514,\n",
       " 0.927536231884058,\n",
       " 0.9776357827476039,\n",
       " 0.9566326530612245,\n",
       " 0.992,\n",
       " 0.9681697612732095,\n",
       " 0.9919354838709677,\n",
       " 0.9898734177215189,\n",
       " 0.9957264957264957,\n",
       " 1.0,\n",
       " 0.9620689655172414,\n",
       " 0.9924812030075187,\n",
       " 0.9978308026030369,\n",
       " 0.9714285714285714,\n",
       " 0.9008264462809917,\n",
       " 0.99079754601227,\n",
       " 0.9525368248772504,\n",
       " 0.980246913580247,\n",
       " 0.9940476190476191,\n",
       " 0.9887640449438202,\n",
       " 0.9841772151898734,\n",
       " 0.9669902912621359,\n",
       " 0.9730392156862745]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_acc_per_fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9717551974296857"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(val_acc_per_fold)/30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "filename = 'svm_leave_one_out.sav'\n",
    "pickle.dump(clf, open(filename, 'wb'))\n",
    " \n",
    "#load the model from disk\n",
    "#loaded_model = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "9304ce306e50db976e6fa3a54a2b03a7b422842d1fab80961a2dcce864134bd5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
